{"./":{"url":"./","title":"简介","keywords":"","body":"Curiouser's Devops RoadmapThis gitbook records the technical roadmap of Devops Curiouser.LinkGitBook Access URL: https://gitbook.curiouser.top/docsGitHub: https://github.com/RationalMonsterWhat I had done at Openshift or KubernetesCI/CD Flow1. Gitlab Webhook + Jenkins SharedLibraries/Kubernetes + SonarScanner Maven PluginCopyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-12-02 21:50:07 "},"origin/openshift-allinone安装.html":{"url":"origin/openshift-allinone安装.html","title":"Allinone","keywords":"","body":"搭建Allinone全组件Openshift 3.11一、OverviewsPrerequisiteIP地址：192.168.1.86 CentOS：7.5.1804 硬盘划分 1. 系统盘60G / 2. 数据盘100G /var/lib/docker ; 100G /data/nfs 开启Selinuxsed -i \"s/SELINUX=disabled/SELINUX=enforcing/\" /etc/selinux/config && \\ setenforce 0 ContextDocker：版本 1.13，Overlay2(执行Ansible准备脚本时会进行安装) Openshift：版本 3.11 Kubernetes：版本 v1.11.0 二、使用Ansible安装部署设置主机名并在本地Host文件中添加IP地址域名映射关系ipaddr=$(ip addr | awk '/^[0-9]+: / {}; /inet.*global/ {print gensub(/(.*)\\/(.*)/, \"\\\\1\", \"g\", $2)}'| sed -n '1p') && \\ echo $ipaddr $HOSTNAME >> /etc/hosts 配置中科大Openshift的YUM 源mkdir /etc/yum.repos.d/bak && \\ mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak && \\ bash -c ' cat > /etc/yum.repos.d/all.repo 安装基础软件yum install -y git vim net-tools lrzsz unzip bind-utils yum-utils bridge-utils python-passlib wget java-1.8.0-openjdk-headless httpd-tools lvm2 安装Ansible 2.6.5yum install -y https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ansible-2.6.5-1.el7.ans.noarch.rpm 获取openshift ansible部署脚本代码，禁用ansible脚本中的指定repogit clone https://github.com/openshift/openshift-ansible.git -b release-3.11 && \\ sed -i 's/enabled=1/enabled=0/g' /root/openshift-ansible/roles/openshift_repos/templates/CentOS-OpenShift-Origin.repo.j2 && \\ sed -i 's/enabled=1/enabled=0/g' /root/openshift-ansible/roles/openshift_repos/templates/CentOS-OpenShift-Origin311.repo.j2 (可选)将附件中定制化的OKD登陆页面文件放置/etc/origin/master/custom路径下（自定义的登陆首页）# 路径需要新建 mkdir -p /etc/origin/master/custom 配置Ansible部署Openshift的主机清单/etc/ansible/hosts [OSEv3:children] masters nodes etcd nfs [OSEv3:vars] openshift_ip=192.168.1.86 openshift_public_ip=192.168.1.86 ansible_default_ipv4.address=192.168.1.86 ansible_ssh_user=root openshift_deployment_type=origin deployment_type=origin openshift_release=3.11 openshift_image_tag=v3.11.0 ansible_ssh_pass=**Root用户SSH密码** ######################### Components Cert and CA Expire Days ################# openshift_hosted_registry_cert_expire_days=36500 openshift_ca_cert_expire_days=36500 openshift_node_cert_expire_days=36500 openshift_master_cert_expire_days=36500 etcd_ca_default_days=36500 ####################### Multitenant Network ####################### os_sdn_network_plugin_name=redhat/openshift-ovs-multitenant ####################### OKD ####################### openshift_clock_enabled=true openshift_enable_unsupported_configurations=True openshift_node_groups=[{'name': 'allinone', 'labels': ['node-role.kubernetes.io/master=true', 'node-role.kubernetes.io/infra=true', 'node-role.kubernetes.io/compute=true']}] openshift_disable_check=memory_availability,disk_availability,package_availability,package_update,docker_image_availability,docker_storage_driver,docker_storage ####################### OKD master config ####################### openshift_master_api_port=8443 openshift_master_cluster_public_hostname=allinone.okd311.curiouser.com openshift_master_cluster_hostname=allinone.okd311.curiouser.com openshift_master_default_subdomain=apps.okd311.curiouser.com openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}] openshift_master_htpasswd_users={'admin':'$apr1$eG8zNL.C$fvACBzDJ7.N7KdJORT12E0'} openshift_master_oauth_template=custom/login.html openshift_master_session_name=ssn openshift_master_session_max_seconds=3600 ####################### Docker ####################### container_runtime_docker_storage_setup_device=/dev/sdb container_runtime_docker_storage_type=overlay2 openshift_examples_modify_imagestreams=true openshift_docker_options=\"--selinux-enabled -l warn --ipv6=false --insecure-registry=0.0.0.0/0 --log-opt max-size=10M --log-opt max-file=3 --registry-mirror=https://zlsoueh7.mirror.aliyuncs.com\" ####################### Web Console ####################### openshift_web_console_extension_script_urls=[\"https://xhua-static.sh1a.qingstor.com/allinone/allinone-webconsole.js\"] openshift_web_console_extension_stylesheet_urls=[\"https://hermes-uat.mbcloud.com/mbcloud/M00/00/3A/rBACF1vz8NyALOS3AAApT8C9PDY549.css\"] ####################### Registry ####################### openshift_hosted_registry_storage_kind=nfs openshift_hosted_registry_storage_access_modes=['ReadWriteMany'] openshift_hosted_registry_storage_nfs_directory=/data/nfs openshift_hosted_registry_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_hosted_registry_storage_volume_name=registry openshift_hosted_registry_storage_volume_size=10Gi ####################### metrics ####################### openshift_metrics_install_metrics=true openshift_metrics_image_version=v3.11.0 openshift_metrics_storage_kind=nfs openshift_metrics_storage_access_modes=['ReadWriteOnce'] openshift_metrics_storage_nfs_directory=/data/nfs openshift_metrics_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_metrics_storage_volume_name=metrics openshift_metrics_storage_volume_size=10Gi ####################### logging ####################### openshift_logging_install_logging=true openshift_logging_image_version=v3.11.0 openshift_logging_es_ops_nodeselector={\"node-role.kubernetes.io/infra\":\"true\"} openshift_logging_es_nodeselector={\"node-role.kubernetes.io/infra\":\"true\"} openshift_logging_elasticsearch_pvc_size=5Gi openshift_logging_storage_kind=nfs openshift_logging_storage_access_modes=['ReadWriteOnce'] openshift_logging_storage_nfs_directory=/data/nfs openshift_logging_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_logging_storage_volume_name=logging openshift_logging_storage_volume_size=10Gi ################### Prometheus Cluster Monitoring ################### openshift_cluster_monitoring_operator_install=true openshift_cluster_monitoring_operator_prometheus_storage_enabled=true openshift_cluster_monitoring_operator_alertmanager_storage_enabled=true openshift_cluster_monitoring_operator_prometheus_storage_capacity=50Gi openshift_cluster_monitoring_operator_alertmanager_storage_capacity=5Gi ##################### Disable Components ############# openshift_enable_service_catalog=false ansible_service_broker_install=false [masters] allinone.okd311.curiouser.com [etcd] allinone.okd311.curiouser.com [nfs] allinone.okd311.curiouser.com [nodes] allinone.okd311.curiouser.com openshift_node_group_name='allinone' 创建NFS挂载目录 pvcreate /dev/sdc && \\ vgcreate -s 4m data /dev/sdc && \\ lvcreate --size 45G -n nfs data && \\ mkfs.xfs /dev/data/nfs && \\ echo \"/dev/data/nfs /data/nfs xfs defaults 0 0\" >> /etc/fstab && \\ mkdir /data/nfs -p && \\ mount -a && \\ df -mh （可选）预先拉取安装过程中可能使用的镜像docker pull docker.io/openshift/origin-node:v3.11.0 && \\ docker pull docker.io/openshift/origin-control-plane:v3.11.0 && \\ docker pull docker.io/openshift/origin-haproxy-router:v3.11.0 && \\ docker pull docker.io/openshift/origin-deployer:v3.11.0 && \\ docker pull docker.io/openshift/origin-pod:v3.11.0 && \\ docker pull docker.io/openshift/origin-docker-registry:v3.11.0 && \\ docker pull docker.io/openshift/origin-console:v3.11.0 && \\ docker pull docker.io/openshift/origin-service-catalog:v3.11.0 && \\ docker pull docker.io/openshift/origin-web-console:v3.11.0 && \\ docker pull docker.io/cockpit/kubernetes:latest && \\ docker pull docker.io/openshift/oauth-proxy:v1.1.0 && \\ docker pull docker.io/openshift/origin-docker-builder:v3.11.0 && \\ docker pull docker.io/openshift/prometheus-alertmanager:v0.15.2 && \\ docker pull docker.io/openshift/prometheus-node-exporter:v0.16.0 && \\ docker pull docker.io/openshift/prometheus:v2.3.2 && \\ docker pull docker.io/grafana/grafana:5.2.1 && \\ docker pull quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 && \\ docker tag quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 quay.io/coreos/kube-rbac-proxy:v0.3.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 && \\ docker pull quay-mirror.qiniu.com/coreos/etcd:v3.2.22 && \\ docker tag quay-mirror.qiniu.com/coreos/etcd:v3.2.22 quay.io/coreos/etcd:v3.2.22 && \\ docker rmi quay-mirror.qiniu.com/coreos/etcd:v3.2.22 && \\ docker pull quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 && \\ docker tag quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 quay.io/coreos/kube-state-metrics:v1.3.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 && \\ docker pull quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 && \\ docker tag quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 quay.io/coreos/configmap-reload:v0.0.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 && \\ docker pull quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker tag quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 quay.io/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker pull quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 && \\ docker tag quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 quay.io/coreos/prometheus-config-reloader:v0.23.2 && \\ docker rmi quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 && \\ docker pull quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 && \\ docker tag quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 quay.io/coreos/prometheus-operator:v0.23.2 && \\ docker rmi quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 执行OKD Ansible Playbook先执行安装检查的Playbookansible-playbook /root/openshift-ansible/playbooks/prerequisites.yml 再执行安装Playbookansible-playbook /root/openshift-ansible/playbooks/deploy_cluster.yml 授予admin用户以管理员权限 oc adm policy add-cluster-role-to-user cluster-admin admin 三、配置Openshift的后端存储使用Ceph RBD作为后端存储 1. 搭建单节点的Ceph，详见（Ceph RBD单节点安装） 2. 创建Storageclass 使用NFS作为后端存储：详见 参考文章链接当主机有多网卡时指定组件监听的网卡IP地址：https://github.com/ViaQ/Main/blob/master/README-install.mdCopyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:10 "},"origin/openshift-Kubernetes的持久化存储.html":{"url":"origin/openshift-Kubernetes的持久化存储.html","title":"数据持久化","keywords":"","body":"一、Kubernetes持久化存储简介通常情况下，我们可以认为容器或者Pod的生命周期时短暂的，当容器被销毁时，容器内部的数据也同时被清除。对于容器，数据持久化存储的重要性不言而喻。Docker有存储卷的概念，用来将磁盘上的或另一个容器中的目录挂载到容器的某一个路径下。即使容器挂掉了，挂载Volume中的数据依旧存在。然而没有对其生命周期进行管理。而Kubernetes提供了多种不同类型资源的Volume存储卷，供POD挂载到容器的不同路径下,常见的有：emptyDir：pod被调度到某个宿主机上的时候才创建，而同一个pod内的容器都能读写EmptyDir中的同一个文件。删除容器并不会对它造成影响，只有删除整个Pod时，它才会被删除，它的生命周期与所挂载的Pod一致 hostPath：将宿主机的文件系统的文件或目录挂接到Pod中 secret：将Kubernetes中secret对象资源挂载到POD中 configMap：将Kubernetes中config对象资源挂载到POD中 persistentVolumeClaim：将PersistentVolume挂接到Pod中作为存储卷。使用此类型的存储卷，用户不需要关注存储卷的详细信息。 nfs glusterfs cephfs vspherevolume iscsi .... 对于以上大部分的volume类型，对使用用户是极其不友好的。理解他们体系中的概念配置是一件复杂的事情，有时我们其实并不关心他们的各种存储实现，只希望能够简单安全可靠地存储数据。所以K8S对存储的供应和使用做了抽象，以API形式提供给管理员和用户使用。因此引入了两个新的API资源：Persistent Volume（持久卷PV）和Persistent Volume Claim（持久卷申请PVC）。PVC负责定义使用多大的存储空间，什么样的读写方式等常见要求即可，而PV负责抽象各种存储系统的技术细节（例如存储系统IP地址端口，客户端证书密钥等），满足PVC的存储需求，继而作为Kubernetes集群的存储对象资源。apiVersion: \"v1\" kind: \"PersistentVolumeClaim\" metadata: name: \"ceph-pvc-test\" namespace: \"default\" spec: accessModes: - \"ReadWriteMany\" resources: requests: storage: \"2Gi\" volumeName: \"pv-nfs-test\" # 指定PV PersistentVolumesClaim的属性Access ModesReadWriteOnce —— 该volume只能被单个节点以读写的方式映射 ReadOnlyMany —— 该volume可以被多个节点以只读方式映射 ReadWriteMany —— 该volume只能被多个节点以读写的方式映射 Volume Modes：在Kubernetes 1.9之前，所有卷插件都在pv上创建了一个文件系统。现在，可以将volumeMode的值设置为block以使用原始块设备，或者将filesystem设置为使用文件系统。如果省略该值，则默认为filesystem。 Resources：指定使用多大的存储空间 Selector：PVC可以指定标签选择器进行更深度的过滤PV，只有匹配了选择器标签的PV才能绑定给PVC。选择器包含两个字段：matchLabels（匹配标签） - PV必须有一个包含该值得标签 matchExpressions（匹配表达式） - 一个请求列表，包含指定的键、值的列表、关联键和值的操作符。合法的操作符包含In，NotIn，Exists，和DoesNotExist。 　　所有来自matchLabels和matchExpressions的请求，都是逻辑与关系的，它们必须全部满足才能匹配上。 Class apiVersion: v1 kind: PersistentVolume metadata: name: ceph-pv-test spec: capacity: storage: 2Gi accessModes: - ReadWriteOnce rbd: monitors: - 192.168.122.133:6789 pool: rbd image: ceph-image user: admin secretRef: name: ceph-secret fsType: ext4 readOnly: false persistentVolumeReclaimPolicy: Retain claimRef: name: \"pvc-test\" namespace: \"default\" PersistentVolumes的属性Capacity：指定存储容量大小 Volume Mode：在Kubernetes 1.9之前，所有卷插件都在pv上创建了一个文件系统。现在，可以将volumeMode的值设置为block以使用原始块设备，或者将filesystem设置为使用文件系统。如果省略该值，则默认为filesystem。 Class: 一个PV可以有一种class，通过设置storageClassName属性来选择指定的StorageClass。有指定class的PV只能绑定给请求该class的PVC。没有设置storageClassName属性的PV只能绑定给未请求class的PVC(过去，使用volume.beta.kubernetes.io/storage-class注解，而不是storageClassName属性。该注解现在依然可以工作，但在Kubernetes的未来版本中已经被完全弃用了) Reclaim Policy Mount Options Node Affinity Access Modes ReadWriteOnce —— 该volume只能被单个节点以读写的方式映射 ReadOnlyMany —— 该volume可以被多个节点以只读方式映射 ReadWriteMany —— 该volume只能被多个节点以读写的方式映射 PersistentVolumes的周期状态Available: 空闲的，未绑定给PVC Bound: 绑定上了某个PVC Released: PVC已经删除了，但是PV还没有被回收 Failed: PV在自动回收中失败了 PV支持的存储系统:GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk CSI FC (Fibre Channel) Flexvolume Flocker NFS iSCSI RBD (Ceph Block Device) CephFS Cinder (OpenStack block storage) Glusterfs VsphereVolume Quobyte Volumes HostPath (Single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster) Portworx Volumes ScaleIO Volumes StorageOS PV和PVC 之间的关联遵循如下的生命周期：Provisioning-供应: PV的创建阶段，有以下两种创建方式静态手工：集群管理员通过手工的方式创建pv 动态自动：通过PersistentVolume Controller动态调度，Kubernetes将能够按照用户的需要，根据PVC的资源请求，寻找StorageClasse定义的符合要求的底层存储自动创建其需要的存储卷。 Binding-绑定: PV分配绑定到PVC Using-使用： POD挂载使用PVC类型的Volume Reclaiming-回收：PV释放后的回收利用策略 Retain保留: 保留现场，人工回收 Delete删除: 自动删除，动态删除后端存储。需要IaaS层的支持，目前只有Ceph RBD和OpenStack Cinder支持 Recycle复用：通过rm -rf删除卷上的所有数据。目前只有NFS和HostPath支持（逐渐在抛弃该方式，建议使用） 二、使用StorageClass提供动态存储供应通常情况下，Kubernetes集群管理员需要手工创建所需的PV存储资源。从Kubernetes 1.2以后可以使用Storageclass实现动态自动地根据用户需求创建某种存储系统类型的PV。同时，可以定义多个 StorageClass ，给集群提供不同存储系统类型的PV资源。1. 定义创建StorageClass每一个存储类都必须包含以下参数provisioner: 决定由哪个Provisioner来创建PV parameters: Provisioner需要的参数,可选项：Delete(Default),Retain reclaimPolicy: PV的回收策略 可选参数：Mount Options Volume Binding Mode Allowed Topologies Note: StorageClass一旦被创建，将不能被更新apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-rbd # 指定Provisioner provisioner: kubernetes.io/rbd parameters: monitors: 10.20.30.40:6789 adminId: kube adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: kube userId: kube userSecretName: ceph-secret-user userSecretNamespace: default fsType: ext4 imageFormat: \"2\" imageFeatures: \"layering\" Kubernetes支持的Provisioner Provisioner 是否内置插件 配置例子 AWSElasticBlockStore ✓ [AWS](https://kubernetes.io/docs/concepts/storage/storage-classes/#aws) AzureFile ✓ [Azure File](https://kubernetes.io/docs/concepts/storage/storage-classes/#azure-file) AzureDisk ✓ [Azure Disk](https://kubernetes.io/docs/concepts/storage/storage-classes/#azure-disk) CephFS – – Cinder ✓ [OpenStack Cinder](https://kubernetes.io/docs/concepts/storage/storage-classes/#openstack-cinder) FC – – FlexVolume – – Flocker ✓ – GCEPersistentDisk ✓ [GCE](https://kubernetes.io/docs/concepts/storage/storage-classes/#gce) Glusterfs ✓ [Glusterfs](https://kubernetes.io/docs/concepts/storage/storage-classes/#glusterfs) iSCSI – – PhotonPersistentDisk ✓ – Quobyte ✓ [Quobyte](https://kubernetes.io/docs/concepts/storage/storage-classes/#quobyte) NFS – – RBD ✓ [Ceph RBD](https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd) VsphereVolume ✓ [vSphere](https://kubernetes.io/docs/concepts/storage/storage-classes/#vsphere) PortworxVolume ✓ [Portworx Volume](https://kubernetes.io/docs/concepts/storage/storage-classes/#portworx-volume) ScaleIO ✓ [ScaleIO](https://kubernetes.io/docs/concepts/storage/storage-classes/#scaleio) StorageOS ✓ [StorageOS](https://kubernetes.io/docs/concepts/storage/storage-classes/#storageos) Local – [Local](https://kubernetes.io/docs/concepts/storage/storage-classes/#local) StorageClas可以支持第三方的Provisioner，只要该插件符合Kubernetes的规范 内置的Provisioner名称带有“kubernetes.io”前缀 Github仓库：https://github.com/kubernetes-incubator/external-storage 有官方支持的第三方Provisioner 2. 指定StorageClass动态创建PV在Kubernetes v1.6之前的版本，通过volume.beta.kubernetes.io/storage-class注释类请求动态供应存储； 在Kubernetes v1.6版本之后，用户应该使用PersistentVolumeClaim对象的storageClassName参数来请求动态存储。apiVersion: v1 kind: PersistentVolumeClaim metadata: name: claim1 spec: accessModes: - ReadWriteOnce # 指定所使用的存储类，此存储类将会自动创建符合要求的PV storageClassName: ceph-rbd resources: requests: storage: 30Gi 3. 指定默认的StorageClass创建StorageClass时可添加添加storageclass.kubernetes.io/is-default-class注解来指定为默认的存储类。apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: \"storageclass.kubernetes.io/is-default-class\": \"true\" # 将此storageclass设置为默认 name: nfs-client-storageclass provisioner: fuseim.pri/ifs parameters: archiveOnDelete: \"true\" 一个集群中，最多只能有一个默认的存储类 如果没有默认的存储类，在PersistentVolumeClaim中也没有显示指定storageClassName，将无法创建PersistentVolume。 参考链接https://kubernetes.io/docs/concepts/storage/volumes/ https://kubernetes.io/docs/concepts/storage/storage-classes/ https://www.kubernetes.org.cn/4078.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:10 "},"origin/openshift-Kubernetes-provisioner-nfs-client.html":{"url":"origin/openshift-Kubernetes-provisioner-nfs-client.html","title":"NFS Client provisioner","keywords":"","body":"一、NFS Client Provisionerhttps://github.com/kubernetes-incubator/external-storage/tree/master/nfs-clienthttps://www.kubernetes.org.cn/3894.htmlProvisioner的定义原理: openshift-Kubernetes的持久化存储二、安装部署1. 创建NFS服务端yum install -y nfs-utils rpcbind && \\ systemctl enable nfs && \\ systemctl enable rpcbind && \\ systemctl start nfs && \\ systemctl start rpcbind && \\ mkdir -p /data/nfs/appstorage-nfs-client-provisioner && \\ echo \"/data/nfs/appstorage-nfs-client-provisioner *(rw,no_root_squash,sync)\" >> /etc/exports && \\ exportfs -a && \\ showmount -e $HOSTNAME 2. 创建RBACkind: ServiceAccount apiVersion: v1 metadata: name: nfs-client-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: default roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 3. 修改Deployment并以此部署POD先拉取镜像docker pull quay-mirror.qiniu.com/external_storage/nfs-client-provisioner:latest && \\ docker tag quay-mirror.qiniu.com/external_storage/nfs-client-provisioner:latest quay.io/external_storage/nfs-client-provisioner:latest && \\ docker rmi quay-mirror.qiniu.com/external_storage/nfs-client-provisioner:latest kind: Deployment apiVersion: extensions/v1beta1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: $HOSTNAME # NFS Server的地址 - name: NFS_PATH value: /data/nfs/appstorage-nfs-client-provisioner # NFS Server要挂载的路径 volumes: - name: nfs-client-root nfs: server: $HOSTNAME #指定NFS Server的地址 path: /data/nfs/appstorage-nfs-client-provisioner #指定NFS Server要挂载的路径 三、使用1. 创建StorageClassapiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-client-storageclass provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" # When set to \"false\" your PVs will not be archived by the provisioner upon deletion of the PVC. =======================================================补充内容========================================================= #如果要将此storageclass设置为默认，在metadata里面添加以下注解。（这样创建PVC时就可以不用特意指定StorageClass） apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: \"storageclass.kubernetes.io/is-default-class\": \"true\" name: nfs-client-storageclass provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" # \"false\" 删除PVC时不会保留数据，\"true\"将保留PVC的数据，形成以\"archived-\"开头的文件夹 2. 创建PVC时使用kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-pvc #当默认storageclass就是nfs-client-storageclass，可不要该注解 annotations: volume.beta.kubernetes.io/storage-class: \"nfs-client-storageclass\" spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi 四、测试1. 创建一个PVCkind: PersistentVolumeClaim apiVersion: v1 metadata: name: test spec: accessModes: - ReadWriteMany resources: requests: storage: 100Mi ​ #======================================================================================================================== $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test Bound pvc-e8a15786-5a09-11e9-ad53-000c296286d8 100Mi RWX nfs-client-storageclass 10m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-e8a15786-5a09-11e9-ad53-000c296286d8 100Mi RWX Delete Bound default/test nfs-client-storageclass 10m 2. 创建一个POD使用PVCapiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog persistentVolumeClaim: claimName: test 3. 查看NFS目录/data/nfs/k8s-app-nfs-storage/ └── [drwxrwxrwx 32] default-test-pvc-e8a15786-5a09-11e9-ad53-000c296286d8 ├── [-rw-r--r-- 947] 1.log └── [-rw-r--r-- 1.0K] 2.log Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:09 "},"origin/openshift-Kubernetes-provisioner-nfs-server.html":{"url":"origin/openshift-Kubernetes-provisioner-nfs-server.html","title":"NFS Server Provisioner","keywords":"","body":"一、NFS Server ProvisionerGithub项目地址：https://github.com/kubernetes-incubator/external-storage/tree/v5.2.0/nfsProvisioner的定义原理：openshift-Kubernetes的持久化存储NFS Provisioner的部署文档：https://github.com/kubernetes-incubator/external-storage/blob/master/nfs/docs/deployment.mdNFS Provisioner的使用文档：https://github.com/kubernetes-incubator/external-storage/blob/master/nfs/docs/usage.md二、在Kubernetes上部署1、（可选）预拉取镜像docker pull quay-mirror.qiniu.com/kubernetes_incubator/nfs-provisioner:latest && \\ docker tag quay-mirror.qiniu.com/kubernetes_incubator/nfs-provisioner:latest quay.io/kubernetes_incubator/nfs-provisioner:latest && \\ docker rmi quay-mirror.qiniu.com/kubernetes_incubator/nfs-provisioner:latest 2、创建PodSecurityPolicyapiVersion: extensions/v1beta1 kind: PodSecurityPolicy metadata: name: nfs-provisioner spec: fsGroup: rule: RunAsAny allowedCapabilities: - DAC_READ_SEARCH - SYS_RESOURCE runAsUser: rule: RunAsAny seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny volumes: - configMap - downwardAPI - emptyDir - persistentVolumeClaim - secret - hostPath 3、创建RBACkind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-provisioner subjects: - kind: ServiceAccount name: nfs-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-provisioner rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-provisioner subjects: - kind: ServiceAccount name: nfs-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-nfs-provisioner apiGroup: rbac.authorization.k8s.io 4、使用deployment创建POD（推荐）apiVersion: v1 kind: ServiceAccount metadata: name: nfs-provisioner --- kind: Service apiVersion: v1 metadata: name: nfs-provisioner labels: app: nfs-provisioner spec: ports: - name: nfs port: 2049 - name: mountd port: 20048 - name: rpcbind port: 111 - name: rpcbind-udp port: 111 protocol: UDP selector: app: nfs-provisioner --- kind: Deployment apiVersion: apps/v1 metadata: name: nfs-provisioner spec: selector: matchLabels: app: nfs-provisioner replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-provisioner image: quay.io/kubernetes_incubator/nfs-provisioner:latest ports: - name: nfs containerPort: 2049 - name: mountd containerPort: 20048 - name: rpcbind containerPort: 111 - name: rpcbind-udp containerPort: 111 protocol: UDP securityContext: capabilities: add: - DAC_READ_SEARCH - SYS_RESOURCE args: # 定义提供者的名称，存储类通过此名称指定提供者 - \"-provisioner=example.com/nfs\" env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: SERVICE_NAME value: nfs-provisioner - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: export-volume mountPath: /export volumes: - name: export-volume hostPath: path: /srv 5、（可选）使用StatefulSet创建PODapiVersion: v1 kind: ServiceAccount metadata: name: nfs-provisioner --- kind: Service apiVersion: v1 metadata: name: nfs-provisioner labels: app: nfs-provisioner spec: ports: - name: nfs port: 2049 - name: mountd port: 20048 - name: rpcbind port: 111 - name: rpcbind-udp port: 111 protocol: UDP selector: app: nfs-provisioner --- kind: StatefulSet apiVersion: apps/v1 metadata: name: nfs-provisioner spec: selector: matchLabels: app: nfs-provisioner serviceName: \"nfs-provisioner\" replicas: 1 template: metadata: labels: app: nfs-provisioner spec: serviceAccount: nfs-provisioner terminationGracePeriodSeconds: 10 containers: - name: nfs-provisioner image: quay.io/kubernetes_incubator/nfs-provisioner:latest ports: - name: nfs containerPort: 2049 - name: mountd containerPort: 20048 - name: rpcbind containerPort: 111 - name: rpcbind-udp containerPort: 111 protocol: UDP securityContext: capabilities: add: - DAC_READ_SEARCH - SYS_RESOURCE args: - \"-provisioner=example.com/nfs\" env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: SERVICE_NAME value: nfs-provisioner - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: export-volume mountPath: /export volumes: - name: export-volume hostPath: path: /srv 三、使用创建StorageClasskind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: example-nfs provisioner: example.com/nfs parameters: mountOptions: \"vers=4.1\" ​ =======================================================补充内容========================================================= ​ #其他参数： gid: # \"none\" or a supplemental group like \"1001\". NFS shares will be created with permissions such that pods running with the supplemental group can read & write to the share, but non-root pods without the supplemental group cannot. Pods running as root can read & write to shares regardless of the setting here, unless the rootSquash parameter is set true. If set to \"none\", anybody root or non-root can write to the share. Default (if omitted) \"none\". rootSquash: # \"true\" or \"false\". Whether to squash root users by adding the NFS Ganesha root_id_squash or kernel root_squash option to each export. Default \"false\". mountOptions: # a comma separated list of mount options for every PV of this class to be mounted with. The list is inserted directly into every PV's mount options annotation/field without any validation. Default blank \"\". ​ #如果要将此storageclass设置为默认，在metadata里面添加以下注解。（这样创建PVC时就可以不用特意指定StorageClass） kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: #注解 annotations: \"storageclass.kubernetes.io/is-default-class\": \"true\" name: example-nfs provisioner: example.com/nfs parameters: mountOptions: \"vers=4.1\" 创建PVC时指定storageclasskind: PersistentVolumeClaim apiVersion: v1 metadata: name: nfs annotations: volume.beta.kubernetes.io/storage-class: \"example-nfs\" spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi 四、测试创建一个PVCkind: PersistentVolumeClaim apiVersion: v1 metadata: name: test annotations: volume.beta.kubernetes.io/storage-class: \"example-nfs\" spec: accessModes: - ReadWriteMany resources: requests: storage: 100Mi ​ #======================================================================================================================== $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test Bound pvc-e00e9603-56a5-11e9-95dd-080027c8ba17 100Mi RWX example-nfs 5m3s $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-e00e9603-56a5-11e9-95dd-080027c8ba17 100Mi RWX Delete Bound default/test example-nfs 5m9s 创建一个POD使用PVCapiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog persistentVolumeClaim: claimName: test 查看/srv目录/srv ├── [-rw-r--r-- 4.4K] ganesha.log ├── [-rw------- 36] nfs-provisioner.identity ├── [drwxrwsrwx 32] pvc-e00e9603-56a5-11e9-95dd-080027c8ba17 │ ├── [-rw-r--r-- 5.1K] 1.log │ └── [-rw-r--r-- 5.7K] 2.log └── [-rw------- 1.1K] vfs.conf 注意：删除掉PVC，PV也会自动删除，底层的NFS目录也会跟着删除 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:10 "},"origin/openshift-Kubernetes-provisioner-glusterfs.html":{"url":"origin/openshift-Kubernetes-provisioner-glusterfs.html","title":"Glusterfs Provisioner","keywords":"","body":"一、OKD集群中添加容器化的GlusteFSPrerequisiteOKD集群（3.11）至少有三个节点 OKD官方操作指南：https://docs.okd.io/3.11/install_config/persistent_storage/persistent_storage_glusterfs.html#install-config-persistent-storage-persistent-storage-glusterfs GlusterFS官方操作指南：https://docs.gluster.org/en/latest/Administrator%20Guide/overview/ heketi-cli官方操作指南：https://github.com/heketi/heketi 配置ansible主机清单/etc/ansible/hosts[OSEv3:children] ... glusterfs ​ [OSEv3:vars] ... openshift_storage_glusterfs_namespace=app-storage openshift_storage_glusterfs_storageclass=true openshift_storage_glusterfs_storageclass_default=false openshift_storage_glusterfs_block_deploy=true openshift_storage_glusterfs_block_host_vol_size=100 openshift_storage_glusterfs_block_storageclass=true openshift_storage_glusterfs_block_storageclass_default=false ​ [glusterfs] allinone311.okd.curiouser.com glusterfs_devices='[ \"/dev/vdf\" ]' node1.okd.curiouser.com glusterfs_devices='[ \"/dev/vdd\" ]' node2.okd.curiouser.com glusterfs_devices='[ \"/dev/vdd\" ]' #至少是三个节点 glusterfs节点上安装软件yum install glusterfs-fuse && \\ yum update glusterfs-fuse 配置glusterfs节点上的Selinuxsetsebool -P virt_sandbox_use_fusefs on && \\ setsebool -P virt_use_fusefs on 执行openshift ansible playbookansible-playbook /root/openshift-ansible/playbooks/openshift-glusterfs/config.yml 二、向OKD集群中添加集群外的GlusteFS配置ansible主机清单/etc/ansible/hosts[OSEv3:children] ... glusterfs ​ [OSEv3:vars] ... openshift_storage_glusterfs_namespace=app-storage openshift_storage_glusterfs_storageclass=true openshift_storage_glusterfs_storageclass_default=false openshift_storage_glusterfs_block_deploy=true openshift_storage_glusterfs_block_host_vol_size=100 openshift_storage_glusterfs_block_storageclass=true openshift_storage_glusterfs_block_storageclass_default=false openshift_storage_glusterfs_is_native=false openshift_storage_glusterfs_heketi_is_native=true openshift_storage_glusterfs_heketi_executor=ssh openshift_storage_glusterfs_heketi_ssh_port=22 openshift_storage_glusterfs_heketi_ssh_user=root openshift_storage_glusterfs_heketi_ssh_sudo=false openshift_storage_glusterfs_heketi_ssh_keyfile=\"/root/.ssh/id_rsa\" ​ [glusterfs] gluster1.example.com glusterfs_ip=192.168.10.11 glusterfs_devices='[ \"/dev/xvdc\", \"/dev/xvdd\" ]' gluster2.example.com glusterfs_ip=192.168.10.12 glusterfs_devices='[ \"/dev/xvdc\", \"/dev/xvdd\" ]' gluster3.example.com glusterfs_ip=192.168.10.13 glusterfs_devices='[ \"/dev/xvdc\", \"/dev/xvdd\" ]' 执行openshift ansible playbookansible-playbook /root/openshift-ansible/playbooks/openshift-glusterfs/config.yml 三、卸载ansible-playbook -e \"openshift_storage_glusterfs_wipe=true\" /root/openshift-ansible/playbooks/openshift-glusterfs/uninstall.yml 四、OKD中通过storage动态使用glusterfs作为PVC的后端存储1. 创建storage class创建storage class(ansible playbook执行过程中会自动创建storageclass)kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: glusterfs-storage provisioner: kubernetes.io/glusterfs parameters: resturl: 'http://heketi-storage.app-storage.svc:8080' restuser: admin secretName: heketi-storage-admin-secret secretNamespace: app-storage reclaimPolicy: Delete volumeBindingMode: Immediate 如果使用的集群外的Glusterfs集群，需要手动创建storage class。kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: glusterfs-storage provisioner: kubernetes.io/glusterfs parameters: resturl: \"http://10.42.0.0:8080\" restauthenabled: \"false\" 2. 创建PVC时使用Glusterfs的storage classapiVersion: v1 kind: PersistentVolumeClaim metadata: name: gluster1 spec: accessModes: - ReadWriteMany resources: requests: storage: 30Gi storageClassName: glusterfs-storage 五、主机上mount挂载使用容器化的GlusterFS挂载命令格式：mount -t glusterfs GlusterFS容器化pod所在的节点IP地址:/volume_name /mnt/glusterfs 示例：$ mount -t glusterfs 172.16.1.4:/vol_fe0de9d2f43731d1af7a5dc296041d83 /mnt/glusterfs && \\ df -mh 172.16.1.4:/vol_fe0de9d2f43731d1af7a5dc296041d83 10G 136M 9.9G 2% /mnt/glusterfs Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:08 "},"origin/openshift-Kubernetes-provisioner-cephfs.html":{"url":"origin/openshift-Kubernetes-provisioner-cephfs.html","title":"Ceph FileSystem Provisioner","keywords":"","body":"相关链接 官方文档： https://github.com/kubernetes-incubator/external-storage/tree/master/ceph/cephfs Provisioner的定义原理：Kubernetes的存储--> StorageClass provisioner 姊妹篇：Preflight1. Openshift创建cephfs命名空间oc new-project cephfs --display-name=\"Ceph FileSystem Provisioner\" 2. 拉取镜像docker pull quay-mirror.qiniu.com/external_storage/cephfs-provisioner:latest && \\ docker tag quay-mirror.qiniu.com/external_storage/cephfs-provisioner:latest quay.io/external_storage/cephfs-provisioner:latest && \\ docker rmi quay-mirror.qiniu.com/external_storage/cephfs-provisioner:latest 一、安装部署1. 获取Ceph Filesystem Client.admin用户的密钥环ceph auth get client.admin # [client.admin] # key = AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== # caps mds = \"allow *\" # caps mon = \"allow *\" # caps osd = \"allow *\" 2. 创建Secretsoc create secret generic cephfs-secret-admin --from-literal=key='AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ==' --namespace=cephfs 3. 创建RBAC--- apiVersion: v1 kind: ServiceAccount metadata: name: cephfs-provisioner namespace: cephfs --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: cephfs-provisioner namespace: cephfs rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"create\", \"get\", \"delete\"] - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: cephfs-provisioner namespace: cephfs roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cephfs-provisioner subjects: - kind: ServiceAccount name: cephfs-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cephfs-provisioner namespace: cephfs rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"kube-dns\",\"coredns\"] verbs: [\"list\", \"get\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cephfs-provisioner subjects: - kind: ServiceAccount name: cephfs-provisioner namespace: cephfs roleRef: kind: ClusterRole name: cephfs-provisioner apiGroup: rbac.authorization.k8s.io 4. 使用Deployment创建Ceph-FileSystem-provisioner的PODapiVersion: extensions/v1beta1 kind: Deployment metadata: name: cephfs-provisioner namespace: cephfs spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: cephfs-provisioner spec: containers: - name: cephfs-provisioner image: \"quay.io/external_storage/cephfs-provisioner:latest\" env: - name: PROVISIONER_NAME value: ceph.com/cephfs - name: PROVISIONER_SECRET_NAMESPACE value: cephfs command: - \"/usr/local/bin/cephfs-provisioner\" args: - \"-id=cephfs-provisioner-1\" serviceAccount: cephfs-provisioner 二、使用1. 创建StorageClasskind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: cephfs provisioner: ceph.com/cephfs parameters: monitors: allinone.okd311.curiouser.com:6789 adminId: admin adminSecretName: cephfs-secret-admin adminSecretNamespace: \"cephfs\" claimRoot: /pvc-volumes 2、创建PVC时使用kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cephfs-test spec: storageClassName: cephfs accessModes: - ReadWriteMany resources: requests: storage: 1Gi Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:07 "},"origin/openshift-Kubernetes-provisioner-cephrbd.html":{"url":"origin/openshift-Kubernetes-provisioner-cephrbd.html","title":"Ceph RBD Provisioner","keywords":"","body":"一、获取ceph client admin用户的密钥环keyring查看Ceph集群Admin节点的集群配置文件夹my-cluster下的ceph.client.admin.keyring文件来获取key值$> cat ceph.client.admin.keyring [client.admin] key = AQBUilha86ufLhAA2BxJn7sG8qVYndokVwtvyA== caps mds = \"allow *\" caps mon = \"allow *\" caps osd = \"allow *\" ​ $ ceph auth list #获取所有客户端用户 $ ceph auth get client.admin #获取客户端指定用户 二、使用admin的keyring在openshift上创建secretCLI$> oc create secret generic ceph-secret --type=\"kubernetes.io/rbd\" --from-literal=key='AQAil11anEPOORAArxzRkH9iS1IOGKQfK87+Ag==' --namespace=default YAMLkind: Secret apiVersion: v1 metadata: name: ceph-secret namespace: default selfLink: /api/v1/namespaces/default/secrets/ceph-secret data: key: QVFDcFNlMWJ0Y3VxSFJBQWlST25zY1VDMWpnTWRwZkRJMFd0THc9PQ== type: kubernetes.io/rbd 三、创建storageclassapiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-rbd-sc provisioner: kubernetes.io/rbd parameters: monitors: 192.168.0.26:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: default pool: rbd userId: admin userSecretName: ceph-secret #说明:adminId默认值为admin,pool默认值为rbd, userId默认值与adminId一样.所以这三个值可以不填写。 四、可以在console界面创建，也可以通过PVC的YAML配置文件中指定使用Ceph$> cat ceph-rbd-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: myclaim spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: ceph-rbd-sc 结果如下图：Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:08 "},"origin/openshift-资源对象常见操作.html":{"url":"origin/openshift-资源对象常见操作.html","title":"常见资源对象操作","keywords":"","body":"常用资源对象操作1、登录oc project oc login -u 用户名 集群master的URL oc whoami #查看当前登录的用户，加-t参数可查看当前用户的token 2、切换Projectoc project 3、查看集群节点oc get node/no oc get node/no node1.test.openshift.com 4、查看集群节点的详细信息oc describe node node1.test.openshift.com 5、查看某个节点上的所有Podsoc adm manage-node node1.test.openshift.com --list-pods 6、使节点禁止参与调度oc adm manage-node router1.test.openshift.com --schedulable=false 7、疏散某个节点上的所有PODoc adm drain router1.test.openshift.com --ignore-daemonsets 8、清除旧的Build和Deployments历史版（所有namespace）统计要清除的资源个数 #oc adm prune deployments --keep-younger-than=24h --keep-complete=5 --keep-failed=5|wc -l 确认清除动作 # oc adm prune [deployments|builds|images] --confirm --keep-younger-than=24h --keep-complete=5 --keep-failed=5 参数详解 --confirm 确认操作 --keep-younger-than=1h0m0s Specify the minimum age of a Build for it to be considered a candidate for pruning. --keep-complete=5 Per BuildConfig, specify the number of builds whose status is complete that will be preserved. --keep-failed=1 Per BuildConfig, specify the number of builds whose status is failed, error, or cancelled that will be preserved. --orphans=false If true, prune all builds whose associated BuildConfig no longer exists and whose status is complete, failed, error, or cancelled. 示例： 清理images（在admin用户下执行） # oc adm prune images --keep-younger-than=400m --keep-tag-revisions=10 --registry-url=docker-registry.default.svc:5000 --certificate-authority=/etc/origin/master/registry.crt --confirm 9、删除所有Namespace中非Running的podsfor i in `oc get po --all-namespaces|grep -v \"Running\"|grep -v \"NAMESPACE\"|awk '{print $1}'|sort -u` ; do echo \"===================Namespace $i===================\"; oc -n $i delete po `oc get po -n $i |grep -v \"Running\"|grep -v \"NAME\"|awk 'BEGIN{ORS=\" \"}{print $1}'`; done 10、强制删除PODoc delete po gitlab-ce-16-ntzst --force --grace-period=0 11、资源的查看#查看当前项目的所有资源 oc get all #查看当前项目的所有资源，外加输出label信息 oc get all --show-labels # 查看指定资源 oc get pod/po oc get service/svc oc get persistentvolumes/pv 12、通过label选择器删除namespace下所有的资源#如果namespace下所有的资源都打上了“name=test”标签 oc delete all -l name=test 13、项目的管理#创建项目 oc new-project --display-name=显示的项目名 --description=项目描述 project_name #删除项目 oc delete project 项目名 #查看当前处于哪个项目下 oc project #查看所有项目 oc projects 14、模板的管理#创建模板(模板文件格式为YAML/JSON.也可以在Openshift的web页面上直接导入) oc create -f #查看模板 oc get templates #编辑模板 oc edit template #删除模板 oc delete template 附录buildconfigs (aka 'bc') #构建配置 builds #构建版本 certificatesigningrequests (aka 'csr') clusters (valid only for federation apiservers) clusterrolebindings clusterroles componentstatuses (aka 'cs') configmaps (aka 'cm') daemonsets (aka 'ds') deployments (aka 'deploy') deploymentconfigs (aka 'dc') endpoints (aka 'ep') events (aka 'ev') horizontalpodautoscalers (aka 'hpa') imagestreamimages (aka 'isimage') imagestreams (aka 'is') imagestreamtags (aka 'istag') ingresses (aka 'ing') groups jobs limitranges (aka 'limits') namespaces (aka 'ns') networkpolicies nodes (aka 'no') persistentvolumeclaims (aka 'pvc') persistentvolumes (aka 'pv') poddisruptionbudgets (aka 'pdb') podpreset pods (aka 'po') podsecuritypolicies (aka 'psp') podtemplates policies projects replicasets (aka 'rs') replicationcontrollers (aka 'rc') resourcequotas (aka 'quota') rolebindings roles routes secrets serviceaccounts (aka 'sa') services (aka 'svc') statefulsets users storageclasses thirdpartyresources Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:11 "},"origin/openshift-将Secret和ConfigMap以文件的形式挂载到容器.html":{"url":"origin/openshift-将Secret和ConfigMap以文件的形式挂载到容器.html","title":"将Secret和ConfigMap以文件的形式挂载到容器","keywords":"","body":"将Secret和ConfigMap以文件的形式挂载到容器一、ContextConfigMap或者Secret在默认挂载到容器是以Volumes的形式，如果挂载路径下原有的其他文件，则会覆盖掉。 如果将挂载路径直接写成文件的绝对路径，这会在挂载路径下创建以文件名为名字的文件夹，文件会在这个文件夹下containers: - image: 'busybox:latest' name: test volumeMounts: - mountPath: /etc/test/test.txt name: test-volume volumes: - name: test-volume secret: defaultMode: 420 secretName: test-secret 二、操作挂载Secret或Config类型的volume时，添加一个subPath字段即可，可将其以文件的形式挂载，而不是以目录的形式。如下：containers: - image: 'busybox:latest' name: test volumeMounts: - mountPath: /etc/test/test.txt name: test-volume readOnly: true subPath: test.txt volumes: - name: test-volume secret: defaultMode: 420 secretName: test-secret secretapiVersion: v1 kind: Secret metadata: name:test-secret type: Opaque data: test.txt: >- ************************ 此时，secret中的test.txt文件将会单个文件的形式挂载到/etc/test/目录下Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:11 "},"origin/openshift-集群节点管理.html":{"url":"origin/openshift-集群节点管理.html","title":"节点管理","keywords":"","body":"一、集群添加Node节点Ansible脚本有新增节点的Playbook脚本，准备好新增节点的基础环境，在集群的ansible管理节点上执行该Playbook就行。Context OKD版本 OS版本 Docker版本 Ansible版本 3.11 CentOS 7.5.1804 1.13.1 2.6.5 1. 新增节点Prerequisite新增node节点IP地址及主机名：192.168.1.23 node6.okd.curiouser.com开启seLinux sed -i \"s/SELINUX=disabled/SELINUX=enforcing/\" /etc/sysconfig/selinux && \\ setenforce 1 安装docker，jdk及基础软件yum install -y docker vim lrzsz wget unzip net-tools telnet bind-utils && \\ systemctl enable docker && \\ systemctl start docker && \\ systemctl status docker && \\ yum localinstall -y jdk-8u191-linux-x64.rpm && \\ docker info && \\ java -version 配置DNS，发现集群其他节点的IP地址与域名的映射关系.(注意DNSMasq服务端的iptables是否放行DNS的53 UDP端口) 由于集群内有DNSMasq服务端，配置/etc/resolv.confecho \"nameserver 192.168.1.22\" >> /etc/resolv.conf && \\ ping allinone311.okd.curiouser.com Note:#DNSMasq服务端放行DNS的53 UDP端口 iptables -I OS_FIREWALL_ALLOW -p udp -m udp --dport 53 -j ACCEPT && \\ iptables-save 配置Openshift的YUM源 mkdir /etc/yum.repos.d/bak && \\ mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak && \\ scp allinone311.okd.curiouser.com:/etc/yum.repos.d/all.repo /etc/yum.repos.d/ && \\ yum clean all && \\ yum makecache 2. ansible管理节点打通ansible管理节点到新增node节点的SSH免密通道ssh-copy-id -i root@node6.okd.curiouser.com && \\ ssh root@node1.okd.curiouser.com ansible管理节点的ansible主机清单文件inventory中添加新增节点相关信息[OSEv3:children] ... new_nodes [new_nodes] node1.okd.curiouser.com openshift_node_group_name=\"node-config-all-in-one\" ansible管理节点执行新增节点的Ansible Playbook ansible-playbook /root/openshift-ansible/playbooks/openshift-node/scaleup.yml 注意1：当执行脚本时tower主机会把它的dnsmasq配置/etc/dnsmasq.d/origin-upstream-dns.conf同步到新增节点/etc/dnsmasq.d/路径下。由于tower主机的/etc/dnsmasq.d/origin-upstream-dns.conf设置的上游DNS服务器为外网的。不希望新增节点的上游DNS服务器走外网，而是走tower主机，形成集群只有Tower主机一个节点的dns对外，其他主机作为Tower主机dns服务的客户端。所以当tower主机/etc/dnsmasq.d/origin-upstream-dns.conf同步到新增节点/etc/dnsmasq.d/路径下的时候，及时修改上游dns服务器为tower主机。然后重启dnsmasq。有两个明显的坑:① 无法重启dnsmasq，报以下错误：DBus error: Connection \":1.50\" is not allowed to own the service \"uk.org.thekelleys.dnsmasq\" due to security policies in the configuration file 解决方案：重启dbus，再重启dnsmasqsystemctl restart dbus && \\ systemctl restart dnsmasq ②tower主机的iptables服务开启，dns的53端口没有放开，导致新增节点的dns无法连接上游dns服务器（即Tower主机的dns服务） 解决方案：tower主机放行dns服务的UDP 53端口。（可在新增节点尝试nslookup解析域名试一下）iptables -I OS_FIREWALL_ALLOW -p udp -m udp --dport 53 -j ACCEPT && \\ iptables-save 注意2：如果出现收集allinone节点facts超时的报错，出现一下错误提示 The full traceback is: Traceback (most recent call last): File \"/tmp/ansible_d9POp0/ansible_modlib.zip/ansible/module_utils/basic.py\", line 2853, in run_command cmd = subprocess.Popen(args, **kwargs) File \"/usr/lib64/python2.7/subprocess.py\", line 711, in __init__ errread, errwrite) File \"/usr/lib64/python2.7/subprocess.py\", line 1308, in _execute_child data = _eintr_retry_call(os.read, errpipe_read, 1048576) File \"/usr/lib64/python2.7/subprocess.py\", line 478, in _eintr_retry_call return func(*args) File \"/tmp/ansible_d9POp0/ansible_modlib.zip/ansible/module_utils/facts/timeout.py\", line 37, in _handle_timeout raise TimeoutError(msg) TimeoutError: Timer expired after 10 seconds TimeoutError: Timer expired after 10 seconds 请在/etc/ansible/ansible.cfg 设置\"gather_subset = !all\"或者\"gather_timeout=300\"。原因可能是已经运行allinone节点上的facts（特别是docker images layer的挂载信息）过多，造成收集facts超时，默认收集facts超时时间是10。相关连接：https://github.com/ansible/ansible/issues/43884二、删除节点疏散要删除节点上的POD oc adm drain [--pod-selector=] --force=true --grace-period=-1 --timeout=5s --delete-local-data=true 删除Node oc delete node Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:11 "},"origin/openshift-使用Cockpit监控集群节点的系统状态.html":{"url":"origin/openshift-使用Cockpit监控集群节点的系统状态.html","title":"节点状态监控","keywords":"","body":"一、Cockpit简介Cockpit 是一个自由开源的服务器管理软件，它使得我们可以通过它好看的 web 前端界面轻松地管理我们的 GNU/Linux 服务器。Cockpit 使得 linux 系统管理员、系统维护员和开发者能轻松地管理他们的服务器并执行一些简单的任务，例如管理存储、检测日志、启动或停止服务以及一些其它任务。它的报告界面添加了一些很好的功能使得可以轻松地在终端和 web 界面之间切换。另外，它不仅使得管理一台服务器变得简单，更重要的是只需要一个单击就可以在一个地方同时管理多个通过网络连接的服务器。它非常轻量级，web 界面也非常简单易用。在这篇博文中，我们会学习如何安装 Cockpit 并用它管理我们的运行着 Fedora、CentOS、Arch Linux 以及 RHEL 发行版操作系统的服务器。下面是 Cockpit 在我们的 GNU/Linux 服务器中一些非常棒的功能：它包含 systemd 服务管理器。 有一个用于故障排除和日志分析的 Journal 日志查看器。 包括 LVM 在内的存储配置比以前任何时候都要简单。 用 Cockpit 可以进行基本的网络配置。 可以轻松地添加和删除用户以及管理多台服务器。 二、Cockpit安装所有集群节点安装cockpit并启动服务 yum install -y cockpit cockpit-docker cockpit-kubernetes ;\\ systemctl enable cockpit ;\\ systemctl start cockpit ;\\ netstat -lanp |grep 9090 iptables放行端口 vi /etc/sysconfig/iptables #-A INPUT -p tcp -m state --state NEW -m tcp --dport 9090 -j ACCEPT systemctl restart iptables Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:11 "},"origin/openshift-集群组件TLS证书管理.html":{"url":"origin/openshift-集群组件TLS证书管理.html","title":"集群组件TLS证书管理","keywords":"","body":"Openshift组件:Master、Node、Etcd、Router、Registry之间的TLS证书管理一、安装时指定证书的有效期默认情况下，etcd证书、openshift证书的有效期为5年，kubelet证书、私有镜像仓库registry证书、Route证书的有效期为2年。在集群安装时可以通过设置ansible/hosts中的参数来指定证书的有效期[OSEv3:vars] openshift_hosted_registry_cert_expire_days=730 openshift_ca_cert_expire_days=1825 openshift_node_cert_expire_days=730 openshift_master_cert_expire_days=730 etcd_ca_default_days=1825 二、使用openshift的ansible playbook查看当前集群所有证书的有效期在/etc/ansible/hosts中添加变量[OSEv3:vars] ... openshift_is_atomic=false ansible_distribution=centos openshift_certificate_expiry_config_base=/etc/origin openshift_certificate_expiry_warning_days=30 openshift_certificate_expiry_show_all=no # 可选项 # openshift_certificate_expiry_generate_html_report=no # openshift_certificate_expiry_html_report_path=$HOME/cert-expiry-report.yyyymmddTHHMMSS.html # openshift_certificate_expiry_save_json_results=no # openshift_certificate_expiry_json_results_path=$HOME/cert-expiry-report.yyyymmddTHHMMSS.json ... 检查$ ansible-playbook playbooks/openshift-checks/certificate_expiry/easy-mode.yaml #执行完成后可在roles/openshift_certificate_expiry/defaults/main.yml中的openshift_certificate_expiry_html_report_path变量指定路径下看到证书检查报告文件。分别是HTML格式和JSON格式的文件。 # （默认证书检查报告文件路径是：当前用户家目录下~/cert-expiry-report.时间戳.html和cert-expiry-report.时间戳.JSON）查看所有证书的过期时间 它将会展示出所有Master oc证书、etcd证书、kube证书、router默认证书、私有镜像仓库registry证书的过期时间三、更新证书更新证书方法可以只针对Master oc证书、etcd证书、kube证书、router默认证书、私有镜像仓库registry证书中的一种进行更新，也可以全部进行更新。确保ansible/hosts中的参数有如下信息 openshift_master_cluster_hostname=master.example.com openshift_master_cluster_public_hostname=master.example.com 重新生成证书进行更新 ①全部一次性更新 ansible-playbook playbooks/redeploy-certificates.yml ②只更新master CA证书 ansible-playbook playbooks/openshift-master/redeploy-openshift-ca.yml ③只更新etcd CA证书 ansible-playbook playbooks/openshift-etcd/redeploy-ca.yml ④只更新master Certificates证书 ansible-playbook playbooks/openshift-master/redeploy-certificates.yml ⑤只更新etcd Certificates证书 ansible-playbook playbooks/openshift-etcd/redeploy-certificates.yml ⑥只更新node Certificates证书 ansible-playbook playbooks/openshift-node/redeploy-certificates.yml ⑦只更新私有镜像仓库Rgistry Certificates证书 ansible-playbook playbooks/openshift-hosted/redeploy-registry-certificates.yml ⑧只更新Router Certificates证书 ansible-playbook playbooks/openshift-hosted/redeploy-router-certificates.yml 四、安装时使用自定义Master CA证书（以Master的CA证书为例）将证书的路径写在inventory的配置参数中... [OSEv3.vars] ... openshift_master_ca_certificate={'certfile': '', 'keyfile': ''} ... 执行正常部署ansible-playbook playbooks/deploy_cluster.yml 五、已运行的集群，更新自定义证书同步骤四，将证书的路径写在inventory的配置参数中，运行更新Master CA证书的playbookansible-playbook playbooks/openshift-master/redeploy-openshift-ca.yml 六、更新完成后可能遇到的问题The installer detected the wrong host names and the issue was identified too late The certificates are expired and you need to update them You have a new CA and want to create certificates using it instead allinone的集群下更新所有证书时，在重启docker那一步中，容易卡住 参考连接https://docs.openshift.com/container-platform/3.11/install/configuring_inventory_file.html#advanced-install-custom-certificates https://docs.openshift.com/container-platform/3.11/install_config/redeploying_certificates.html#install-config-cert-expiry https://www.jianshu.com/p/ffc4d6369d4e Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:11 "},"origin/openshift-WebConsole定制化.html":{"url":"origin/openshift-WebConsole定制化.html","title":"定制WebConsole界面","keywords":"","body":"一、定制WebConsole中左上角的logo制作图标 使用Windows 10自带的Paint 3D。制作高度40pixel，宽度为logo字体宽的透明画布（建议logo字体宽度为100-300pixel之间）。保存为PNG格式。 将PNG图片转成SVG格式http://www.bejson.com/convert/image_to_svg/ 将SVG文件进行Base64加密https://www.css-js.com/tools/base64.html 将下面CSS文件上传到一个HTTPS的静态资源服务器上 #header-logo { background-image: url('data:image/svg+xml;base64,base64加过密的SVG图片源码'); width: 230px; height: 40px; } # 参考 #header-logo{ background-image:url('data:image/svg+xml;base64,77u/PD******'); width: 230px; height: 40px; } 或者 #header-logo{ background-image: url(\"logo图片的访问UTRL（必须是HTTPS）\"); width: 300px; height: 40px; } 修改WebConsole的配置文件 待Webconsole的容器重启过后（等待约5分钟），再次刷新页面可见修改过后的效果。可使用F12调出浏览器开发者模式，查看页面渲染的元素。 二、汉化项目左侧导航栏创建js(function() { window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[0].label=\"概览\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[1].label=\"应用\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[2].label=\"构建\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[3].label=\"资源\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[4].label=\"存储\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[5].label=\"监控\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[6].label=\"商店\" window.OPENSHIFT_CONSTANTS.APP_LAUNCHER_NAVIGATION = [ { title: \"Sharing Videos\", iconClass: \"fa fa-video-camera\", href: \"https://yun.baidu.com/s/1xIwYILHQebEHZOcW4yvsAw\", tooltip: \"一键部署Openshift相关视频\" }]; }()); 上传到https服务器上修改WebConsole的配置文件三、定制登陆页面导出login模板文件 oc adm create-login-template > login.html 修改该HTML文件，然后放到master节点上的/etc/origin/master/login-template/路径下（示例可见附件） 修改Master节点的/etc/origin/master/master-config.yaml文件 oauthConfig: ... templates: login: login-template/login.html #login-template/login.html是相对于/etc/origin/master/master-config.yaml文件路径的相对位置 重启master节点上的OKD的api进程 # 使用okd3.11新命令：master-restart master-restart api Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:10 "},"origin/openshift-no-IP-addresses-available-in-range-set解决方案.html":{"url":"origin/openshift-no-IP-addresses-available-in-range-set解决方案.html","title":"集群管理遇到的问题","keywords":"","body":"问题一描述正在运行的Openshift Allinone 3.11集群创建POD突然报出 network: failed to allocate for range 0: no IP addresses available in range set 现象整个Allinone集群所有的POD不超多100个，但是/var/lib/cni/networks/openshift-sdn/的IP地址文件却有252个。造成当前节点的容器网络无法再为POD分配IP地址解决方案将对应节点标记为不可调用 oc adm manage-node node1.test.openshift.com --schedulable=false 驱散对应节点上的POD oc adm drain node1.test.openshift.com --ignore-daemonsets 停止docker和origin-node服务 systemctl stop docker origin-node.service 删除/var/lib/cni/networks/openshift-sdn/路径下所有文件 rm -rf /var/lib/cni/networks/openshift-sdn/* 重启docker和origin-node服务 systemctl start docker origin-node.service 将节点标记为可调度 oc adm manage-node node1.test.openshift.com --schedulable=true 相关链接https://access.redhat.com/solutions/3328541 https://github.com/debianmaster/openshift-examples/issues/59 https://github.com/cloudnativelabs/kube-router/issues/383 https://github.com/jsenon/api-cni-cleanup/blob/master/k8s/deployment.yml#L42Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:10 "},"origin/openshift-openshift的用户认证.html":{"url":"origin/openshift-openshift的用户认证.html","title":"用户认证","keywords":"","body":"一、用户认证Openshift通过OAuth进行用户的认证。在Openshift的master节点上运行着一个内置的OAuth服务对用户的请求进行认证检查。一旦OAuth服务器通过登录信息确认了用户的信息，OAuth服务器就返回用户的访问Token。通过这个Token，用户可以在有效的时间内对系统进行访问。#登录命令 $ oc login -u 用户名 ​ #查看以哪个用户登录的 $ oc whoami $ oc whoami -t 查看当前用户当前Session的Token ​ #system:admin是集群默认的管理员，该用户是一个特殊用户，它不能通过用户名密码登录，它也没有Token。 作为身份验证的登录信息，如用户名密码，并非保存在Openshift集群中，而是保存在用户信息管理系统中，这些用户信息管理系统在Openshift中被称为Identity Provider。但Openshift并不提供用户信息管理系统，而是提供了不同的适配器连接不同的用户信息管理系统。通过配置，Openshift可以连接到以下用户信息管理系统：LADP（Lightweight Directory Access Protocol） 微软的活动目录（Active Directory） AllowALL DenyAll HTPasswd Github #查看当前Openshift集群支持的用户信息管理系统 cat /etc/origin/master/master-config.yaml|grep provider -A 3 provider: apiVersion: v1 file: /etc/origin/master/htpasswd kind: HTPasswdPasswordIdentityProvider #Htpasswd是Apache提供的一个基于文本文件管理用户名密码的用户信息管理工具 Openshift的用户管理，在后台创建用户时，会同时创建一个User对象和Identity对象（该对象保存了用户来源哪一个Identity Provider及用户信息）。#查看集群中所有用户 $oc get user NAME UID FULL NAME IDENTITIES admin a04e0467-c8e7-11e7-b9d9-5254ac31d0ec htpasswd_auth:admin dev 1ffbda60-cb72-11e7-bd9b-5254c1caedf4 htpasswd_auth:dev #查看用户的Identity对象 $oc get identity NAME IDP NAME IDP USER NAME USER NAME USER UID htpasswd_auth:admin htpasswd_auth admin admin a04e0467-c8e7-11e7-b9d9-5254ac31d0ec htpasswd_auth:dev htpasswd_auth dev dev 1ffbda60-cb72-11e7-bd9b-5254c1caedf4 Openshift的用户组管理。用户组的信息来源有两个：一个是Identity Provider，二是通过用户在Openshift中定义的。 #通过oadm groups命令在Openshift中对组及组成员进行管理 $> oadm groups #添加用户到用户组 $> oadm groups add-users group_name user_name #查看用户组 $> oc get group #创建用户组 $> oadm groups new group_name #删除组 $> oc delete group group_name 二、用户权限管理用户角色权限管理 1. 授予及撤销用户某种角色 oc policy add-role-to-user view test oc policy remove-role-from-user view test 查看项目的角色绑定关系oc get rolebinding -n 项目名 授予某用户对某项目的某角色oc policy add-role-to-user view test -n test 查看角色绑定的规则oc describe clusterrole registry-viewer 用户管理 1. 新增用户 ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd test test\" 查看已创建的用户oc get user 或 cat /etc/origin/master/htpasswd 删除用户oc delete user test ansible masters -m shell -a \"htpasswd -D /etc/origin/master/htpasswd ha\" 用户组管理 1. 创建用户组、添加用户到用户组 oc adm groups new test oc adm groups add-users test 用户1 用户2 用户3 查看创建的用户组及组内的成员用户oc get group 删除用户组oc delete group test Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:10 "},"origin/openshift-openshift用户权限管理实例.html":{"url":"origin/openshift-openshift用户权限管理实例.html","title":"用户权限管理实例","keywords":"","body":"Openshift用户权限管理实例由于公司的日常项目开发测试环境都迁移到openshift上了。有众多开发测试人员需要登陆到openshift上进行操作，如果直接给admin权限，肯定是不行的。而openshift是支持多租户的权限管理。所以，就在创建普通用户的基础上赋予各种不同的权限限制来自控制对openshift上project的操作。一、Prerequisite开发人员对CI环境有操作权限，对SIT、UAT环境只有查看权限 测试人员对SIT环境有操作权限，对CI环境只有查看权限 所有人员有自己的登录账户，均可见openshift上所有的业务项目，不可见系统项目 二、实现过程创建登录用户 ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd dev1 dev1\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd dev2 dev2\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd dev3 dev3\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd tester1 tester1\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd tester2 tester2\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd tester3 tester3\" 创建用户组 oc adm groups new developer oc adm groups new tester 将用户添加到用户组中 oc adm groups add-users developer dev1 dev2 dev3 oc adm groups add-users tester tester1 tester2 tester3 针对项目，给用户组赋予系统角色 oc adm policy add-role-to-group edit developer -n aci oc adm policy add-role-to-group edit developer -n bci oc adm policy add-role-to-group edit developer -n cci oc adm policy add-role-to-group view developer -n asit oc adm policy add-role-to-group view developer -n auat oc adm policy add-role-to-group view developer -n bsit oc adm policy add-role-to-group view developer -n buat oc adm policy add-role-to-group view developer -n csit oc adm policy add-role-to-group view developer -n cuat ​ oc adm policy add-role-to-group edit tester -n asit oc adm policy add-role-to-group edit tester -n auat oc adm policy add-role-to-group edit tester -n bsit oc adm policy add-role-to-group edit tester -n buat oc adm policy add-role-to-group edit tester -n csit oc adm policy add-role-to-group edit tester -n cuat oc adm policy add-role-to-group view tester -n aci oc adm policy add-role-to-group view tester -n bci oc adm policy add-role-to-group view tester -n cci 实际操作过程中，在以某以开发人员登录过程openshift过程中，依旧会看到openshift 其他一些项目的namespace。例如base namespace，该namespace项目是在registry镜像注册仓库中创建镜像项目时自动创建的openshift namespace（在registry镜像注册仓库中创建base镜像项目是为了存放一些自定义的s2i镜像）。为了使其他openshift namespace使用其中的s2i镜像，特别在registry镜像注册仓库中是镜像项目的访问策略设置为共享的。种种以上，导致openshift上的base namespace是能被所有的已认证的用户查看到。在openshift中查看base项目的membership可以发现，凡是在registry镜像注册仓库中设置问访问策略设置为共享的，都会在openshift 项目中添加一个系统用户system:authenticated 。这个系统用户上绑定的是这个角色registry-viewer。在openshift后台查看该角色的详细信息 # oc describe clusterrole registry-viewer Name: registry-viewer Created: About an hour ago Labels: Annotations: authorization.openshift.io/system-only=true openshift.io/reconcile-protect=false Verbs Non-Resource URLs Resource Names API Groups Resources [get list watch] [] [] [ image.openshift.io] [imagestreamimages imagestreammappings imagestreams imagestreamtags] [get] [] [] [ image.openshift.io] [imagestreams/layers] [get] [] [] [] [namespaces] [get] [] [] [project.openshift.io ] [projects] 发现该角色有对namespace资源拥有get动作。仔细想想，该system:authenticated用户只是让openshift其他项目的系统用户能够拉取其下的镜像流。而在Kubernetes中使用命名空间的概念来分隔资源。在同一个命名空间中，某一个对象的名称在其分类中必须唯一，但是分布在不同命名空间中的对象则可以同名。OpenShift中继承了Kubernetes命名空间的概念，而且在其之上定义了Project对象的概念。每一个Project会和一个Namespace相关联，甚至可以简单地认为，Project就是Namespace。所以，该用户对project资源有获取权限，那就把对namespace的权限给去掉试试。 先导出角色registry-viewer的bindding配置文件oc export clusterrole registry-viewer > registry-viewer.yml 然后修改配置文件，注释掉get namespace的动作apiVersion: v1 kind: ClusterRole metadata: annotations: authorization.openshift.io/system-only: \"true\" openshift.io/reconcile-protect: \"false\" creationTimestamp: null name: registry-viewer rules: - apiGroups: - \"\" - image.openshift.io attributeRestrictions: null resources: - imagestreamimages - imagestreammappings - imagestreams - imagestreamtags verbs: - get - list - watch - apiGroups: - \"\" - image.openshift.io attributeRestrictions: null resources: - imagestreams/layers verbs: - get #- apiGroups: # - \"\" # attributeRestrictions: null # resources: # - namespaces # verbs: # - get - apiGroups: - project.openshift.io - \"\" attributeRestrictions: null resources: - projects verbs: - get 再将集群中角色删掉（此时特别注意:从集群中删掉registry-viewer角色后会导致已有镜像注册仓库中镜像的访问策略从共有变成私有,base项目的membership中会删掉system:authenticated该用户）oc delete clusterrole registry-viewer 接着再从配置文件中创建角色oc create -f registry-viewer.yml 最后再次修改镜像注册仓库中镜像的访问策略从私有变成共有。再次查看base项目中membership.最有再以测试人员账户登录查看。不再显示base项目。测试其他项目去拉取base项目中的镜像，看去掉registry-viewer角色中role是否有影响。实际使用过程中，测试人员需要以openshift上的用户名密码登录openshift上的jenkins，还要对jenkins做操作，比如在jenkins上做构建操作，查看构建日志等。需要对测试人员分组tester赋予对jenkins的编辑权限。初步思路是直接给tester分组服务系统角色clusterrole edit（oc adm policy add-cluster-role-to-group edit tester）。但是再以测试人员登录时还是能看到jenkins的项目，甚至能操作openshift上jenkins pod的重新部署。这是不可接受的。那就换个思路。自己创建一个集群角色clusterrole，在角色上绑定若干规则，再将这个集群角色赋予测试组，相应的测试组成员能登录jenkins，并对jenkins做操作。 具体过程如下： 1. 先查看集群角色edit的配置，看edit都对那些资源都有什么动作 oc describe clusterrole edit ​ Name: edit Created: 7 months ago Labels: Annotations: openshift.io/description=A user that can create and edit most objects in a project, but can not update the project's membership. Verbs Non-Resource URLs Resource Names API Groups Resources [create delete deletecollection get list patch update watch] [] [] [] [pods pods/attach pods/exec pods/portforward pods/proxy] [create delete deletecollection get list patch update watch] [] [] [] [configmaps endpoints persistentvolumeclaims replicationcontrollers replicationcontrollers/scale secrets serviceaccounts services services/proxy] [get list watch] [] [] [] [bindings events limitranges namespaces namespaces/status pods/log pods/status replicationcontrollers/status resourcequotas resourcequotas/status] [impersonate] [] [] [] [serviceaccounts] [create delete deletecollection get list patch update watch] [] [] [autoscaling] [horizontalpodautoscalers] [create delete deletecollection get list patch update watch] [] [] [batch] [cronjobs jobs scheduledjobs] [create delete deletecollection get list patch update watch] [] [] [extensions] [deployments deployments/rollback deployments/scale horizontalpodautoscalers jobs replicasets replicasets/scale replicationcontrollers/scale] [get list watch] [] [] [extensions] [daemonsets] [create delete deletecollection get list patch update watch] [] [] [apps] [deployments deployments/scale deployments/status statefulsets] [create delete deletecollection get list patch update watch] [] [] [build.openshift.io ] [buildconfigs buildconfigs/webhooks builds] [get list watch] [] [] [build.openshift.io ] [builds/log] [create] [] [] [build.openshift.io ] [buildconfigs/instantiate buildconfigs/instantiatebinary builds/clone] [update] [] [] [build.openshift.io ] [builds/details] [edit view] [] [] [build.openshift.io] [jenkins] [create delete deletecollection get list patch update watch] [] [] [apps.openshift.io ] [deploymentconfigs deploymentconfigs/scale generatedeploymentconfigs] [create] [] [] [apps.openshift.io ] [deploymentconfigrollbacks deploymentconfigs/instantiate deploymentconfigs/rollback] [get list watch] [] [] [apps.openshift.io ] [deploymentconfigs/log deploymentconfigs/status] [create delete deletecollection get list patch update watch] [] [] [image.openshift.io ] [imagestreamimages imagestreammappings imagestreams imagestreams/secrets imagestreamtags] [get list watch] [] [] [image.openshift.io ] [imagestreams/status] [get update] [] [] [image.openshift.io ] [imagestreams/layers] [create] [] [] [image.openshift.io ] [imagestreamimports] [get] [] [] [project.openshift.io ] [projects] [get list watch] [] [] [quota.openshift.io ] [appliedclusterresourcequotas] [create delete deletecollection get list patch update watch] [] [] [route.openshift.io ] [routes] [create] [] [] [route.openshift.io ] [routes/custom-host] [get list watch] [] [] [route.openshift.io ] [routes/status] [create delete deletecollection get list patch update watch] [] [] [template.openshift.io ] [processedtemplates templateconfigs templateinstances templates] [create delete deletecollection get list patch update watch] [] [] [build.openshift.io ] [buildlogs] [get list watch] [] [] [] [resourcequotausages] 导出集群角色edit的配置文件到本地文件，在其上做修改 oc export clusterrole edit > jenkins-clusterrole.yml 编辑 jenkins-clusterrole.yml（只保留相重要的，其他的都删掉） apiVersion: v1 kind: ClusterRole metadata: annotations: openshift.io/description: A user that can view jenkins project, and edit jenkins job. #添加clusterrole角色的说明简介 creationTimestamp: null name: jenkins #修改clusterrole名字为jenkins rules: - apiGroups: - \"\" attributeRestrictions: null resources: #clusterrole edit中有好多对其他资源的操作。例persistentvolumeclaims、replicationcontrollers、replicationcontrollersscale。对这些资源没有什么用处，就可以删掉啦。 - configmaps - endpoints - secrets - serviceaccounts - services - services/proxy verbs: - get - list - apiGroups: - \"\" attributeRestrictions: null resources: - serviceaccounts verbs: - impersonate - apiGroups: - build.openshift.io attributeRestrictions: null resources: - jenkins verbs: - edit - view 导入jenkins clusterrole oc create -f jenkins-clusterrole.yml 将jenkins clusterrole赋予测试组 oc adm policy add-cluster-role-to-group jenkins tester 以测试组成员登录openshift，jenkins项目不可见了。再以测试组成员登录jenkins，发现登录 出现以下界面点击\"Allow selected permissions\"，发现也能正常登录jenkins。然后进行一次构建触发。发现一切正常。Bazinga！Everything is ok ! Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:10 "},"origin/openshift-开启router的haproxy-statisc.html":{"url":"origin/openshift-开启router的haproxy-statisc.html","title":"openshift开启router的haproxy-statisc","keywords":"","body":"设置router POD 所在节点的iptables对1936端口的放行 iptables -I OS_FIREWALL_ALLOW -p tcp -m tcp --dport 1936 -j ACCEPT 获取访问router haproxy statics 页面的用户名密码。 删除掉router dc中的环境变量”ROUTER_METRICS_TYPE“这个环境变量默认值为“haproxy”。不删除的话，访问的时候会报一下错误 Forbidden: User \"system:anonymous\" cannot get routers/metrics.route.openshift.io at the cluster scope 将健康检查readiness的HTTP GET URL由“/healthz/ready”改为\"/healthz\"。（不然router POD无法通过健康检查） 验证监听端口80，443，1936 ss -ntl|grep 80 ss -ntl|grep 443 ss -ntl|grep 1936 访问router haproxy statistics 页面。 访问方式是：http://:@router所在节点IP地址:1936 例如：http://admin:MJbJFvODhP@allinone.curiouser.com:1936 相关链接https://docs.openshift.com/container-platform/3.11/install_config/router/default_haproxy_router.html#using-wildcard-routes https://bugzilla.redhat.com/show_bug.cgi?id=1579054 https://github.com/openshift/origin/issues/17025 https://blog.chmouel.com/2016/09/27/how-to-view-openshift-haproxy-stats/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:11 "},"origin/openshift-多租户网络.html":{"url":"origin/openshift-多租户网络.html","title":"openshift的多租户网络","keywords":"","body":"一、Openshift容器网络简介Openshift容器网络默认是基于Open vSwitch（OVS）实现的。Openshift提供两种网络方案：ovs-subnet(子网模式)：为集群节点上的容器提供一个扁平化的二层虚拟网路，所有在这个二层网路中容器可直接通信。 ovs-multitenet(多租户模式)：基于项目的网络隔离，即不同项目间的容器之间不能直接通信。启动多租户网络隔离后，每个项目创建后都会被分配一个虚拟网络ID（Virtual Network ID ,VNID）.OVS网桥会为该项目的所有数据流量标记上VNID，在默认情况下，只有数据包上的VNID与目标容器所在项目的VNID匹配上后，数据包才允许被转发到目标容器中。当有些项目的容器应用是通过公共服务的，后期可通过配置将多个项目见的网络连通，或者将项目设置为全局可访问。 二、启动多租户网络需要将集群中所有的master节点配置文件/etc/origin/master/master-config.yaml和node节点配置文件/etc/origin/node/node-config.yaml中的networkPluginName的属性值从redhat/openshift-ovs-subnet修改为redhat/openshift-ovs-multitenant，然后重启Openshift集群Master节点的origin-master-controllers.service服务和Node节点的origin-node.service服务三、测试，查看网络隔离在一个项目中的一个pod的终端中ping/telnet/curl/nslook另一个项目中的pod的ip地址或者对应svc的FQDN（..svc.cluster.local） 查看namespace的Netid是否一致 $ oc get netnamespaces NAME NETID EGRESS IPS default 0 [] kube-public 5899696 [] kube-service-catalog 0 [] demo 13843039 [] dubbo 11344186 [] jenkins 13843039 [] 当NETID相同时，表示这个两个project的网络是相通的当NETID为0时，表示这个Project的网络全局可访问 四、连通隔离的网络# project 1,2,3中所有的pod，service可以通过容器IP相互访问（通过service的FQDN不能相互访问） oc adm pod-network join-projects --to= #将某个project中所有的pod和service设置为全局可访问 oc adm pod-network make-projects-global 参考链接https://docs.okd.io/3.11/admin_guide/managing_networking.htmlCopyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:11 "},"origin/openshift-kubernetes的审计日志功能.html":{"url":"origin/openshift-kubernetes的审计日志功能.html","title":"Kubernetes的审计日志功能","keywords":"","body":"一、Overviewskubernetes 在 v1.7 中支持了日志审计功能（Alpha），在 v1.8 中为 Beta 版本，v1.12 为 GA 版本。Kubernetes 审计功能提供了与安全相关的按时间顺序排列的记录集，记录单个用户、管理员或系统其他组件影响系统的活动顺序。 它能帮助集群管理员处理以下问题：发生了什么？ 什么时候发生的？ 谁触发的？ 活动发生在哪个（些）对象上？ 在哪观察到的？ 它从哪触发的？ 活动的后续处理行为是什么？ kube-apiserver 是负责接收及相应用户请求的一个组件，每一个请求都会有几个阶段，每个阶段都有对应的日志，当前支持的阶段有：RequestReceived ：apiserver 在接收到请求后且在将该请求下发之前会生成对应的审计日志。 ResponseStarted ：在响应 header 发送后并在响应 body 发送前生成日志。这个阶段仅为长时间运行的请求生成（例如 watch）。 ResponseComplete ：当响应 body 发送完并且不再发送数据。 Panic：内部服务器出错，请求未完成。 也就是说对 apiserver 的每一个请求理论上会有三个阶段的审计日志生成日志记录级别，当前支持的日志记录级别有：None: 符合这条规则的日志将不会记录。 Metadata: 记录请求的 metadata（请求的用户、timestamp、resource、verb 等等），但是不记录请求或者响应的消息体。 Request: 记录事件的 metadata 和请求的消息体，但是不记录响应的消息体。这不适用于非资源类型的请求。 RequestResponse: 记录事件的 metadata，请求和响应的消息体。这不适用于非资源类型的请求。 输出的审计日志格式json{ \"kind\": \"Event\", \"apiVersion\": \"audit.k8s.io/v1beta1\", \"metadata\": { \"creationTimestamp\": \"2019-07-23T09:02:19Z\" }, \"level\": \"Request\", \"timestamp\": \"2019-07-23T09:02:19Z\", \"auditID\": \"eb481add-fdac-48a3-a302-1c33d73bfdbf\", \"stage\": \"RequestReceived\", \"requestURI\": \"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\", \"verb\": \"update\", \"user\": { \"username\": \"system:openshift-master\", \"groups\": [ \"system:masters\", \"system:authenticated\" ] }, \"sourceIPs\": [ \"192.168.1.96\" ], \"objectRef\": { \"resource\": \"configmaps\", \"namespace\": \"kube-system\", \"name\": \"openshift-master-controllers\", \"apiVersion\": \"v1\" }, \"requestReceivedTimestamp\": \"2019-07-23T09:02:19.148057Z\", \"stageTimestamp\": \"2019-07-23T09:02:19.148057Z\" } legacy 2019-07-23T23:50:06.223368641+08:00 AUDIT: id=\"3574e2e0-06b1-44d8-bc6c-5983c402d55e\" stage=\"ResponseComplete\" ip=\"192.168.1.96\" method=\"update\" user=\"system:openshift-master\" groups=\"\\\"system:masters\\\",\\\"system:authenticated\\\"\" as=\"\" asgroups=\"\" namespace=\"kube-system\" uri=\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\" response=\"200\" 审计后端可以将审计事件导出到外部存储。 Kube-apiserver 提供两个后端：Log 后端: 将事件写入到磁盘 Webhook 后端: 将事件发送到外部 API Note:审计日志记录功能会增加 API server 的内存消耗，因为需要为每个请求存储审计所需的某些上下文。 此外，内存消耗取决于审计日志记录的配置。 二、openshift开启自定义策略的审计功能创建审计日志的存储路径mkdir /etc/origin/master/audit # 注意：审计日志文件的存储路径必须是kube-system命名空间下apiservser pod挂载目录下的子路径。 # ocp 3.11版本的apiserver是以pod的形式运行在kube-system命名空间下的，它所需要的配置文件等Volume资源都是以hostpath的形式挂载上去的，例如ocp节点上的/etc/origin/master目录 编辑/etc/origin/master/master-config.yaml****省略******** auditConfig: auditFilePath: \"/etc/origin/master/audit/audit-ocp.log\" # 指定审计日志文件的存储路径 enabled: true # 开启审计功能 logFormat: \"json\" # 指定输出审计日志的格式。可指定为\"json\"或\"legacy\" maximumFileRetentionDays: 10 # 指定审计日志文件的保留天数 maximumFileSizeMegabytes: 100 # 指定审计日志文件的最大Byte maximumRetainedFiles: 5 # 指定审计日志文件的保留个数 policyConfiguration: null # 是否使用默认的审计策略 policyFile: \"/etc/origin/master/audit-policy.yaml\" # 自定义的审计策略配置文件 ****省略******** 创建自定义的审计策略配置文件kind: Policy omitStages: - \"ResponseStarted\" rules: - level: None users: [\"system:kube-proxy\"] verbs: [\"watch\"] resources: - group: \"\" resources: [\"endpoints\", \"services\"] - level: None userGroups: [\"system:authenticated\"] nonResourceURLs: - \"/api*\" # Wildcard matching. - \"/version\" - level: Request verbs: [\"update\"] resources: - group: \"\" # core API group resources: [\"configmaps\",\"secrets\"] # This rule only applies to resources in the \"kube-system\" namespace. # The empty string \"\" can be used to select non-namespaced resources. namespaces: [\"kube-system\"] # Log configmap and secret changes in all other namespaces at the metadata level. - level: None verbs: [\"update\"] resources: - group: \"\" # core API group resources: [\"secrets\", \"configmaps\"] # Log all other resources in core and extensions at the request level. - level: None resources: - group: \"\" # core API group - group: \"extensions\" # Version of group should NOT be included. # Log login failures from the web console or CLI. Review the logs and refine your policies. - level: Metadata nonResourceURLs: - /login* - /oauth* - level: Metadata userGroups: [\"system:authenticated:oauth\"] verbs: [\"create\", \"delete\"] resources: - group: \"project.openshift.io\" resources: [\"projectrequests\", \"projects\"] omitStages: - RequestReceived 重启APIServer和Controller#对于OCP版本大于3.9的 /usr/local/bin/master-restart api /usr/local/bin/master-restart controllers # 对于OCP版本小于3.9的 systemctl restart atomic-openshift-master-api systemctl restart atomic-openshift-master-controllers 三、使用openshift集群的Fluentd收集审计日志到集群内的elasticsearch配置OCP集群中的Fluentd挂载审计日志的存储目录(OCP集群中日志系统的fluentd是以DaemonSet形式收集节点上容器的日志到elasticsearch的，它是将节点的/var/lib/docker目录以hostpath形式挂载到容器中的)oc set volume ds/logging-fluentd --add --mount-path=/etc/origin/master/audit --name=audit --type=hostPath --path=/etc/origin/master/audit -n openshift-logging 配置OCP集群中的Fluentd监控审计日志目录下的日志oc edit cm/logging-fluentd -n openshift-logging *****省略******* ## sources *****省略******* @include configs.d/user/input-audit.conf *****省略******* input-audit.conf: | @type tail @id audit-ocp path /etc/origin/master/audit/audit-ocp.log pos_file /etc/origin/master/audit/audit.pos tag audit.requests format json @type copy @type elasticsearch log_level debug host \"#{ENV['OPS_HOST']}\" port \"#{ENV['OPS_PORT']}\" scheme https ssl_version TLSv1_2 index_name .audit user fluentd password changeme client_key \"#{ENV['OPS_CLIENT_KEY']}\" client_cert \"#{ENV['OPS_CLIENT_CERT']}\" ca_file \"#{ENV['OPS_CA']}\" type_name com.redhat.ocp.audit reload_connections \"#{ENV['ES_RELOAD_CONNECTIONS'] || 'false'}\" reload_after \"#{ENV['ES_RELOAD_AFTER'] || '100'}\" sniffer_class_name \"#{ENV['ES_SNIFFER_CLASS_NAME'] || 'Fluent::ElasticsearchSimpleSniffer'}\" reload_on_failure false flush_interval \"#{ENV['ES_FLUSH_INTERVAL'] || '5s'}\" max_retry_wait \"#{ENV['ES_RETRY_WAIT'] || '300'}\" disable_retry_limit true buffer_type file buffer_path '/var/lib/fluentd/buffer-output-es-auditlog' buffer_queue_limit \"#{ENV['BUFFER_QUEUE_LIMIT'] || '1024' }\" buffer_chunk_limit \"#{ENV['BUFFER_SIZE_LIMIT'] || '1m' }\" buffer_queue_full_action \"#{ENV['BUFFER_QUEUE_FULL_ACTION'] || 'exception'}\" request_timeout 2147483648 *****省略******* 重启Fluentdoc delete po -l component=fluentd -n openshift-logging 在ocp集群系统的Kibana上添加\".audit*\"的Index Pattern,并在\"Discover\"查看、筛选审计日志四、将审计日志通过WebHook发送到OCP外部的Logstash或者Fluentd 接收后端可使用Logstash或者Fluentd作为后端来接收Api-Server通过web hook方式发送的审计日志。Logstash和Fluentd可以是ocp集群外二进制方式安装运行的，也可以是原生Docker运行的，甚至可以是另外一个集群中容器化的。一个原则就是不要放到审计日志产生集群的内部。防止apiserver启动起来了，有了一些操作，logstash还没有启动起来，丢失审计日志。再者审计日志后端最好选择适合自己的，审计日志落一份，重复记录也没多大意义。方式一：使用OCP集群外二进制方式安装的Logstash来接收ApiServer通过web hook方式发送过来的审计日志并过滤、存储到本地文件中安装logstashbash -c 'cat > /etc/yum.repos.d/elasticsearch.repo 设置logstash，/etc/logstash/logstash.yml# ------------ Pipeline Configuration Settings -------------- # Where to fetch the pipeline configuration for the main pipeline path.config: /etc/logstash/conf.d/ *************************省略****************************** # ------------ Data path ------------------ # Which directory should be used by logstash and its plugins for any persistent needs. Defaults to LOGSTASH_HOME/data path.data: /data/logs/logstash/data/ *************************省略****************************** # ------------ Debugging Settings ------------- # Options for log.level: fatal/error/warn/info (default)/debug/trace log.level: info path.logs: /data/logs/logstash/logs 创建监听HTTP 8081端口的pipelinecat /etc/logstash/conf.d/accept-audit-log.conf input{ http{ host => \"0.0.0.0\" port => 8081 } } filter{ split{ # Webhook audit backend sends several events together with EventList # split each event here. field=>[items] # We only need event subelement, remove others. remove_field=>[headers, metadata, apiVersion, kind, \"@version\", host] } mutate{ rename => {items=>event} } } output{ file{ # Audit events from different users will be saved into different files. path=>\"/data/logs/logstash/ocp-audit-logs/ocp-audit-%{[event][user][username]}/audit-%{+YYYY-MM-dd}.log\" } } EOF 启动logstashmkdir -p /data/logs/logstash/{data,logs,ocp-audit-logs} chown -R logstash:logstash /data/logs/logstash system start logstash # 或者 /usr/share/logstash/bin/logstash -f /etc/logstash/config --path.settings /etc/logstash/ 测试logstash的联通性。一是看logstash pipeline监听的HTTP端口是否开启。二是尝试发送一个带有模拟数据的POST请求，看其是否会pipeline指定的数据目录生成日志文件ss -ntl |grep 8081 curl -X POST \\ http://192.168.1.96:8081 \\ -H 'Accept: */*' \\ -H 'Cache-Control: no-cache' \\ -H 'Connection: keep-alive' \\ -H 'Content-Type: application/json' \\ -H 'accept-encoding: gzip, deflate' \\ -d '{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1beta1\",\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:27:54Z\"},\"level\":\"Request\",\"timestamp\":\"2019-07-23T14:27:54Z\",\"auditID\":\"29bf32ba-4bea-4b4f-a1fb-cd091b2188ff\",\"stage\":\"ResponseComplete\",\"requestURI\":\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\",\"verb\":\"update\",\"user\":{\"username\":\"system:openshift-master\",\"groups\":[\"system:masters\",\"system:authenticated\"]},\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"resource\":\"configmaps\",\"namespace\":\"kube-system\",\"name\":\"openshift-master-controllers\",\"uid\":\"d54578ea-425e-11e9-b1bd-000c2976c04e\",\"apiVersion\":\"v1\",\"resourceVersion\":\"8285989\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ConfigMap\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"openshift-master-controllers\",\"namespace\":\"kube-system\",\"selfLink\":\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\",\"uid\":\"d54578ea-425e-11e9-b1bd-000c2976c04e\",\"resourceVersion\":\"8285989\",\"creationTimestamp\":\"2019-03-09T11:31:08Z\",\"annotations\":{\"control-plane.alpha.kubernetes.io/leader\":\"{\\\"holderIdentity\\\":\\\"allinone.okd311.curiouser.com\\\",\\\"leaseDurationSeconds\\\":15,\\\"acquireTime\\\":\\\"2019-03-09T11:31:01Z\\\",\\\"renewTime\\\":\\\"2019-07-23T14:27:54Z\\\",\\\"leaderTransitions\\\":0}\"}}},\"requestReceivedTimestamp\":\"2019-07-23T14:27:54.894767Z\",\"stageTimestamp\":\"2019-07-23T14:27:54.899643Z\",\"annotations\":{\"authorization.k8s.io/decision\":\"allow\",\"authorization.k8s.io/reason\":\"\"}}' 创建audit的webhook配置文件/etc/origin/master/audit-policy.yamlcat /etc/origin/master/audit-policy.yaml apiVersion: v1 clusters: - cluster: server: http://192.168.1.96:8081 name: logstash contexts: - context: cluster: logstash user: \"\" name: default-context current-context: default-context kind: Config preferences: {} users: [] EOF 编辑/etc/origin/master/master-config.yaml，添加webhook相关的参数****省略******** auditConfig: auditFilePath: \"/etc/origin/master/audit/audit-ocp.log\" # 指定审计日志文件的存储路径 enabled: true # 开启审计功能 logFormat: \"json\" # 指定输出审计日志的格式。可指定为\"json\"或\"legacy\" maximumFileRetentionDays: 10 # 指定审计日志文件的保留天数 maximumFileSizeMegabytes: 100 # 指定审计日志文件的最大Byte maximumRetainedFiles: 5 # 指定审计日志文件的保留个数 policyConfiguration: null # 是否使用默认的审计策略 policyFile: \"/etc/origin/master/audit-policy.yaml\" # 自定义的审计策略配置文件 #==========以下配置项为添加的webhook参数=========================================================================== webHookKubeConfig: /etc/origin/master/audit-webhook-config.yaml # 指定WebHook的配置文件（同样路径要指定在ApiServer POD已挂载的路径下） webHookMode: batch # 可选参数\"batch\"和\"blocking\" ****省略******** 重启APIServer和Controller#对于OCP版本大于3.9的 /usr/local/bin/master-restart api /usr/local/bin/master-restart controllers # 对于OCP版本小于3.9的 systemctl restart atomic-openshift-master-api systemctl restart atomic-openshift-master-controllers 验证，用除\"system:admin\"用户外的其他用户创建project，然后再删除project，最后查看logstash配置的审计日志存储目录下是否生成对应的文件oc login -u admin -p oc new-project test oc delete project test $ tree -L 2 /data/logs/logstash/ocp-audit-logs/ /data/logs/logstash/ocp-audit-logs/ ├── ocp-audit-admin │ └── audit-2019-07-23.log └── ocp-audit-system:openshift-master └── audit-2019-07-23.log $ cat /data/logs/logstash/ocp-audit-logs/ocp-audit-admin/audit-2019-07-23.log 产生以下内容。显示一次创建成功，另一次创建失败，原因是project已经存在（特意测试），一次删除project等日志。{\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:03Z\"},\"stageTimestamp\":\"2019-07-23T14:45:03.019249Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:02Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projectrequests\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"code\":201},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:02.964347Z\",\"auditID\":\"eec24884-b70a-4b27-80c1-431111d2f4f5\",\"verb\":\"create\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"cluster-admin-0\\\" of ClusterRole \\\"cluster-admin\\\" to User \\\"admin\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projectrequests\"}} {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:10Z\"},\"stageTimestamp\":\"2019-07-23T14:45:10.842999Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:10Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projectrequests\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Failure\",\"reason\":\"AlreadyExists\",\"code\":409},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:10.836487Z\",\"auditID\":\"a7b64f47-94eb-4723-b101-24e112cd0735\",\"verb\":\"create\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"self-provisioners\\\" of ClusterRole \\\"self-provisioner\\\" to Group \\\"system:authenticated:oauth\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projectrequests\"}} {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:19Z\"},\"stageTimestamp\":\"2019-07-23T14:45:19.813911Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:19Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projects\",\"namespace\":\"test\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Success\",\"code\":200},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:19.805940Z\",\"auditID\":\"9d3260ca-3bff-49da-9fe9-346043a29991\",\"verb\":\"delete\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"cluster-admin-0\\\" of ClusterRole \\\"cluster-admin\\\" to User \\\"admin\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projects/test\"}} 方式二：复用ocp集群内日志系统的Fluentd接收ApiServer通过web hook方式发送过来的审计日志并过滤、存储到挂载的文件中整得太晚了，后续更新。五、审计策略配置详解整得太晚了，后续更新。参考链接https://austindewey.com/2018/10/17/integrating-advanced-audit-with-aggregated-logging-in-openshift-3-11/#test-it-outhttps://www.outcoldsolutions.com/docs/monitoring-openshift/v4/audit/https://docs.openshift.com/container-platform/3.11/install_config/master_node_configuration.html#master-node-config-advanced-audithttps://docs.openshift.com/container-platform/3.11/security/monitoring.htmlhttps://kubernetes.io/docs/tasks/debug-application-cluster/audit/https://medium.com/@noqcks/kubernetes-audit-logging-introduction-464a34a53f6chttps://www.jianshu.com/p/8117bc2fb966https://cloud.google.com/kubernetes-engine/docs/concepts/audit-policy?hl=zh-cnhttps://github.com/rbo/openshift-examples/tree/master/efk-auditloghttps://github.com/openshift/origin-aggregated-logging/issues/1226Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-12 22:47:23 "},"origin/openshift-elasticsearch容器化部署.html":{"url":"origin/openshift-elasticsearch容器化部署.html","title":"Elasticsearch容器化部署","keywords":"","body":"一、拉取镜像docker pull docker.io/elasticsearch/elasticsearch:6.6.1 #或者 docker pull docker.elastic.co/elasticsearch/elasticsearch:6.6.1 二、Docker部署修改系统echo \"vm.max_map_count=262144\" >> /etc/sysctl.conf sysctl -w vm.max_map_count=262144 Docker单节点部署docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.io/elasticsearch/elasticsearch:6.6.1 Docker compose集群部署version: '2.2' services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:6.6.1 container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - esdata1:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - esnet elasticsearch2: image: docker.elastic.co/elasticsearch/elasticsearch:6.6.1 container_name: elasticsearch2 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" - \"discovery.zen.ping.unicast.hosts=elasticsearch\" ulimits: memlock: soft: -1 hard: -1 volumes: - esdata2:/usr/share/elasticsearch/data networks: - esnet volumes: esdata1: driver: local esdata2: driver: local networks: esnet: 三、OKD上部署DeploymentConfigapiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: labels: app: elasticsearch name: elasticsearch spec: replicas: 1 selector: app: elasticsearch deploymentconfig: elasticsearch strategy: type: Recreate template: metadata: labels: app: elasticsearch deploymentconfig: elasticsearch spec: containers: - env: - name: discovery.type value: single-node - name: cluster.name value: curiouser - name: bootstrap.memory_lock value: 'true' - name: path.repo value: /usr/share/elasticsearch/snapshots-repository - name: TZ value: Asia/Shanghai - name: ES_JAVA_OPTS value: '-Xms1g -Xmx2g' - name: xpack.monitoring.collection.enabled value: 'true' - name: xpack.security.enabled value: 'true' - name: ELASTIC_USERNAME value: \"elastic\" - name: \"ELASTIC_PASSWORD\" value: \"elastic\" image: 'docker.elastic.co/elasticsearch/elasticsearch:7.1.1' imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 initialDelaySeconds: 90 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 name: elasticsearch ports: - containerPort: 9200 protocol: TCP - containerPort: 9300 protocol: TCP readinessProbe: failureThreshold: 3 initialDelaySeconds: 80 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 resources: limits: cpu: '2' memory: 3Gi requests: cpu: '1' memory: 2Gi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /usr/share/elasticsearch/data name: elasticsearch-data - mountPath: /usr/share/elasticsearch/snapshots-repository name: elasticsearch-snapshots-repository dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: elasticsearch-data persistentVolumeClaim: claimName: elasticsearch-data - name: elasticsearch-snapshots-repository persistentVolumeClaim: claimName: elasticsearch-snapshots-repository test: false triggers: - type: ConfigChange SVCapiVersion: v1 kind: Service metadata: labels: app: elasticsearch name: elasticsearch spec: ports: - name: 9200-tcp port: 9200 protocol: TCP targetPort: 9200 - name: 9300-tcp port: 9300 protocol: TCP targetPort: 9300 selector: deploymentconfig: elasticsearch sessionAffinity: None type: ClusterIP 数据目录PVCapiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: nfs-client-storageclass name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi snapshot repository存储目录PVCapiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: nfs-client-storageclass name: elasticsearch-snapshots-repository spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi 四. Kubernetes部署Deploymentkind: Deployment apiVersion: apps/v1 metadata: labels: elastic-app: elasticsearch role: master name: elasticsearch-master namespace: elk spec: replicas: 1 revisionHistoryLimit: 10 strategy: type: Recreate selector: matchLabels: elastic-app: elasticsearch role: master template: metadata: labels: elastic-app: elasticsearch role: master spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1.k8s.curiouser.com initContainers: - name: init-scheduler image: busybox:latest imagePullPolicy: IfNotPresent command: ['sh', '-c', 'chmod -R 777 /usr/share/elasticsearch/data /usr/share/elasticsearch/snapshots-repository && chown -R 1000.0 /usr/share/elasticsearch/data /usr/share/elasticsearch/snapshots-repository'] volumeMounts: - name: elasticsearch-data mountPath: /usr/share/elasticsearch/data - name: elasticsearch-snapshots-repository mountPath: /usr/share/elasticsearch/snapshots-repository containers: - name: elasticsearch-master-data image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9200 protocol: TCP - containerPort: 9300 protocol: TCP env: - name: \"cluster.name\" value: \"Curiouser\" - name: \"bootstrap.memory_lock\" value: \"false\" - name: discovery.type value: single-node - name: \"node.master\" value: \"true\" - name: \"node.data\" value: \"true\" - name: \"node.ingest\" value: \"false\" - name: xpack.monitoring.collection.enabled value: \"true\" - name: \"xpack.monitoring.elasticsearch.collection.enabled\" value: \"true\" - name: \"xpack.security.enabled\" value: \"true\" - name: \"path.repo\" value: \"/usr/share/elasticsearch/snapshots-repository\" - name: \"ES_JAVA_OPTS\" value: \"-Xms2048m -Xmx2048m\" - name: TZ value: Asia/Shanghai - name: \"xpack.monitoring.exporters.my_local.type\" value: \"local\" - name: \"xpack.monitoring.exporters.my_local.use_ingest\" value: \"false\" resources: requests: memory: \"2Gi\" cpu: \"2\" limits: memory: \"4096Mi\" cpu: \"3\" readinessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 livenessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 volumeMounts: - name: elasticsearch-data mountPath: \"/usr/share/elasticsearch/data\" - name: elasticsearch-snapshots-repository mountPath: \"/usr/share/elasticsearch/snapshots-repository\" restartPolicy: Always terminationGracePeriodSeconds: 30 dnsPolicy: ClusterFirst securityContext: {} schedulerName: default-scheduler volumes: - name: elasticsearch-data persistentVolumeClaim: claimName: elasticsearch-data - name: elasticsearch-snapshots-repository persistentVolumeClaim: claimName: elasticsearch-snapshots-repository PersistentVolumeClaim--- apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: cephfs labels: app: elasticsearch name: elasticsearch-data namespace: elk spec: accessModes: - ReadWriteMany resources: requests: storage: 20Gi --- apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: cephfs labels: app: elasticsearch name: elasticsearch-snapshots-repository namespace: elk spec: accessModes: - ReadWriteMany resources: requests: storage: 30Gi Servicekind: Service apiVersion: v1 metadata: labels: elastic-app: elasticsearch-service name: elasticsearch namespace: elk spec: ports: - port: 9200 targetPort: 9200 protocol: TCP selector: elastic-app: elasticsearch type: ClusterIP Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:10 "},"origin/openshift-Kibana容器化部署.html":{"url":"origin/openshift-Kibana容器化部署.html","title":"Kibana容器化部署","keywords":"","body":"一、拉取镜像docker pull docker.io/kibana/kibana:6.6.1 #或者 docker pull docker.elastic.co/kibana/kibana:6.6.1 二、Docker部署单机Docker部署docker run -p 5601:5601 \\ -e \"ELASTICSEARCH_HOST=http://ElasticSearch_HostIP:9200\" \\ -e \"SERVER_NAME=Curiouser\" \\ docker.elastic.co/kibana/kibana:6.6.1 三、OKD上部署DeploymentConfigapiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: labels: app: kibana name: kibana spec: replicas: 1 selector: app: kibana deploymentconfig: kibana strategy: activeDeadlineSeconds: 21600 resources: {} rollingParams: intervalSeconds: 1 maxSurge: 25% maxUnavailable: 25% timeoutSeconds: 600 updatePeriodSeconds: 1 type: Rolling template: metadata: labels: app: kibana deploymentconfig: kibana spec: containers: - env: - name: ELASTICSEARCH_USERNAME value: kibana - name: ELASTICSEARCH_PASSWORD value: uLAWAfW1b7UHZdHEigCW - name: TZ value: Asia/Shanghai image: docker.elastic.co/kibana/kibana:7.1.1 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 name: kibana ports: - containerPort: 5601 protocol: TCP readinessProbe: failureThreshold: 3 initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 resources: limits: cpu: \"1\" memory: 1500Mi requests: cpu: 500m memory: 800Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 test: false triggers: - type: ConfigChange SVCapiVersion: v1 kind: Service metadata: labels: app: kibana name: kibana spec: ports: - name: 5601-tcp port: 5601 protocol: TCP targetPort: 5601 selector: deploymentconfig: kibana sessionAffinity: None type: ClusterIP RouteapiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: apache-kibana name: kibana spec: port: targetPort: 5601-tcp to: kind: Service name: kibana weight: 100 wildcardPolicy: None 四. Kubernetes上部署DeploymentapiVersion: apps/v1beta2 kind: Deployment metadata: labels: app: kibana name: kibana namespace: elk spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: kibana strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: labels: app: kibana spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1.k8s.curiouser.com containers: - image: kibana/kibana:7.2.0 imagePullPolicy: IfNotPresent name: kibana envFrom: - secretRef: name: kibana-config-env env: - name: TZ value: Asia/Shanghai - name: ELASTICSEARCH_HOSTS value: '[\"http://elasticsearch.elk.svc:9200\"]' ports: - containerPort: 5601 name: web protocol: TCP resources: requests: memory: \"1Gi\" cpu: \"0.5\" limits: memory: \"2Gi\" cpu: \"1\" readinessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 livenessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 securityContext: allowPrivilegeEscalation: false capabilities: {} privileged: false procMount: Default readOnlyRootFilesystem: false runAsNonRoot: false stdin: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File tty: true dnsConfig: {} dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 SecretapiVersion: v1 kind: Secret metadata: labels: app: kibana name: kibana-config-env namespace: elk stringData: ELASTICSEARCH_USERNAME: kibana ELASTICSEARCH_PASSWORD: kibana ServiceapiVersion: v1 kind: Service metadata: name: kibana namespace: elk labels: app: kibana spec: ports: - port: 5601 name: web selector: app: kibana IngressapiVersion: extensions/v1beta1 kind: Ingress metadata: name: kibana namespace: elk spec: rules: - host: kibana.apps.k8s.curiouser.com http: paths: - path: / backend: serviceName: kibana servicePort: 5601 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:07 "},"origin/kubernetes-集群角色及插件.html":{"url":"origin/kubernetes-集群角色及插件.html","title":"Kubernetes的集群角色及插件","keywords":"","body":"一、Master节点上的组件kube-apiserver：对外暴露了Kubernetes API。它是的 Kubernetes 前端控制层。它被设计为水平扩展，即通过部署更多实例来缩放。 kube-controller-manager：运行控制器，它们是处理集群中常规任务的后台线程。逻辑上，每个控制器是一个单独的进程，但为了降低复杂性，它们都被编译成独立的可执行文件，并在单个进程中运行。这些控制器包括:节点控制器: 当节点移除时，负责注意和响应。 副本控制器: 负责维护系统中每个副本控制器对象正确数量的 Pod。 端点控制器: 填充端点(Endpoints) 对象(即连接 Services & Pods)。 服务帐户和令牌控制器: 为新的命名空间创建默认帐户和 API 访问令牌 kube-scheduler：监视没有分配节点的新创建的 Pod，选择一个节点供他们运行。 etcd：用于 Kubernetes 的后端存储。存储所有集群数据。 cloud-controller-manager：用于与底层云提供商交互的控制器。云控制器管理器可执行组件是 Kubernetes v1.6 版本中引入的 Alpha 功能。仅运行云提供商特定的控制器循环。您必须在 - kube-controller-manager 中禁用这些控制器循环，您可以通过在启动 kube-controller-manager 时将 --cloud-provider 标志设置为external来禁用控制器循环。允许云供应商代码和 Kubernetes 核心彼此独立发展，在以前的版本中，Kubernetes 核心代码依赖于云提供商特定的功能代码。在未来的版本中，云供应商的特定代码应由云供应商自己维护，并与运行 Kubernetes 的云控制器管理器相关联。以下控制器具有云提供商依赖关系: 节点控制器: 用于检查云提供商以确定节点是否在云中停止响应后被删除 路由控制器: 用于在底层云基础架构中设置路由 服务控制器: 用于创建，更新和删除云提供商负载平衡器 数据卷控制器: 用于创建，附加和装载卷，并与云提供商进行交互以协调卷 二、Node节点上的组件kubelet：是主要的节点代理,它监测已分配给其节点的 Pod(通过 apiserver 或通过本地配置文件)，提供如下功能:挂载 Pod 所需要的数据卷(Volume)。 下载 Pod 的 secrets。 通过 Docker 运行(或通过 rkt)运行 Pod 的容器。 周期性的对容器生命周期进行探测。 如果需要，通过创建镜像Pod（Mirror Pod） 将 Pod 的状态报告回系统的其余部分。 将节点的状态报告回系统的其余部分。 kube-proxy：通过维护主机上的网络规则并执行连接转发，实现了Kubernetes服务抽象 Container Runtime：运行容器的底层平台。Kubernetes支持的容器平台：Docker、containerd、cri-o、rktlet 三、插件网络插件ACI: provides integrated container networking and network security with Cisco ACI. Calico is a secure L3 networking and network policy provider. Canal unites Flannel and Calico, providing networking and network policy. Cilium is a L3 network and network policy plugin that can enforce HTTP/API/L7 - policies transparently. Both routing and overlay/encapsulation mode are - supported. CNI-Genie enables Kubernetes to seamlessly connect to a choice of CNI plugins,such as Calico, Canal, Flannel, Romana, or Weave. Contiv provides configurable networking (native L3 using BGP, overlay using - vxlan, classic L2, and Cisco-SDN/ACI) for various use cases and a rich policy - framework. Contiv project is fully open sourced. The installer provides both - kubeadm and non-kubeadm based installation options. Contrail, based on Tungsten Fabric, is a open source, multi-cloud network virtualization and policy management platform. Contrail and Tungsten Fabric are integrated with orchestration systems such as Kubernetes, OpenShift, OpenStack and Mesos, and provide isolation modes for virtual machines, containers/pods and bare metal workloads. Flannel is an overlay network provider that can be used with Kubernetes. Knitter is a network solution supporting multiple networking in Kubernetes. Multus is a Multi plugin for multiple network support in Kubernetes to support - all CNI plugins (e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, - DPDK, OVS-DPDK and VPP based workloads in Kubernetes. NSX-T Container Plug-in (NCP) provides integration between VMware NSX-T and - container orchestrators such as Kubernetes, as well as integration between - NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service - (PKS) and OpenShift. Nuage is an SDN platform that provides policy-based networking between - Kubernetes Pods and non-Kubernetes environments with visibility and security - monitoring. Romana is a Layer 3 networking solution for pod networks that also supports the - NetworkPolicy API. Kubeadm add-on installation details available here. Weave Net provides networking and network policy, will carry on working on both - sides of a network partition, and does not require an external database 服务发现插件CoreDNS is a flexible, extensible DNS server which can be installed as the in-cluster DNS for pods 可视化及控制插件Dashboard is a dashboard web interface for Kubernetes. Weave Scope is a tool for graphically visualizing your containers, pods, services etc. Use it in conjunction with a Weave Cloud account or host the UI yourself 参考链接https://kubernetes.io/docs/concepts/cluster-administration/addons/Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:02 "},"origin/kubernetes-使用Kubeadm安装单机版Kubernetes.html":{"url":"origin/kubernetes-使用Kubeadm安装单机版Kubernetes.html","title":"Kubeadm安装单机版Kubernetes","keywords":"","body":"使用Kubeadm安装单机版Kuberneteskubeadm是Kubernetes官方提供的用于快速安装 Kubernetes 集群的工具，通过将集群的各个组件进行容器化安装管理，通过kubeadm的方式安装集群比二进制的方式安装要方便Prerequisite **Hostname** **IP 地址** **硬件** **Kubernetes版本** **Docker版本** allinone.k8s114.curiouser.com 172.16.1.12 最低2C2G v1.14.0 18.09.4 关闭防火墙 关闭Selinux 关闭Swap 加载br_netfilter 添加配置内核参数 hosts文件添加主机名与IP的映射关 #关闭防火墙 \\ #关闭Swap \\ #关闭Selinux \\ #加载br_netfilter \\ #添加配置内核参数 \\ #加载配置 \\ systemctl disable firewalld && systemctl stop firewalld \\ swapoff -a && sed -i 's/.\\\\*swap.\\\\*/#&/' /etc/fstab \\ setenforce 0 \\ sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/sysconfig/selinux && \\ sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config && \\ sed -i \"s/^SELINUX=permissive/SELINUX=disabled/g\" /etc/sysconfig/selinux && \\ sed -i \"s/^SELINUX=permissive/SELINUX=disabled/g\" /etc/selinux/config && \\ modprobe br_netfilter && \\ bash -c 'cat > /etc/sysctl.d/k8s.conf > /etc/hosts 一、安装安装docker kubeadm kubelet kubectl yum -y install yum-utils && \\ yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo && \\ bash -c 'cat > /etc/yum.repos.d/kubernetes.repo (可选)添加dockers日志相关配置 --log-driver=json-file --log-opt=max-size=10m --log-opt=max-file=5 (可选)预拉取镜像 docker pull mirrorgooglecontainers/kube-apiserver:v1.14.0 && \\ docker pull mirrorgooglecontainers/kube-controller-manager:v1.14.0 && \\ docker pull mirrorgooglecontainers/kube-scheduler:v1.14.0 && \\ docker pull mirrorgooglecontainers/kube-proxy:v1.14.0 && \\ docker pull mirrorgooglecontainers/pause:3.1 && \\ docker pull mirrorgooglecontainers/etcd:3.3.10 && \\ docker pull coredns/coredns:1.3.1 && \\ docker pull quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64 && \\ docker pull calico/cni:v3.3.6 && \\ docker pull calico/node:v3.3.6 && \\ docker tag mirrorgooglecontainers/kube-proxy:v1.14.0 k8s.gcr.io/kube-proxy:v1.14.0 && \\ docker tag mirrorgooglecontainers/kube-scheduler:v1.14.0 k8s.gcr.io/kube-scheduler:v1.14.0 && \\ docker tag mirrorgooglecontainers/kube-apiserver:v1.14.0 k8s.gcr.io/kube-apiserver:v1.14.0 && \\ docker tag mirrorgooglecontainers/kube-controller-manager:v1.14.0 k8s.gcr.io/kube-controller-manager:v1.14.0 && \\ docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 && \\ docker tag mirrorgooglecontainers/etcd:3.3.10 k8s.gcr.io/etcd:3.3.10 && \\ docker tag coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1 && \\ docker tag quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64 quay.io/coreos/flannel:v0.11.0-amd64 && \\ docker rmi mirrorgooglecontainers/kube-apiserver:v1.14.0 && \\ docker rmi mirrorgooglecontainers/kube-controller-manager:v1.14.0 && \\ docker rmi mirrorgooglecontainers/kube-scheduler:v1.14.0 && \\ docker rmi mirrorgooglecontainers/kube-proxy:v1.14.0 && \\ docker rmi mirrorgooglecontainers/pause:3.1 && \\ docker rmi mirrorgooglecontainers/etcd:3.3.10 && \\ docker rmi coredns/coredns:1.3.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64 初始化apiserver kubeadm init --apiserver-advertise-address=0.0.0.0 --kubernetes-version=v1.14.0 --pod-network-cidr=192.168.0.0/16 配置常规用户或root用户如何使用kubectl访问集群 mkdir -p $HOME/.kube && \\ cp -i /etc/kubernetes/admin.conf $HOME/.kube/config && \\ chown $(id -u):$(id -g) $HOME/.kube/config 设置Master节点可被调度 kubectl taint nodes --all node-role.kubernetes.io/master- #该参数node-role.kubernetes.io/master会污染所有节点，包括master节点,这意味着调度器可以调度POD到所有节点。 (可选)设置Kubectl命令别名及命令补全 yum install -y bash-completion && \\ echo \"alias k='kubectl'\" >> /etc/bashrc && \\ source 二、安装容器网络插件Calicokubeadm初始化apiserver时添加\"--pod-network-cidr=192.168.0.0/16\" 网络工作在amd64，arm64，ppc64le kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml && \\ kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml FlannelPrerequisite设置 /proc/sys/net/bridge/bridge-nf-call-iptables为1，将桥接的IPv4流量传递到iptables的链 sysctl net.bridge.bridge-nf-call-iptables=1 kubeadm初始化apiserver时添加\"--pod-network-cidr=10.244.0.0/16\" flannel网络工作在amd64, arm, arm64, ppc64le,s390x 安装kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml 三、验证查看所有namespace下的POD kubectl get pods --all-namespaces kubectl get pod -n kube-system 查看集群Node节点 kubectl get node systemctl status kubelet.service 查看版本 kubectl version 显示集群信息 kubectl cluster-info 四、添加Node节点kubeadm join 172.16.1.12:6443 --token i7xcb9.sz5t4sa8xx3ntc2h --discovery-token-ca-cert-hash sha256:487275a22ea4af5a1ea30ee4b0f21f8c27104d17f6a259bf4990f1569a3301cd 查看Master的Tokenkubeadm token list Master创建Tokenkubeadm token create Master节点创建\"--discovery-token-ca-cert-hash\"值openssl x509 -pubkey -in /etc/kubernet 五、安装UI管理界面DashBoard项目GitHub：https://github.com/kubernetes/dashboard.git# ------------------- Dashboard Secret ------------------- # apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-system type: Opaque --- # ------------------- Dashboard Service Account ------------------- # apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system --- # ------------------- Dashboard Role & Role Binding ------------------- # kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kubernetes-dashboard-minimal namespace: kube-system rules: # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret. - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"create\"] # Allow Dashboard to create 'kubernetes-dashboard-settings' config map. - apiGroups: [\"\"] resources: [\"configmaps\"] verbs: [\"create\"] # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\"] verbs: [\"get\", \"update\", \"delete\"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map. - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"kubernetes-dashboard-settings\"] verbs: [\"get\", \"update\"] # Allow Dashboard to get metrics from heapster. - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"heapster\"] verbs: [\"proxy\"] - apiGroups: [\"\"] resources: [\"services/proxy\"] resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\"] verbs: [\"get\"] --- apiVersion: rbac.authorization.k8s.io/v1 #===修改原rolebind类型RoleBinding为ClusterRoleBinding kind: ClusterRoleBinding metadata: name: kubernetes-dashboard-minimal namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io #修改原role类型Role为ClusterRole kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system --- # ------------------- Dashboard Deployment ------------------- # kind: Deployment apiVersion: apps/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1 ports: #====修改原容器端口8443为9090 - containerPort: 9090 protocol: TCP args: #==== #- --auto-generate-certificates # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTP path: / #修改原健康检查端口8443为9090 port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: {} serviceAccountName: kubernetes-dashboard # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule --- # ------------------- Dashboard Service ------------------- # #使用Nodeport的方式访问Dashboard kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-external namespace: kube-system spec: ports: - port: 9090 targetPort: 9090 nodePort: 30090 type: NodePort selector: k8s-app: kubernetes-dashboard 拉取Template中使用的Imagedocker pull mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1 && \\ docker tag mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1 && \\ docker rmi mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1 Weave Scope官方文档： https://www.weave.works/docs/scope/latest/installing/#k8s安装部署 kubectl apply -f \"https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" （可选）修改svc使用NodePort访问 $ kubectl edit service/weave-scope-app -n weave apiVersion: v1 kind: Service #.........省略........ spec: externalTrafficPolicy: Cluster ports: - name: app #===== nodePort: 30040 port: 80 protocol: TCP targetPort: 4040 selector: app: weave-scope name: weave-scope-app weave-cloud-component: scope weave-scope-component: app sessionAffinity: None #====== type: NodePort Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:02 "},"origin/kubernetes-kubectl多集群上下文配置.html":{"url":"origin/kubernetes-kubectl多集群上下文配置.html","title":"kubectl多集群上下文配置","keywords":"","body":"kubectl命令行配置多Kubernetes集群一. 下载kubectlkubectl github下载地址：https://github.com/kubernetes/kubectl/releases二. 创建配置文件夹Linux mkdir ~/.kube Windows CMD mkdir %USERPROFILE%\\.kube # %USERPROFILE% 当前用户目录 三. 创建编辑kubectl配置文件apiVersion: v1 # 集群信息 clusters: - cluster: certificate-authority-data: **CA证书*** server: https://开发k8s环境APIServer的IP地址:6443 name: k8s-dev - cluster: certificate-authority-data: **CA证书*** server: https://测试k8s环境APIServer的IP地址:8443 name: k8s-test - cluster: certificate-authority-data: **CA证书*** server: https://UAT k8s环境APIServer的IP地址:8443 name: k8s-uat - cluster: certificate-authority-data: **CA证书*** server: https://生产k8s环境APIServer的IP地址:8443 name: k8s-pro # 集群上下文环境 contexts: - context: cluster: k8s-dev user: k8s-dev-admin name: k8s-dev - context: cluster: k8s-test user: k8s-test-admin name: k8s-test - context: cluster: k8s-uat user: k8s-uat-admin name: k8s-uat - context: cluster: k8s-pro user: k8s-pro-readonly name: k8s-pro # 当前使用的上下文环境 current-context: k8s-dev kind: Config preferences: {} #集群用户信息及证书信息 users: - name: k8s-dev user: client-certificate-data: **用户证书** client-key-data： **用户私钥** - name: k8s-test user: client-certificate-data: **用户证书** client-key-data： **用户私钥** - name: k8s-uat user: client-certificate-data: **用户证书** client-key-data： **用户私钥** - name: k8s-pro user: client-certificate-data: **用户证书** client-key-data： **用户私钥** 四. 切换Kubernetes集群上下文#切换至开发k8s环境上下文 kubectl config use-context k8s-dev #切换至开发k8s环境上下文 kubectl config use-context k8s-test #切换至开发k8s环境上下文 kubectl config use-context k8s-uat #切换至开发k8s环境上下文 kubectl config use-context k8s-pro 五. kubectl命令的别名和快速切换集群上下文的别名设置别名快速使用kubectl命令Windows doskey k=kubectl $* # $*表示这个命令还可能有其他参数 Linux alias k='kubectl' 设置别名快速切换Kubectl集群上下文 Windows doskey k2d=kubectl config use-context k8s-dev doskey k2t=kubectl config use-context k8s-test doskey k2u=kubectl config use-context k8s-uat doskey k2p=kubectl config use-context k8s-pro Linux alias k2d='kubectl config use-context k8s-dev' alias k2t='kubectl config use-context k8s-test' alias k2u='kubectl config use-context k8s-uat' alias k2p='kubectl config use-context k8s-pro' Windows和Linux下设置别名永久生效 Windows①创建bat脚本cmdalias.cmd @doskey k=kubectl $* @doskey k2d=kubectl config use-context k8s-dev @doskey k2t=kubectl config use-context k8s-test @doskey k2u=kubectl config use-context k8s-uat @doskey k2p=kubectl config use-context k8s-pro # @表示执行这条命令时不显示这条命令本身 ②修改注册表 方式1：手动在注册HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor下添加一项AutoRun，把值设为bat脚本的路径 方式2：创建编写一个注册表修改文件，名为：add-regkey.reg，双击行这个文件,导入注册表添加的值 Windows Registry Editor Version 5.00 [HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor] \"AutoRun\"=\"%USERPROFILE%\\\\.kube\\\\cmdalias.cmd\" Linuxecho \"alias k='kubectl'\" >> /etc/profile && \\ echo \"alias k2d='kubectl config use-context k8s-dev'\" >> /etc/profile && \\ echo \"alias k2t='kubectl config use-context k8s-test'\" >> /etc/profile && \\ echo \"alias k2u='kubectl config use-context k8s-uat'\" >> /etc/profile && \\ echo \"alias k2p='kubectl config use-context k8s-pro'\" >> /etc/profile && \\ source /etc/profile 六. Kubectl Config命令详解 1. If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes place. 2. If $KUBECONFIG environment variable is set, then it is used a list of paths (normal path delimitting rules for your system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the last file in the list. 3. Otherwise, ${HOME}/.kube/config is used and no merging takes place. Usage: kubectl config SUBCOMMAND [options] Available Commands: current-context Displays the current-context delete-cluster Delete the specified cluster from the kubeconfig delete-context Delete the specified context from the kubeconfig get-clusters Display clusters defined in the kubeconfig get-contexts Describe one or many contexts rename-context Renames a context from the kubeconfig file. set Sets an individual value in a kubeconfig file set-cluster Sets a cluster entry in kubeconfig set-context Sets a context entry in kubeconfig set-credentials Sets a user entry in kubeconfig unset Unsets an individual value in a kubeconfig file use-context Sets the current-context in a kubeconfig file view Display merged kubeconfig settings or a specified kubeconfig file 参考连接https://blog.csdn.net/u013360850/article/details/83315188 https://www.awaimai.com/2445.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:02 "},"origin/kubernetes-NetworkPolicy.html":{"url":"origin/kubernetes-NetworkPolicy.html","title":"Network Policy容器流量管理","keywords":"","body":"Kubernetes Network Policy容器网络流量限制一、OverviewKubernetes使用命名空间namesapce做多租户隔离，但是如果不配置网络策略，namespace的隔离也仅仅是作用于在kubernetes编排调度时的隔离，实际上不同namespace下的pod还是可以相互联通的。此时就需要使用Kubernetes提供的networkPolicy,用于隔离不同租户应用的网络流量来减少攻击面。它使用标签选择器模拟传统的分段网络，并通过策略控制它们之间的流量以及来自外部的流量。Kubernetes中的Network Policy只定义了规范，并没有提供实现，而是把实现留给了网络插件Kubernetes v1.7+版本GA，API版本为http://networking.k8s.io/v1二、使用Calico做Network policyNetwork Policy的实现仰赖CNI插件的支持，目前已经支持的cni插件包括：Calico Kube-router Romana Weave Net trireme Calico的NetworkPolicy功能支持以下特性：支持多种endpoint: pods/containers, VMs 支持限制入站和出站的流量访问 规则策略支持: 动作: allow, deny, log, pass 源和目标的匹配标准: 端口: numbered, ports in a range, and Kubernetes named ports 协议: TCP, UDP, ICMP, SCTP, UDPlite, ICMPv6, protocol numbers (1-255) HTTP attributes (if using Istio service mesh) ICMP attributes IP version (IPv4, IPv6) IP or CIDR Endpoint selectors (using label expression to select pods, VMs, host - interfaces, and/or network sets) Namespace selectors Service account selectors Optional packet handling controls: disable connection tracking, apply before DNAT, apply to forwarded traffic and/or locally terminated traffic Preflightk8s集群版本大于v1.3.0 calico-cni网络插件的二进制文件 kubelet添加配置Flag需要配置kubelet 让pod启动时使用calico网络插件，kubelet可以配置使用calico在启动时配置参数： --network-plugin=cni --cni-conf-dir=/etc/cni/net.d # CNI插件的配置文件目录,该目录下的配置文件内容需要符合CNI规范 --cni-bin-dir=/opt/cni/bin # CNI插件的可执行文件目录，默认为/opt/cni/bin API Server添加配置Flag --allow-privileged=true # calico-node的POD需要以特权模式运行在各node上 三、策略规则的声明配置Network Policy策略规则是用来定义命名空间有哪些POD，能被谁访问，能访问谁的。相应地，在Network Policy声明文件中的字段有：spec.podSelector: 定义该命名空间有哪些POD遵循本networkpolicy约束 spec.ingress.from: 定义受本networkpolicy约束的POD的入站规则 spec.egress.to：定义受本networkpolicy约束的POD的出站规则 spec.ingress.from的选择器有：podSelector namespaceSelector ...上文省略... ingress: - from: - namespaceSelector: matchLabels: user: alice - podSelector: matchLabels: role: client ...下文省略... namespaceSelector和podSelector（注意YAML语法的区别） ...上文省略... ingress: - from: - namespaceSelector: matchLabels: user: alice podSelector: matchLabels: role: client ...下文省略... ipBlock 四、示例说明apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default spec: # POD选择器，选择遵循本networkpolicy约束的POD podSelector: matchLabels: role: db # 流量访问策略类型 policyTypes: - Ingress - Egress # 流量访问入站规则 ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 # POD流量访问出站规则 egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978 上述例子，网络流量访问策略规则可解释为：\"default\"命名空间下标签为\"role=db\"的POD的入站规则：允许被\"default\"命名空间下，所有带标签\"role=frontend\"的POD访问TCP 6379端口 允许被标签为\"project=myproject\"的命名空间下所有的POD访问TCP 6379端口 允许被IP地址为172.17.0.0–172.17.0.255或172.17.2.0–172.17.255.255的POD访问TCP 6379端口 \"default\"命名空间下标签为\"role=db\"的POD的出站规则：允许访问10.0.0.0/24网段的POD的5978端口 五、默认策略默认情况下，如果namespace下没有network policy,则该namespace下所有POD的入站规则和出站规则都是开放的。network policy只影响命名空间下被Pod Selector选择的POD，其他依旧是默认规则。namespace下的所有pod，入站规则为全部禁止 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny namespace: test spec: podSelector: {} policyTypes: - Ingress namespace下的所有pod，入站规则为全部开放 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all namespace: test spec: podSelector: {} ingress: - {} policyTypes: - Ingress namespace下的所有pod，出站规则为全部禁止 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny namespace: test spec: podSelector: {} policyTypes: - Egress namespace下的所有pod，出站规则为全部开放 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all namespace: test spec: podSelector: {} egress: - {} policyTypes: - Egress 同namespace的pod，入站和出站规则为全部禁止 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: {} policyTypes: - Ingress - Egress 六、场景测试限制服务只能被带有特定label的应用访问 $ kubectl create ns test1 $ kubectl -n test1 run nginx --image=nginx $ kubectl -n test1 expose deployment nginx --port=80 kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: test1 name: access-nginx spec: podSelector: matchLabels: run: nginx ingress: - from: - podSelector: matchLabels: access: \"true\" # 创建没有Label的临时POD去访问Nginx的SVC $ kubectl -n test1 run busybox --rm -ti --image=busybox /bin/sh / # wget http://nginx.test1.svc:80/ (无法访问) # 创建没有Label的临时POD去访问Nginx的SVC $ kubectl -n test1 run busybox --labels=\"access=true\" --rm -ti --image=busybox /bin/sh / # wget http://nginx.test1.svc:80/ Connecting to nginx.test1.svc:80 (10.68.86.216:80) saving to 'index.html' index.html 100% | *********************************************************************************************************************| 612 0:00:00 ETA 'index.html' saved 限制带\"run=busybox\"标签的Pod只能访问www.baidu.com kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: busybox-egress-baidu-a-policy namespace: test1 spec: podSelector: matchLabels: run: busybox egress: - to: - ipBlock: cidr: 180.101.49.11/32 - to: - ipBlock: cidr: 0.0.0.0/0 ports: - protocol: UDP port: 53 $ kubectl -n test1 run busybox --labels=\"run=busybox\" --rm -ti --image=busybox /bin/sh / # wget www.baidu.com Connecting to www.baidu.com (180.101.49.11:80) saving to 'index.html' index.html 100% |*********************************************************************************************************************| 2381 0:00:00 ETA 'index.html' saved / # wget www.sohu.com Connecting to www.sohu.com (101.227.172.11:80) ^C / # wget https://github.com Connecting to github.com (13.250.177.223:443) ^C / # 参考链接https://kubernetes.io/docs/concepts/services-networking/network-policies/#sctp-support https://docs.projectcalico.org/v3.8/security/kubernetes-network-policy#best-practice-create-deny-all-default-network-policy https://yq.aliyun.com/articles/640190 https://www.jianshu.com/p/c0d2618d2849 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:01 "},"origin/kubernetes-helm.html":{"url":"origin/kubernetes-helm.html","title":"K8S应用管理工具Helm","keywords":"","body":"Helm简介、安装、配置、使用一、简介全面拥抱Helm3二、原理三、安装四、配置五、命令行参数The Kubernetes package manager Common actions for Helm: - helm search: search for charts - helm pull: download a chart to your local directory to view - helm install: upload the chart to Kubernetes - helm list: list releases of charts Environment variables: +------------------+-----------------------------------------------------------------------------+ | Name | Description | +------------------+-----------------------------------------------------------------------------+ | $XDG_CACHE_HOME | set an alternative location for storing cached files. | | $XDG_CONFIG_HOME | set an alternative location for storing Helm configuration. | | $XDG_DATA_HOME | set an alternative location for storing Helm data. | | $HELM_DRIVER | set the backend storage driver. Values are: configmap, secret, memory | | $HELM_NO_PLUGINS | disable plugins. Set HELM_NO_PLUGINS=1 to disable plugins. | | $KUBECONFIG | set an alternative Kubernetes configuration file (default \"~/.kube/config\") | +------------------+-----------------------------------------------------------------------------+ Helm stores configuration based on the XDG base directory specification, so - cached files are stored in $XDG_CACHE_HOME/helm - configuration is stored in $XDG_CONFIG_HOME/helm - data is stored in $XDG_DATA_HOME/helm By default, the default directories depend on the Operating System. The defaults are listed below: +------------------+---------------------------+--------------------------------+-------------------------+ | Operating System | Cache Path | Configuration Path | Data Path | +------------------+---------------------------+--------------------------------+-------------------------+ | Linux | $HOME/.cache/helm | $HOME/.config/helm | $HOME/.local/share/helm | | macOS | $HOME/Library/Caches/helm | $HOME/Library/Preferences/helm | $HOME/Library/helm | | Windows | %TEMP%\\helm | %APPDATA%\\helm | %APPDATA%\\helm | +------------------+---------------------------+--------------------------------+-------------------------+ Usage: helm [command] Available Commands: completion Generate autocompletions script for the specified shell (bash or zsh) create create a new chart with the given name dependency manage a chart's dependencies env Helm client environment information get download extended information of a named release help Help about any command history fetch release history install install a chart lint examines a chart for possible issues list list releases package package a chart directory into a chart archive plugin install, list, or uninstall Helm plugins pull download a chart from a repository and (optionally) unpack it in local directory repo add, list, remove, update, and index chart repositories rollback roll back a release to a previous revision search search for a keyword in charts show show information of a chart status displays the status of the named release template locally render templates test run tests for a release uninstall uninstall a release upgrade upgrade a release verify verify that a chart at the given path has been signed and is valid version print the client version information Flags: --add-dir-header If true, adds the file directory to the header --alsologtostderr log to standard error as well as files --debug enable verbose output -h, --help help for helm --kube-context string name of the kubeconfig context to use --kubeconfig string path to the kubeconfig file --log-backtrace-at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log-dir string If non-empty, write log files in this directory --log-file string If non-empty, use this log file --log-file-max-size uint Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) -n, --namespace string namespace scope for this request --registry-config string path to the registry config file (default \"/root/.config/helm/registry.json\") --repository-cache string path to the file containing cached repository indexes (default \"/root/.cache/helm/repository\") --repository-config string path to the file containing repository names and URLs (default \"/root/.config/helm/repositories.yaml\") --skip-headers If true, avoid header prefixes in the log messages --skip-log-headers If true, avoid headers when opening log files --stderrthreshold severity logs at or above this threshold go to stderr (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging Use \"helm [command] --help\" for more information about a command. helm template -f values-dev.yaml . helm del --purge logstash-producer-mysql-slowlog helm install --namespace logger --name logstash-producer-mysql-slowlog -f values.yaml . helm upgrade logstash-producer-mysql-slowlog -f values.yaml . Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-26 22:06:44 "},"origin/Kubernetes-helm-charts编写规则.html":{"url":"origin/Kubernetes-helm-charts编写规则.html","title":"helm charts编写规则","keywords":"","body":"Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-26 22:06:44 "},"origin/Service与SpringBoot应用启动参数冲突的问题排查及解决方案.html":{"url":"origin/Service与SpringBoot应用启动参数冲突的问题排查及解决方案.html","title":"Service与SpringBoot应用启动参数冲突的问题排查及解决方案","keywords":"","body":"kubernetes Service与SpringBoot应用启动参数冲突上下文部署Springboot应用到kubernetes集群 创建了名为\"server\"的Service 描述1.部署到kubernetes集群时,原因容器报错日志如下 Caused by: org.springframework.core.convert.ConversionFailedException: Failed to convert from type [java.lang.String] to type [java.lang.Integer] for value 'tcp://192.168.1.55:8080'; nested exception is java.lang.NumberFormatException: For input string: \"tcp://192.168.1.55:8080\" 2.当单独使用Docker部署时,则不报错3.报错表现为SpringBoot框架无法将字符串“tcp://192.168.1.55:8080”转换为数字类型原因SpringBoot应用会读取POD中系统环境变量\"SERVER_PORT\"的值作为应用监听的端口，值应为数字类型 而当该应用在kubernetes中的Service名字命名为\"server\"时,kubernetes会默认在该命名空间下所有的POD中注入一个名叫\"SERVER_PORT=tcp://service-ip地址:镜像Dockerfile暴露出来的端口\"环境变量(该值为字符类型，报错示例中的为\"SERVER_PORT=tcp://192.168.1.55:8080\")来进行POD间的服务发现 解决方案禁止SpringBoot应用部署在kubernetes中的Service名字命名为“server”，建议命名为“项目名-应用名” 在部署Deployment声明文件中添加\"SERVER_PORT=8080\"进行覆盖默认值 附录: Kubernetes中的服务发现创建Pod资源时，kubelet会将其所属名称空间内的每个活动的Service对象以一系列环境变量的形式注入其中。它支持使用Kubernetes Service环境变量以及与Docker的links兼容的变量。简单来说，服务发现就是服务或者应用之间互相定位的过程。不过，服务发现并非什么新概念，传统的单体应用架构时代也会用到，只不过单体应用的动态性不强，更新和重新发布频度较低，通常以月甚至以年计，基本不会进行自动伸缩，因此服务发现的概念无须显性强调。在传统的单体应用网络位置发生变化时，由IT运维人员手工更新一下相关的配置文件基本就能解决问题。但在微服务应用场景中，应用被拆分成众多的小服务，它们按需创建且变动频繁，配置信息基本无法事先写入配置文件中并及时跟踪反映动态变化，服务发现的重要性便随之凸显。服务发现机制的基本实现，一般是事先部署好一个网络位置较为稳定的服务注册中心（也称为服务总线），服务提供者（服务端）向注册中心注册自己的位置信息，并在变动后及时予以更新，相应地，服务消费者则周期性地从注册中心获取服务提供者的最新位置信息从而“发现”要访问的目标服务资源。复杂的服务发现机制还能够让服务提供者提供其描述信息、状态信息及资源使用信息等，以供消费者实现更为复杂的服务选择逻辑。实践中，根据其发现过程的实现方式，服务发现还可分为两种类型：客户端发现和服务端发现。客户端发现：由客户端到服务注册中心发现其依赖到的服务的相关信息，因此，它需要内置特定的服务发现程序和发现逻辑。服务端发现：这种方式额外要用到一个称为中央路由器或服务均衡器的组件；服务消费者将请求发往中央路由器或者负载均衡器，由它们负责查询服务注册中心获取服务提供者的位置信息，并将服务消费者的请求转发给服务提供者。由此可见，服务注册中心是服务发现得以落地的核心组件。事实上，DNS可以算是最为原始的服务发现系统之一，不过，在服务的动态性很强的场景中，DNS记录的传播速度可能会跟不上服务的变更速度，因此它不并适用于微服务环境。另外，传统实践中，常见的服务注册中心是ZooKeeper和etcd等分布式键值存储系统。不过，它们只能提供基本的数据存储功能，距离实现完整的服务发现机制还有大量的二次开发任务需要完成。另外，它们更注重数据一致性，这与有着更高的服务可用性要求的微服务发现场景中的需求不太相吻合。Netflix的Eureka是目前较流行的服务发现系统之一，它是专门开发用来实现服务发现的系统，以可用性目前为先，可以在多种故障期间保持服务发现和服务注册的功能可用，其设计原则遵从“存在少量的错误数据，总比完全不可用要好”。另一个同级别的实现是Consul，它是由HashiCorp公司提供的商业产品，不过还有一个开源基础版本提供。它于服务发现的基础功能之外还提供了多数据中心的部署能力等一众出色的特性。尽管传统的DNS系统不适于微服务环境中的服务发现，但SkyDNS项目（后来称kubedns）却是一个有趣的实现，它结合古老的DNS技术和时髦的Go语言、Raft算法并构建于etcd存储系统之上，为Kubernetes系统实现了一种服务发现机制。Service资源为Kubernetes提供了一个较为稳定的抽象层，这有点类似于服务端发现的方式，于是也就不存在DNS服务的时间窗口的问题。Kubernetes自1.3版本开始，其用于服务发现的DNS更新为了kubeDNS，而类似的另一个基于较新的DNS的服务发现项目是由CNCF（Cloud Native Computing Foundation）孵化的CoreDNS，它基于Go语言开发，通过串接一组实现DNS功能的插件的插件链进行工作。自Kubernetes 1.11版本起，CoreDNS取代kubeDNS成为默认的DNS附件。不过，Kubernetes依然支持使用环境变量进行服务发现。Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:51:58 "},"origin/jenkins-api.html":{"url":"origin/jenkins-api.html","title":"Jenkins API","keywords":"","body":"Jenkins API一、简介Jenkins API支持以下3种格式：XML JSON并支持JSONP跨域访问 Pythonhttps://jenkins-host/api/ Jenkins API没有统一的入口，而是采用“…/api/” 的REST API样式，其中”…” 表示Jenkins资源的URLAPI类型 说明 JobsAPI 任务管理（任务信息、创建、修改） PluginManagerAPI 插件管理（插件信息、安装插件） QueueAPI 任务队列相关（队列状态） StatisticsAPI Jenkins统计信息 CrumbIssuerAPI 系统哈希值信息（用于防御CSRF攻击） SystemAPI Jenkins系统状态（版本、路径）Jenkins 使用 Baisc Auth 的权限验证方式，需要传入 username 和 api token 。但在 Job 的远程触发中，可以设置用于远程触发的 token (在 Job 的配置页面设置)，这样在触发 Job 时就不需要传入 Basic Auth 了。 远程触发的 token 使用 urlencode 的方式放在请求的 body 中，其原始数据为： token=Basic Authcurl -X POST /view//job//build --user : Tokencurl -X POST /view//job//build --data-urlencode token= _class\": \"org.jenkinsci.plugins.workflow.job.WorkflowJob\"\"_class\": \"hudson.model.FreeStyleProject\"二、API1. 创建Job2. 创建视图3. 触发Jo构建4.Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-17 21:42:00 "},"origin/jenkins-SharedLibraries.html":{"url":"origin/jenkins-SharedLibraries.html","title":"Jenkins共享库Shared Libraries","keywords":"","body":"Jenkins Pipeline共享库Shared Libraries一、简介随着Job的增多和pipeline的功能越来越复杂，Pipeline代码冗余度高。所以可以将一些公共的pipeline抽象做成模块代码，在各种项目pipeline之间共享核心实现，同时可以放到SVM中进行版本控制。以减少冗余并保证所有job在构建的时候会调用最新的共享库代码。这时就可以用到pipline的共享库Shared Libraries功能。模块化 可重用性 二、共享库的目录结构共享库根目录 |-- vars |-- test1.groovy |-- src |-- test2.groovy |-- resources vars: 依赖于Jenkins运行环境的Groovy脚本。其中的Groovy脚本被称之为全局变量。 src: 标准的Java源码目录结构,其中的Groovy脚本被称为类库(Library class)。该目录所有下的所有类都一次性静态被添加到类路径classpath下 resources: 目录允许从外部库中使用 libraryResource 步骤来加载有关的非 Groovy 文件。 目前，内部库不支持该特性 三、配置全局共享库可在Jenkins中的Manage Jenkins –> Configure System　–> Global Pipeline Libraries 添加一个或多个全局的共享库，同时也可以在构建过程中的任何位置使用library step动作动态地配置引用共享库，详见动态引用共享库四、引用共享库1. 引用全局共享库格式：@Library('my-shared-library-1@$Branch/Tag','my-shared-library-1@$Branch/Tag') _#!groovy // 引用默认配置的共享库 @Library('demo-shared-library') _ // 引用指定分支、tag的共享库代码 @Library('demo-shared-library@1.0') _ // 引用多个指定分支tag的共享库 @Library('demo-shared-library@$Branch/Tag','demo-shared-library-test@$Branch/Tag') _ @Library('utils') import org.foo.Utilities @Library('utils') import static org.foo.Utilities.* 2. 动态引用共享库2.7版本后的Shared Groovy Libraries插件，增加了一个library的setp,可以随时在构建过程中引用共享库#!groovy library 'demo-shared-library@$BRANCH_NAME' library \"demo-shared-library@${params.LIB_VERSION}\" library('demo-shared-library').com.mycorp.pipeline.Utils.someStaticMethod() // 此时共享库的版本必须指定 library identifier: 'custom-lib@master', retriever: modernSCM( [$class: 'GitSCMSource', remote: 'git@git.mycorp.com:my-jenkins-utils.git', credentialsId: 'my-private-key']) 3. 调用第三方Java库@Grab('org.apache.commons:commons-math3:3.4.1') import org.apache.commons.math3.primes.Primes 引用完的第三方Java库后会缓存在Jenkins Master节点的~/.groovy/grapes/ 目录下五、全局变量和类库的编写规则和调用方法1. /var下定义的全局变量全局变量必须以全小写或驼峰（camelCased）命名以便于能够在流水线中正确的加载 /vars目录中的脚本根据需求以单例的方式实例化，这允许在单个.groovy` 文件中定义多个方法 /vars/*.groovy若实现call()方法，直接引用时默认执行其中的方法，该方法可以让全局变量以一种以类似于step的方式被调用/vars/log.groovy#!groovy def call(String name = 'human') { echo \"Hello, ${name}.\" } def info(message) { echo \"INFO: ${message}\" } def warning(message) { echo \"WARNING: ${message}\" } Jenkinsfile#!groovy @Library('demo-shared-library@1.0') _ log() // 输出\"Hello, human.\" log.info 'Starting' // 输出\"INFO: Starting\" log.warning 'Nothing to do!' // 输出\"WARNING: Nothing to do!\" 从2017年9月下旬发布的声明式 1.2开始，可以在全局变量中直接定义声明式流水线 /vars/evenOrOdd.groovy#!groovy def call(int buildNumber) { if (buildNumber % 2 == 0) { pipeline { agent any stages { stage('Even Stage') { steps { echo \"The build number is even\" } } } } } else { pipeline { agent any stages { stage('Odd Stage') { steps { echo \"The build number is odd\" } } } } } } Jenkinsfile#!groovy @Library('demo-shared-library@1.0') _ evenOrOdd(currentBuild.getNumber()) 全局变量的传参 /vars/buildPlugin.groovy#!groovy def call(Map config) { node { git url: \"https://github.com/jenkinsci/${config.name}-plugin.git\" sh 'mvn install' mail to: '...', subject: \"${config.name} plugin build\", body: '...' } } Jenkinsfile#!groovy @Library('demo-shared-library@1.0') _ buildPlugin name: 'git' 声明式流水线不允许在script指令之外使用全局变量 Jenkinsfile#!groovy @Library('demo-shared-library@1.0') _ pipeline { agent none stage ('Example') { steps { script { log.info 'Starting' log.warning 'Nothing to do!' } } } } 2. /src下定义的类库类库不能直接调用 sh或 git这样的步骤。 但是他们可以在封闭的类的范围之外实现方法，从而调用流水线步骤/src/org/foo/Zot.groovy#!groovy package org.foo; def checkOutFrom(repo) { git url: \"git@github.com:jenkinsci/${repo}\" } return this Jenkinsfile#!groovy @Library('demo-shared-library@1.0') _ def z = new org.foo.Zot() z.checkOutFrom(repo) 类库中使用”class“声明父类 /src/org/foo/Utilities.groovypackage org.foo class Utilities implements Serializable { def steps Utilities(steps) {this.steps = steps} def mvn(args) { steps.sh \"${steps.tool 'Maven'}/bin/mvn -o ${args}\" } } Jenkinsfile@Library('utils') import org.foo.Utilities def utils = new Utilities(this) node { utils.mvn 'clean package' } 类库中的方法访问流水线中的变量 /src/org/foo/Utilities.groovypackage org.foo class Utilities { static def mvn(script, args) { script.sh \"${script.tool 'Maven'}/bin/mvn -s ${script.env.HOME}/jenkins.xml -o ${args}\" } } Jenkinsfile@Library('utils') import static org.foo.Utilities.* node { mvn this, 'clean package' } 六、Jenkins Pipeline生成器生成动态引用共享库的代码Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:00 "},"origin/jenkins-声明式Declarative-pipeline语法.html":{"url":"origin/jenkins-声明式Declarative-pipeline语法.html","title":"声明式Declarative语法","keywords":"","body":"Jenkins声明式Declarative Pipeline一、语法结构Jenkins 2.5新加入的pipeline语法声明式pipeline 基本语法和表达式遵循 groovy语法，但是有以下例外：声明式pipeline 必须包含在固定格式的pipeline{}中 每个声明语句必须独立一行， 行尾无需使用分号 块(Blocks{}) 只能包含章节(Sections),指令（Directives）,步骤(Steps),或者赋值语句 属性引用语句被视为无参数方法调用。 如input() 一个声明式Pipeline中包含的元素pipeline：声明这是一个声明式的pipeline脚本 agent：指定要执行该Pipeline的节点（job运行的slave或者master节点） stages：阶段集合，包裹所有的阶段（例如：打包，部署等各个阶段） stage：阶段，被stages包裹，一个stages可以有多个stage steps：步骤,为每个阶段的最小执行单元,被stage包裹 post：执行构建后的操作，根据构建结果来执行对应的操作 示例：pipeline{ // 指定pipeline在哪个slave节点上允许 agent { label 'jdk-maven' } // 指定pipeline运行时的一些配置 option { timeout(time: 1, unit: 'HOURS') } // 自定义的参数 parameters { string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?') text(name: 'BIOGRAPHY', defaultValue: '', description: 'Enter some information about the person') booleanParam(name: 'TOGGLE', defaultValue: true, description: 'Toggle this value') choice(name: 'CHOICE', choices: ['One', 'Two', 'Three'], description: 'Pick something') password(name: 'PASSWORD', defaultValue: 'SECRET', description: 'Enter a password') } // 自定义的环境变量 environment { Gitlab_Deploy_KEY = credentials('gitlab-jenkins-depolykey') } // 定义pipeline的阶段任务 stages { stage (\"阶段1任务：拉代码\") { steps { // 拉代码的具体命令 } } stage (\"阶段2任务：编译代码\") { steps { // 编译代码的具体命令 } } stage (\"阶段3任务：扫描代码\") { steps { // 拉代码的具体命令 } } stage (\"阶段4任务：打包代码\") { steps { // 打包代码的具体命令 } } stage (\"阶段5任务：构建推送Docker镜像\") { steps { // 构建推送Docker镜像的具体命令 } } stage (\"阶段6任务：部署镜像\") { steps { // 部署镜像的具体命令 } } } post { success { // 当pipeline构建状态为\"success\"时要执行的事情 } always { // 无论pipeline构建状态是什么都要执行的事情 } } } 二、章节Sections1、agent（必须）指定整个Pipeline或特定阶段是在Jenkins Master节点还是Jenkins Slave节点上运行。可在顶级pipeline块和每个stage块中使用（在顶层pipeline{}中是必须定义的 ，但在阶段Stage中是可选的）参数（以下参数值在顶层pipeline{}和stage{}中都可使用）：any：在任何可用的节点上执行Pipeline或Stage none：当在顶层pipeline{}中应用时，将不会为整个Pipeline运行分配全局代理，并且每个stage部分将需要包含其自己的agent部分 label node docker dockerfile kubernetes 公用参数：label customWorkspace reuseNode args 2、post定义在Pipeline运行或阶段结束时要运行的操作。具体取决于Pipeline的状态支持pipeline运行状态:always：无论Pipeline运行的完成状态如何都要运行 changed：只有当前Pipeline运行的状态与先前完成的Pipeline的状态不同时，才能运行 fixed：整个pipeline或者stage相对于上一次失败或不稳定Pipeline的状态有改变。才能运行 regression： aborted：只有当前Pipeline处于“中止”状态时，才会运行，通常是由于Pipeline被手动中止（通常在具有灰色指示的Web UI 中表示） failure：仅当当前Pipeline处于“失败”状态时才运行（通常在Web UI中用红色指示表示） success：仅当当前Pipeline在“成功”状态时才运行（通常在具有蓝色或绿色指示的Web UI中表示） unstable：只有当前Pipeline在不稳定”状态，通常由测试失败，代码违例等引起，才能运行（通常在具有黄色指示的Web UI中表示） unsuccessful： cleanup：无论Pipeline或stage的状态如何，在跑完所有其他的post条件后运行此条件下 的post步骤。 3、stages（必须）至少包含一个用于执行任务的stage指令 pipeline{ }中只能有一个stages{} 4、steps（必须）在stage指令中至少包含一个用于执行命令的steps 三、指令Directives1、Environment环境变量environment{…},使用键值对来定义一些环境变量并赋值。它的作用范围，取决environment{…}所写的位置。写在顶层环境变量，可以让所有stage下的step共享这些变量；也可以单独定义在某一个stage下，只能供这个stage去调用变量，其他的stage不能共享这些变量。一般来说，我们基本上上定义全局环境变量，如果是局部环境变量，我们直接用def关键字声明就可以，没必要放environment{…}里面。同时，environment{…}支持credentials() 方法来访问预先在Jenkins保存的凭据，并赋值给环境变量credentials() 支持的凭据类型：Secret Text Secret File Username and password：使用变量名_USR and 变量名_PSW 来获取其中的用户名和Passwordpipeline { agent any stages { stage('Example Username/Password') { environment { SERVICE_CREDS = credentials('my-prefined-username-password') } steps { sh 'echo \"Service user is $SERVICE_CREDS_USR\"' sh 'echo \"Service password is $SERVICE_CREDS_PSW\"' sh 'curl -u $SERVICE_CREDS https://myservice.example.com' } } } } SSH with Private Key pipeline { agent any stages { stage('Example Username/Password') { environment { SSH_CREDS = credentials('my-prefined-ssh-creds') } steps { sh 'echo \"SSH private key is located at $SSH_CREDS\"' sh 'echo \"SSH user is $SSH_CREDS_USR\"' sh 'echo \"SSH passphrase is $SSH_CREDS_PSW\"' } } } } 2、Parameters参数pipeline{ }中只能有一个parameters{} 参数定义格式parameters { 参数类型(name: '参数名', defaultValue: '默认值', description: '描述') } 参数类型string text boobleanParam choice password 参数调用格式：${params.参数名}示例：pipeline { agent any parameters { string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?') text(name: 'BIOGRAPHY', defaultValue: '', description: 'Enter some information about the person') booleanParam(name: 'TOGGLE', defaultValue: true, description: 'Toggle this value') choice(name: 'CHOICE', choices: ['One', 'Two', 'Three'], description: 'Pick something') password(name: 'PASSWORD', defaultValue: 'SECRET', description: 'Enter a password') } stages { stage('Example') { steps { echo \"Hello ${params.PERSON}\" echo \"Biography: ${params.BIOGRAPHY}\" echo \"Toggle: ${params.TOGGLE}\" echo \"Choice: ${params.CHOICE}\" echo \"Password: ${params.PASSWORD}\" } } } } 3、Options选项pipeline{ }中只能有一个options{} buildDiscarder checkoutToSubdirectory disableConcurrentBuilds disableResume newContainerPerStage overrideIndexTriggers preserveStashes quietPeriod retry skipDefaultCheckout skipStagesAfterUnstable timeout timestamps parallelsAlwaysFailFast 4、Triggers触发器pipeline{ }中只能有一个triggers {} 触发器类型cron pollSCM upstream Jenkins的Cron语法5、Stage阶段(至少有一个)包含在stages{}中 至少有一个 6、Tools工具包含在pipeline{}或stage{} 支持的工具：Maven JDK Gradle 7、Input用户输入8、When条件内置条件：branchExecute the stage when the branch being built matches the branch pattern given, for example: when { branch 'master' }. Note that this only works on a multibranch Pipeline. buildingTagExecute the stage when the build is building a tag. Example: when { buildingTag() } changelogExecute the stage if the build’s SCM changelog contains a given regular expression pattern, for example: when { changelog '.*^\\[DEPENDENCY\\] .+$' } changesetExecute the stage if the build’s SCM changeset contains one or more files matching the given string or glob. Example: when { changeset \"*/.js\" }By default the path matching will be case insensitive, this can be turned off with the caseSensitive parameter, for example: when { changeset glob: \"ReadMe.*\", caseSensitive: true } changeRequestExecutes the stage if the current build is for a \"change request\" (a.k.a. Pull Request on GitHub and Bitbucket, Merge Request on GitLab or Change in Gerrit etc.). When no parameters are passed the stage runs on every change request, for example: when { changeRequest() }.By adding a filter attribute with parameter to the change request, the stage can be made to run only on matching change requests. Possible attributes are id, target, branch, fork, url, title, author, authorDisplayName, and authorEmail. Each of these corresponds to a CHANGE_* environment variable, for example: when { changeRequest target: 'master' }.The optional parameter comparator may be added after an attribute to specify how any patterns are evaluated for a match: EQUALS for a simple string comparison (the default), GLOB for an ANT style path glob (same as for example changeset), or REGEXP for regular expression matching. Example: when { changeRequest authorEmail: \"[\\w_-.]+@example.com\", comparator: 'REGEXP' } environmentExecute the stage when the specified environment variable is set to the given value, for example: when { environment name: 'DEPLOY_TO', value: 'production' } equalsExecute the stage when the expected value is equal to the actual value, for example: when { equals expected: 2, actual: currentBuild.number } expressionExecute the stage when the specified Groovy expression evaluates to true, for example: when { expression { return params.DEBUG_BUILD } } Note that when returning strings from your expressions they must be converted to booleans or return null to evaluate to false. Simply returning \"0\" or \"false\" will still evaluate to \"true\". tagExecute the stage if the TAG_NAME variable matches the given pattern. Example: when { tag \"release-*\" }. If an empty pattern is provided the stage will execute if the TAG_NAME variable exists (same as buildingTag()).The optional parameter comparator may be added after an attribute to specify how any patterns are evaluated for a match: EQUALS for a simple string comparison, GLOB (the default) for an ANT style path glob (same as for example changeset), or REGEXP for regular expression matching. For example: when { tag pattern: \"release-\\d+\", comparator: \"REGEXP\"} notExecute the stage when the nested condition is false. Must contain one condition. For example: when { not { branch 'master' } } allOfExecute the stage when all of the nested conditions are true. Must contain at least one condition. For example: when { allOf { branch 'master'; environment name: 'DEPLOY_TO', value: 'production' } } anyOfExecute the stage when at least one of the nested conditions is true. Must contain at least one condition. For example: when { anyOf { branch 'master'; branch 'staging' } } triggeredByExecute the stage when the current build has been triggered by the param given. For example:when { triggeredBy 'SCMTrigger' } when { triggeredBy 'TimerTrigger' } when { triggeredBy 'UpstreamCause' } when { triggeredBy cause: \"UserIdCause\", detail: \"vlinde\" } 未完待整理更新Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:01 "},"origin/Jenkins-在Kubernetes上使用Kubernetes插件动态创建Slave节点.html":{"url":"origin/Jenkins-在Kubernetes上使用Kubernetes插件动态创建Slave节点.html","title":"Kubernetes Plugin","keywords":"","body":"Jenkins在Kubernetes上使用Kubernetes插件动态创建Slave节点一、Context插件GIthub地址：https://github.com/jenkinsci/kubernetes-pluginJenkins 分布式架构是由一个 Master 和多个 Slave Node组成的分布式架构。在 Jenkins Master 上管理你的项目，可以把你的一些构建任务分担到不同的 Slave Node 上运行，Master 的性能就提高了。Master/Slave相当于Server和agent的概念。Master提供web接口让用户来管理job和slave，job可以运行在master本机或者被分配到slave上运行构建。一个master（jenkins服务所在机器）可以关联多个slave用来为不同的job或相同的job的不同配置来服务。 传统的 Jenkins Slave 一主多从式会存在一些痛点。比如：主 Master 发生单点故障时，整个流程都不可用了； 每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致 管理起来非常不方便，维护起来也是比较费劲； 资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态； 资源有浪费，每台 Slave 可能是实体机或者 VM，当 Slave 处于空闲状态时，也不会完全释放掉资源。 而使用Kubernetes插件可以在Kubernetes上动态创建slave POD作为Slave节点。Jenkins Master 和 Slave 节点以 Docker Container 形式运行在 Kubernetes 集群的 Node 上，Master 运行在其中一个节点，并且将其配置数据存储到一个 Volume 上去，Slave 运行在各个节点上，并且它不是一直处于运行状态，它会按照需求动态的创建并自动删除。这种方式的工作流程大致为：当 Jenkins Master 接收到 Build 请求时，会根据配置的 Label 动态创建一个运行在 Docker Container 中的 Jenkins Slave 并注册到 Master 上，当运行完 Job 后，这个 Slave 会被注销并且 Docker Container 也会自动删除，恢复到最初状态。这种方式带来的好处有很多： 1. 服务高可用，当 Jenkins Master 出现故障时，Kubernetes 会自动创建一个新的 Jenkins Master 容器，并且将 Volume 分配给新创建的容器，保证数据不丢失，从而达到集群服务高可用。 2. 动态伸缩，合理使用资源，每次运行 Job 时，会自动创建一个 Jenkins Slave，Job 完成后，Slave 自动注销并删除容器，资源自动释放，而且 Kubernetes 会根据每个资源的使用情况，动态分配 Slave 到空闲的节点上创建，降低出现因某节点资源利用率高，还排队等待在该节点的情况。 3. 扩展性好，当 Kubernetes 集群的资源严重不足而导致 Job 排队等待时，可以很容易的添加一个 Kubernetes Node 到集群中，从而实现扩展。二、Jenkins与Slave的连接方式Jenkins的Master/Slave相当于Server和agent的概念。Master提供web接口让用户来管理job和slave，job可以运行在master本机或者被分配到slave上运行。一个master可以关联多个slave用来为不同的job或相同的job的不同配置来服务。当job被分配到slave上运行的时候，此时master和slave其实是建立的双向字节流的连接，其中连接方法主要有如下几种： 1. SSH：Jenkins内置有ssh客户端实现，可以用来与远程的sshd通信，从而启动slave agent。这是对_unix系统的slave最方便的方法，因为_unix系统一般默认安装有sshd。在创建ssh连接的slave的时候，你需要提供slave的host名字，用户名和ssh证书。创建public/private keys，然后将public key拷贝到slave的~/.ssh/authorized_keys中，将private key 保存到master上某ppk文件中。jenkins将会自动地完成其他的配置工作，例如copy slave agent的binary，启动和停止slave。Java web start（JNLP：Java Network Lancher Protocol）：jnlp连接方式有个好处就是不用master和slave之间能够ssh连接，只需要能够ping即可。并且如果slave的机器是windows的话，也是可以的这个其实是非常实用的 WMI+DCOM：对于Windows的Slave，Jenkins可以使用Windows2000及以后内置的远程管理功能（WMI+DCOM），你只需要提供对slave有管理员访问权限的用户名和密码，jenkins将远程地创建windows service然后远程地启动和停止他们。对于windows的系统，这是最方便的方法，但是此方法不允许运行有显示交互的GUI程序。 在Kubernetes上的Jenkins通过Kubernetes插件动态创建的Slave POD节点是通过JNLP的方式与Jenkins Master进行通信的！ 三、Jenkins Kubernetes插件的安装配置安装 配置 四、定制Slave镜像Slave镜像中安装的软件信息 工具 版本 说明 Oracel JDK 1.8.0_171 Maven编译打包时使用 Apache Maven 3.6.1 在Slave容器中使用MAVEN编译打包源代码 helm v2.13.1 helm客户端 git 1.8.3.1 git命令 docker client 1.13.1 Dockers客户端，用于在Slave容器中构建应用镜像 sonar-scanner 3.3.0.1492 用于扫描源代码 FROM centos:7.4.1708 ENV TZ=Asia/Shanghai \\ LANG=en_US.UTF-8 \\ JDK_VERSION=Oracle_1.8.0_171 \\ MAVEN_VERSION=Apache_3.6.1 \\ HOME=/home/jenkins \\ MAVEN_HOME=/opt/apache-maven-3.6.1 \\ JAVA_HOME=/opt/jdk1.8.0_171 \\ SONARSCANNER_HOME=/opt/sonar-scanner-3.3.0.1492-linux COPY jdk1.8.0_171 /opt/jdk1.8.0_171 COPY apache-maven-3.6.1 /opt/apache-maven-3.6.1 COPY helm /usr/bin/helm COPY sonar-scanner-3.3.0.1492-linux /opt/sonar-scanner-3.3.0.1492-linux RUN curl https://download.docker.com/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo \\ && yum makecache \\ && yum install -y git docker-ce-cli make \\ && yum clean all \\ && groupadd -g 1000 jenkins \\ && useradd -c \"Jenkins user\" -d /home/jenkins -u 1000 -g 0 -m jenkins \\ && mkdir /home/jenkins/.m2 \\ && chown -R 1000.0 /home/jenkins \\ && ln -s /opt/jdk1.8.0_171/bin/java /usr/bin/java \\ && ln -s /opt/apache-maven-3.6.1/bin/mvn /usr/bin/mvn \\ && ln -s /opt/sonar-scanner-3.3.0.1492-linux/bin/sonar-scanner /usr/bin/sonar-scanner USER jenkins WORKDIR /home/jenkins COPY dumb-init /usr/bin/dumb-init ADD run-jnlp-client /usr/bin/ ENTRYPOINT [\"/usr/bin/dumb-init\", \"--\", \"/usr/bin/run-jnlp-client\"] 五、Pipeline或者Job中使用验证Job创建一个自由风格的Job点击构建后，会自动创建一个Slave POD，并通过JNLP协议与Jenkins Master的Agent端口5000进行通通信 Declarative Pipeline pipeline { agent { label 'maven' } stages { stage (\"代码编译\") { steps { configFileProvider([configFile(fileId: 'nexus-maven-settings', targetLocation: 'settings.xml')]){ sh 'mvn -s settings.xml compile' } } } stage(\"代码扫描\"){ steps{ sh \"sonar-scanner \\ -Dsonar.projectName=demo-springboot2 \\ -Dsonar.projectKey=demo-springboot2 \\ -Dsonar.sources=src \\ -Dsonar.host.url=http://sonarqube.apps.okd311.curiouser.com \\ -Dsonar.login=****** \\ -Dsonar.java.binaries=. \\ -Dsonar.sourceEncoding=UTF-8 \\ -Dsonar.java.source=8 \\ -Dsonar.gitlab.project_id=1 \\ -Dsonar.issuesReport.html.enable=true \\ -Dsonar.gitlab.commit_sha=$GIT_COMMIT \\ -Dsonar.gitlab.ref_name=$GIT_BRANCH \\ -Dsonar.gitlab.user_token=***** \\ -Dsonar.gitlab.url=http://gitlab.apps.okd311.curiouser.com/ \\ -Dsonar.gitlab.ignore_certificate=true \\ -Dsonar.gitlab.comment_no_issue=true \\ -Dsonar.gitlab.max_global_issues=1000 \\ -Dsonar.gitlab.unique_issue_per_inline=true\" } } stage (\"代码打包\") { steps { sh \"mvn -s settings.xml package\" } } stage(\"上传制品\"){ steps{ script{ def pomfile = readMavenPom file: 'pom.xml' sh \"curl -sL -w 'Upload the jar to the repository status code: %{http_code}\\n' -u devops:**** \" + \"--upload-file target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging} \" + \"http://nexus.apps.okd311.curiouser.com/repository/jenkins-product-repo/${pomfile.artifactId}-${pomfile.version}-${env.GIT_COMMIT}.${pomfile.packaging}\" } } } stage(\"构建应用镜像\"){ steps{ sh 'docker login -p ********** -u unused docker-registry-default.apps.okd311.curiouser.com' sh 'make' } } } post { always { emailext attachLog: true, body: ''' 构建任务的完整日志详见见附件,Jenkins查看链接: $BUILD_URL''', subject: '$PROJECT_NAME的第$BUILD_NUMBER次构建$BUILD_STATUS !', to: '*******@163.com' } } } 参考博客：https://github.com/easzlab/kubeasz/blob/master/docs/guide/jenkins.md https://www.qikqiak.com/k8s-book/docs/36.Jenkins%20Slave.html https://jenkins.io/blog/2018/09/14/kubernetes-and-secret-agents/ https://www.jianshu.com/p/1440b5b4b980 https://www.cnblogs.com/guguli/p/7827435.html https://blog.csdn.net/felix_yujing/article/details/78725142 https://jicki.me/kubernetes/2018/02/08/kubernetes-jenkins/ https://testerhome.com/topics/17251 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-12 22:47:23 "},"origin/jenkins-pipeline-utility-steps.html":{"url":"origin/jenkins-pipeline-utility-steps.html","title":"Pipeline Utility Steps","keywords":"","body":"Jenkins Pipeline-Utility-steps插件一、Pipeline-Utility-steps插件简介Jekins中的Pipeline-Utility-steps插件能让你在pipeline的Step中直接使用它的API方法进行某些操作，例如查找文件，读取YAML/JSON/Properties文件、读取Maven工程POM文件等。这些方法有一个前提，任何文件都需要放在jenkins的workspace下，执行的job才能去找到文件。Github地址：https://github.com/jenkinsci/pipeline-utility-steps-plugin相关文档：https://jenkins.io/doc/pipeline/steps/pipeline-utility-steps/二、Pipeline-Utility-steps插件的方法File SystemfindFiles：根据一些字符串规则去查找文件，如果有匹配的查找，返回是一个fille数组对象。（文档）参数excludes(可选，参数类型为String) glob(可选，参数类型为String) 示例def files = findFiles(glob: '**/TEST-*.xml') echo \"\"\"${files[0].name} ${files[0].path} ${files[0].directory} ${files[0].length} ${files[0].lastModified}\"\"\" touch：创建文件（如果文件不存在的话）并设置时间戳. Returns a FileWrapper representing the file that was touched. (文档)参数file(参数类型为String)：The path to the file to touch. timestamp(可选，参数类型为long)：The timestamp to set (number of ms since the epoc), leave empty for current system time. sha1：计算指定文件的SHA1 (文档)参数file(参数类型为String): The path to the file to hash. tee：将输出重定向到文件参数file(参数类型为String) Zip Fileszip：创建Zip文件. (文档)参数zipFile(参数类型为String): The name/path of the zip file to create. archive(可选，参数类型为boolean): If the zip file should be archived as an artifact of the current build. The file will still be kept in the workspace after archiving. dir(可选，参数类型为String): The path of the base directory to create the zip from. Leave empty to create from the current working directory. glob(可选，参数类型为String): Ant style pattern of files to include in the zip. Leave empty to include all files and directories. unzip：解压或读取Zip文件 (文档)参数 zipFile(参数类型为String): The name/path of the zip file to extract. charset(可选，参数类型为String): Specify which Charset you wish to use eg. UTF-8 dir(可选，参数类型为String): The path of the base directory to extract the zip to. Leave empty to extract in the current working directory. glob(可选，参数类型为String): Ant style pattern of files to extract from the zip. Leave empty to include all files and directories. quiet(可选，参数类型为boolean): Suppress the verbose output that logs every single file that is dealt with. E.g. unzip zipFile: 'example.zip', quiet: true read(可选，参数类型为boolean): Read the content of the files into a Map instead of writing them to the workspace. The keys of the map will be the path of the files read. E.g. def v = unzip zipFile: 'example.zip', glob: '*.txt', read: true String version = v['version.txt'] test(可选，参数类型为boolean): Test the integrity of the archive instead of extracting it. When this parameter is enabled, all other parameters (except for zipFile) will be ignored. The step will return true or false depending on the result instead of throwing an exception. Configuration FilesreadProperties：Reads a file in the current working directory or a String as a plain text Java Properties file. The returned object is a normal Map with String keys. The map can also be pre loaded with default values before reading/parsing the data. (文档)参数defaults (可选，Nested Choice of Objects): An Map containing default key/values. These are added to the resulting map first. file(可选，参数类型为String): path to a file in the workspace to read the properties from. These are added to the resulting map after the defaults and so will overwrite any key/value pairs already present. interpolate (可选，参数类型为boolean): Flag to indicate if the properties should be interpolated or not. In case of error or cycling dependencies, the original properties will be returned. text (可选，参数类型为String): An String containing properties formatted data. These are added to the resulting map after file and so will overwrite any key/value pairs already present. 示例 def d = [test: 'Default', something: 'Default', other: 'Default'] def props = readProperties defaults: d, file: 'dir/my.properties', text: 'other=Override' assert props['test'] == 'One' assert props['something'] == 'Default' assert props.something == 'Default' assert props.other == 'Override' def props = readProperties interpolate: true, file: 'test.properties' assert props.url = 'http://localhost' assert props.resource = 'README.txt' // if fullUrl is defined to ${url}/${resource} then it should evaluate to http://localhost/README.txt assert props.fullUrl = 'http://localhost/README.txt' readManifest：Reads a Jar Manifest file or text and parses it into a set of Maps. The returned data structure has two properties: main for the main attributes, and entries containing each individual section (except for main). (文档)参数file(可选，参数类型为String。值只能是file或text，两者不能同时设置): path to a file to read. It could be a plain text, .jar, .war or .ear. In the latter cases the manifest will be extracted from the archive and then read. text(可选，参数类型为String。值只能是file或text，两者不能同时设置): text containing the manifest data. 示例 def man = readManifest file: 'target/my.jar' assert man.main['Version'] == '6.15.8' assert man.main['Application-Name'] == 'My App' assert man.entries['Section1']['Key1'] == 'value1-1' assert man.entries['Section2']['Key2'] == 'value2-2' readYaml：Reads a file in the current working directory or a String as a plain text YAML file. It uses SnakeYAML as YAML processor. The returned objects are standard Java objects like List, Long, String, ...: bool: [true, false, on, off] int: 42 float: 3.14159 list: ['LITE', 'RES_ACID', 'SUS_DEXT'] map: {hp: 13, sp: 5}. (文档)参数file(可选，参数类型为String) text(可选，参数类型为String) 示例 // 读取单个YAML文件 def datas = readYaml text: \"\"\" something: 'my datas' size: 3 isEmpty: false \"\"\" assert datas.something == 'my datas' assert datas.size == 3 assert datas.isEmpty == false // 读取多个YAML文件 def datas = readYaml text: \"\"\" --- something: 'my first document' --- something: 'my second document' \"\"\" assert datas.size() == 2 assert datas[0].something == 'my first document' assert datas[1].something == 'my second document' // With file dir/my.yml containing something: 'my datas' : def datas = readYaml file: 'dir/my.yml', text: \"something: 'Override'\" assert datas.something == 'Override' writeYaml：Write a YAML file from an object. (文档)参数file(参数类型为String): Mandatory path to a file in the workspace to write the YAML datas to. data: A Mandatory Object containing the data to be serialized. charset: Optionally specify the charset to use when writing the file. Defaults to UTF-8 if nothing else is specified. What charsets that are available depends on your Jenkins master system. The java specification tells us though that at least the following should be available: [ US-ASCII、ISO-8859-1、UTF-8、UTF-16BE、UTF-16LE、UTF-16] 示例```groovy def amap = ['something': 'my datas', 'size': 3, 'isEmpty': false] writeYaml file: 'datas.yaml', data: amap def read = readYaml file: 'datas.yaml' assert read.something == 'my datas' assert read.size == 3 assert read.isEmpty == false \\`\\`\\` readJSON：Reads a file in the current working directory or a String as a plain text JSON file. The returned object is a normal Map with String keys or a List of primitives or Map. (文档)参数file(可选，参数类型为String。值只能是file或text，两者不能同时设置): Path to a file in the workspace from which to read the JSON data. Data could be access as an array or a map. text(可选，参数类型为String。值只能是file或text，两者不能同时设置): A string containing the JSON formatted data. Data could be access as an array or a map. 示例```groovy def props = readJSON file: 'dir/input.json' assert props['attr1'] == 'One' assert props.attr1 == 'One' def props = readJSON text: '{ \"key\": \"value\" }' assert props['key'] == 'value' assert props.key == 'value'def props = readJSON text: '[ \"a\", \"b\"]' assert props[0] == 'a' assert props[1] == 'b' - `writeJSON`：Write a [JSON](http://www.json.org/json-it.html) file in the current working directory. That for example was previously read by `readJSON`. ([文档](https://github.com/jenkinsci/pipeline-utility-steps-plugin/blob/master/src/main/resources/org/jenkinsci/plugins/pipeline/utility/steps/json/WriteJSONStep/help.html)) - 参数 - file(可选，参数类型为String): Path to a file in the workspace to write to. - json(Nested Choice of Objects): The JSON object to write. - pretty (可选，参数类型为int): Prettify the output with this number of spaces added to each level of indentation. - 示例 ```groovy def input = readJSON file: 'myfile.json' //Do some manipulation writeJSON file: 'output.json', json: input //or pretty print it, indented with a configurable number of spaces writeJSON file: 'output.json', json: input, pretty: 4 readCSV：Reads a file in the current working directory or a String as a plain text. A List of CSVRecord instances is returned. (文档)参数file(可选，参数类型为String。值只能是file或text，两者不能同时设置): Path to a file in the workspace from which to read the CSV data. Data is accessed as a List of String Array. text(可选，参数类型为String。值只能是file或text，两者不能同时设置): A string containing the CSV formatted data. Data is accessed as a List of String Arrays. format(可选，org.apache.commons.csv.CSVFormat) 示例 def records = readCSV file: 'dir/input.csv' assert records[0][0] == 'key' assert records[1][1] == 'b' def content = readCSV text: 'key,value\\na,b' assert records[0][0] == 'key' assert records[1][1] == 'b' // 进阶示例 def excelFormat = CSVFormat.EXCEL def records = readCSV file: 'dir/input.csv', format: excelFormat assert records[0][0] == 'key' assert records[1][1] == 'b' def content = readCSV text: 'key,value\\na,b', format: CSVFormat.DEFAULT.withHeader() assert records[1].get('key') == 'a' assert records[1].get('value') == 'b' writeCSV：Write a CSV file in the current working directory. That for example was previously read by readCSV. See CSVPrinter for details.(文档) 参数file(参数类型为String): Path to a file in the workspace to write to. records(java.lang.Iterable): The list of CSVRecord instances to write. format(可选，org.apache.commons.csv.CSVFormat):See CSVFormat for details. 示例 def records = [['key', 'value'], ['a', 'b']] writeCSV file: 'output.csv', records: records, format: CSVFormat.EXCEL Maven ProjectsreadMavenPom：读取Maven POM文件到一个Model数据结构中. (文档)参数file(可选，参数类型为String)：默认读取目前工作区下的POM.xml文件 示例 stage ('上传制品') { steps { script{ def pomfile = readMavenPom file: 'pom.xml' nexusPublisher nexusInstanceId: 'curiouser-okd-nexus', \\ nexusRepositoryId: 'Maven-Releases', \\ packages: [[$class: 'MavenPackage', \\ mavenAssetList: [[classifier: '', extension: '', \\ filePath: \"target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging}\"]], \\ mavenCoordinate: [artifactId: \"${pomfile.artifactId}\", \\ groupId: \"${pomfile.groupId}\", \\ packaging: \"${pomfile.packaging}\", \\ version: \"${pomfile.version}\"]]] } } } writeMavenPom：Writes a Maven project file. That for example was previously read by readMavenPom. (文档)参数 model(参数类型为org.apache.maven.model.Model): The Model object to write.file(可选，参数类型为String): Optional path to a file in the workspace to write to. If left empty the step will write to pom.xml in the current working directory. 示例 def pom = readMavenPom file: 'pom.xml' //Do some manipulation writeMavenPom model: pom Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:00 "},"origin/jenkins-Nexus-Platform的使用.html":{"url":"origin/jenkins-Nexus-Platform的使用.html","title":"Nexus Platform Plugin","keywords":"","body":"Preflight官方插件文档：https://help.sonatype.com/integrations/nexus-and-continuous-integration/nexus-platform-plugin-for-jenkins 安装插件：Pipeline Utility Steps 功能： 一、安装二、配置系统管理--> 系统设置--> Sonatype Nexus 三、使用上传构建后的制品到Nexus的Hosted类型仓库中JobDeclarative Pipeline stage ('上传制品') { steps { script{ //读取源代码中的POM文件，获取生成制品的maven坐标信息（Jenkins需要安装pipeline-utility-steps插件） def pomfile = readMavenPom file: 'pom.xml' //使用Nexus Platform插件上传maven制品到Nexus的maven格式release仓库 nexusPublisher nexusInstanceId: 'curiouser-okd-nexus', \\ nexusRepositoryId: 'Maven-Releases', \\ packages: [[$class: 'MavenPackage', \\ mavenAssetList: [[classifier: '', extension: '', \\ filePath: \"target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging}\"]], \\ mavenCoordinate: [artifactId: \"${pomfile.artifactId}\", \\ groupId: \"${pomfile.groupId}\", \\ packaging: \"${pomfile.packaging}\", \\ version: \"${pomfile.version}\"]]] //拼接maven制品的搜索链接,该链接是以源代码POM文件中的maven制品坐标信息参数对nexus api进行搜索，返回的response会重定向到制品的下载链接 echo \"The Jar Format Asset of Maven have been pushed to Hosted Repository: Maven-Release. The Download URL of the Asset: http://nexus-nexus.apps.okd311.curiouser.com/service/rest/v1/search../assets/download?maven.groupId=${pomfile.groupId}&maven.artifactId=${pomfile.artifactId}&maven.baseVersion=${pomfile.version}&maven.extension=jar&maven.classifier\" } } } 四、注意如果Job再次构建，产生相同的Jar，上传信息还是一样的，Nexus的Release仓库需要设置为\"允许Redeploy\"。不然，仓库中已经相同版本信息的制品，会造成上传失败参考链接https://support.sonatype.com/hc/en-us/articles/115009108987-Jenkins-Publish-Using-Maven-Coordinates-from-the-pom-xml https://www.jianshu.com/p/29403ecf7fc2 https://stackoverflow.com/questions/37603619/extract-version-id-from-pom-in-a-jenkins-pipeline Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:00 "},"origin/jenkins-配置SMTP邮箱服务.html":{"url":"origin/jenkins-配置SMTP邮箱服务.html","title":"Mail Plugin","keywords":"","body":"Jenkins配置SMTP邮箱服务Prerequisite自己邮箱运营商设置了开通SMTP服务 Jenkins 安装了Jenkins Mailers Plugin 一、ContextJenkins默认有个插件叫\"Mailer Plugin\"用来发送通知邮件。该插件使用的\"JavaMail \"来进行配置自定义个邮箱服务器二、配置系统管理-->系统设置 1. 配置Jenkins的系统管理员邮箱地址 2. 配置SMTP邮件服务器地址 三、使用Job中 四、问题当构建不成功时发送的邮件，内容包含构建的日志。 当初次构建成功时会发送邮件通知，当再次重复构建成功时，则不会发送邮件通知，得等到构建失败时才会再次发送通知邮件功能太弱，可使用\"Mail Extension\"插件进行功能扩展。详见：jenkins-Mailer邮箱功能扩展插件Email-Extension 不知Jenkins的系统管理员邮箱时，发送会报错 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:01 "},"origin/jenkins-Mailer邮箱功能扩展插件Email-Extension.html":{"url":"origin/jenkins-Mailer邮箱功能扩展插件Email-Extension.html","title":"Mail Extension","keywords":"","body":"Jenkins Mailer邮箱功能扩展插件Email-Extension一、ContextJenkins自带的邮件插件功能太弱，有个邮箱扩展插件。官方文档WIKI：https://wiki.jenkins.io/display/JENKINS/Email-ext+plugin优势：邮件格式改为HTML，更美观 使用模板来配置邮件内容 为不同的Job配置不一样的收件人 为不同的事件配置不一样的trigger 在Jenkins pipeline中集成发送邮件通知功能 二、插件安装配置1、安装2、配置三、使用1、Jobs中 2、Pipeline中pipeline{ ...上文省略... post { always { emailext attachLog: true, body: ''' 构建任务的完整日志详见见附件,Jenkins查看链接: $BUILD_URL''', subject: '$PROJECT_NAME的第$BUILD_NUMBER次构建$BUILD_STATUS !', to: '*******@163.com' } } } 四、发送HTML格式的邮件1、Pipeline中Prerequisite准备格式化好的HTML ${ENV, var=\"JOB_NAME\"}-第${BUILD_NUMBER}次构建日志 Jenkins构建信息邮件，请勿回复！ 构建信息 项目名称： ${PROJECT_NAME} 构建编号： ${BUILD_NUMBER} 构建状态： ${BUILD_STATUS} 构建人员： ${GITLABUSERNAME} 构建日志： 见附件 Jenkins构建页面： ${BUILD_URL} 变更代码： ${GITLABSOURCEREPOHOMEPAGE}/commit/${gitlabMergeRequestLastCommit} 单元测试报告： ${BUILD_URL}jacoco 测试报告 特别说明：Instructions指令覆盖，Branches分支覆盖，Cyclomatic Complexity非抽象方法计算圈复杂度，Lines行覆盖，Methods方法覆盖，Classes类覆盖 ${FILE,path=\"./target/site/jacoco/index.html\"} 使用pipeline语法生成器生成pipeline 压缩pipeline. (压缩HTML源代码的工具网站：http://tool.oschina.net/jscompress?type=2) pipeline{ ...上文省略... post { always { emailext attachLog: true, body: '''${ENV, var=\"JOB_NAME\"}-第${BUILD_NUMBER}次构建日志Jenkins构建信息邮件，请勿回复！构建信息项目名称： ${PROJECT_NAME}构建编号： ${BUILD_NUMBER}构建状态： ${BUILD_STATUS}构建人员： ${GITLABUSERNAME}构建日志： 见附件Jenkins构建页面：${BUILD_URL}变更代码：${GITLABSOURCEREPOHOMEPAGE}/commit/${gitlabMergeRequestLastCommit}单元测试报告：${BUILD_URL}jacoco测试报告特别说明：Instructions指令覆盖，Branches分支覆盖，Cyclomatic Complexity非抽象方法计算圈复杂度，Lines行覆盖，Methods方法覆盖，Classes类覆盖${FILE,path=\"./target/site/jacoco/index.html\"}''', mimeType: 'text/html', subject: '项目构建报告：$PROJECT_NAME的第$BUILD_NUMBER次构建$BUILD_STATUS !', to: '*******@163.com' } } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:00 "},"origin/jenkins-gitlab插件的使用.html":{"url":"origin/jenkins-gitlab插件的使用.html","title":"Gitlab","keywords":"","body":"Jenkins Gitlab插件的使用一、OverviewsJenkins的Gitlab插件能接收gitlab仓库代码事件触发的Webhook来触发Jenkins Job/Pipeline的构建，并且能将构建状态同步到Gitlab中.插件Github：https://github.com/jenkinsci/gitlab-plugin利用此插件可实现如下效果：二、安装Jenkins -> 系统管理 ->插件管理->可用插件->搜索\"gitlab\"插件下载地址：https://plugins.jenkins.io/gitlab-plugin三、配置1、Gitlab发送Webhook到Jenkins时的安全认证配置Gitlab代码仓库配置web hook时需要Jenkins Gitlab插件的Token,来加密验证webhook的安全性方式一：全局性的认证Token① Jenkins创建新用户（授予Job/Build权限即可）② 获取新用户的User ID和API Token③ Gitlab配置web hook填写URL时，使用http://USERID:APITOKEN@JENKINS_URL/project/YOUR_JOB即可（Secret Token可忽略）方式二(推荐)：单独项目Jenkins Job的认证Token 在Gitlab中配置Webhook时，将图中的Token填入即可方式三(不推荐)：不使用认证① Manage Jenkins -> Configure System -> GitLab section -> 取消勾选 \"Enable authentication for '/project' end-point\"2、Jenkins回写构建状态到Gitlab时需要的认证授权配置Jenkins通过Gitlab API将构建状态回写到对应代码仓库时，需要有权限在代码仓库中创建评论，更改状态等步骤一① Gitlab创建新用户（建议命名用户名时尽量见名知意，例如：Jenkins）② 登陆新用户，获取Access Tokens,Token权限只要api即可(及时复制Token,只显示一次)③ 在Jenkins中创建GitLab API token类型的凭据，在API token字段中保存上一步获取的Token④ 在Jenkins Gitlab插件中配置gitlab Server相关信息步骤二对应代码仓库配置Jenkins Job时要将该Token所属的用户加入members中，授予Developers角色或有更高权限的角色五、Jenkins Job中配置Webhook1、配置Gitlab Buid Trigger2、配置构建后Gitlab回写信息的动作添加构建后动作\"Pushlish build status to Gitlab\"之后会在Gitlab代码仓库的CI/CD-->Pipeline和Merge Request中看到构建状态图标 添加构建后动作\"Add note with build status on Gitlab merge request\"之后会根据构建状态的不同在Gitlab的Merge Request中添加不同的comment评论 添加构建后动作\"Add Vote for build status on Gitlab merge request\"之后会根据构建状态的不同在Gitlab的Merge Request中显示不同的投票 添加构建后动作\"Accept Gitlab merge request on success\"该动作会在build构建成功后在Gitla上自动同意接受Merge Request 不添加构建后动作Gitlab的事件只会触发Jenkins构建，不会回写任何信息到gitlab对应代码仓库中 五、Gitlab 代码仓库配置Webhook详见：gitlab-配置代码仓库事件触发器Webhook六、获取Webhook HTTP请求信息的变量对于Gitlab发送过来的Webhook HTTP POST请求信息，可直接通过以下变量来获取。gitlabBranch gitlabSourceBranch gitlabActionType gitlabUserName gitlabUserEmail gitlabSourceRepoHomepage gitlabSourceRepoName gitlabSourceNamespace gitlabSourceRepoURL gitlabSourceRepoSshUrl gitlabSourceRepoHttpUrl gitlabMergeRequestTitle gitlabMergeRequestDescription gitlabMergeRequestId gitlabMergeRequestIid gitlabMergeRequestState gitlabMergedByUser gitlabMergeRequestAssignee gitlabMergeRequestLastCommit gitlabMergeRequestTargetProjectId gitlabTargetBranch gitlabTargetRepoName gitlabTargetNamespace gitlabTargetRepoSshUrl gitlabTargetRepoHttpUrl gitlabBefore gitlabAfter gitlabTriggerPhrase Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:00 "},"origin/jenkins-generic-webhook-trigger插件.html":{"url":"origin/jenkins-generic-webhook-trigger插件.html","title":"Generic Webhook Trigger","keywords":"","body":"Jenkins Generic Webhook Trigger插件之前写过在Jenkins中使用Gitlab插件接收gitlab仓库代码指定事件触发的webhook来触发Job或Pipeline的执行构建,详见:Jenkins的Gitlab插件。这种方式在某种场景下有些限制，无法满足一些功能性复杂的webhook触发。所以可以使用Jenkins Generic Webhook Trigger插件监听包含自定义设置的HTTP请求来触发job/pipeline的构建！对比Gitlab插件，有以下可自定义的特性：暴露出来的回调API URL统一，不同的job/pipeline使用不同的Token或者指定特殊的请求参数进行区分 可使用不同的方式提取HTTP请求中的各种信息，然后通过环境变量的形式传递给job/pipeline使用 可设置白名单，只允许接收指定IP地址的Webhook请求 一、简介Generic Webhook Trigger 是一款Jenkins插件，简称GWT，安装后会暴露出来一个公共API，GWT插件接收到 JSON 或 XML 的 HTTP POST 请求后，根据我们配置的规则决定触发哪个Jenkins项目 安装的话在Jenkins的插件管理中心直接搜索安装即可，下载HPI文件手动安装, 插件下载地址 插件Github地址：https://github.com/jenkinsci/generic-webhook-trigger-plugin 支持以下系统发送的Webhook: Bitbucket Cloud Bitbucket Server GitHub GitLab Gogs and Gitea Assembla Jira 二、GenericTrigger 触发器设置GenericTrigger 触发条件分为5部分：从 HTTP POST 请求中提取数据 Token, GWT 插件用于标识Jenkins项目的唯一性 根据请求参数值判断是否触发Jenkins项目的执行 日志打印控制 Webhook 响应控制 1. 从 HTTP POST 请求中匹配数据GWT 插件可以从一个HTTP POST请求的Body、URL参数、header中提取数据并赋予指定的环境变量，后续任务可以直接引用。在Job中设置数据的匹配在Pipeline中设置数据的匹配genericRequestVariables：从URL参数中提取值 genericVariables： 从HTTP POST的body 中提取值 genericHeaderVariables：从HTTP header 中提取值。用法和genericRequestVariables一样 genericVariables: [ [key: 'ref', value: '$.ref'], [key: 'before',value: '$.before', expressionType: 'JSONPath', //Optional, defaults to JSONPath regexpFilter: '', //Optional, defaults to empty string defaultValue: '' //Optional, defaults to empty string ] ], genericRequestVariables: [ [key: 'requestWithNumber', regexpFilter: '[^0-9]'], [key: 'requestWithString', regexpFilter: ''] ], genericHeaderVariables: [ [key: 'headerWithNumber', regexpFilter: '[^0-9]'], [key: 'headerWithString', regexpFilter: ''] ] 2. Token 参数当多个Jenkins Jobs中使用该插件时，Webhook Trigger都是同一个相同的URL。如果你只想触发一个特定的工作，可以使用令牌参数为不同的作业提供不同的令牌。 可以再header或post中添加一些请求参数，并仅在该参数具有特定值时使用regexp筛选器触发。 发送Webhook请求时可使用以下方式在HTTP POST请求中携带Token：请求URL参数:curl -vs http://localhost:8080/jenkins/generic-webhook-trigger/invoke?token=abc123 token的请求header:curl -vs -H \"token: abc123\" http://localhost:8080/jenkins/generic-webhook-trigger/invoke Bearer类型Authorization的请求header:curl -vs -H \"Authorization: Bearer abc123\" http://localhost:8080/jenkins/generic-webhook-trigger/invoke 3. 日志打印控制Silent response： 当为true，只返回http 200 状态码，不返回触发状态等信息。 Print post content： 是否在构建日志中打印webhook 请求的内容 Print contributed variables： 是否在构建日志中打印提取后的变量 4. 显示触发信息5. 设置进行下一步构建的条件例如只允许gitlab仓库Pipeline分支commit事件触发的Webhook才能触发构建三、设置白名单可在Jenkins Generic Webhook Trigger的全局配置中，配置IP地址白名单，指定、验证请求的来源IP地址，IP地址格式可为CIDR 或 ranges。1.2.3.4 2.2.3.0/24 3.2.1.1-3.2.1.10 2001:0db8:85a3:0000:0000:8a2e:0370:7334 2002:0db8:85a3:0000:0000:8a2e:0370:7334/127 同时还支持HMAC验证(HMAC百度百科)。Jenkins --> Manage Jenkins --> Configure SystemCopyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-12 22:47:23 "},"origin/gitlab-配置SMTP邮件服务.html":{"url":"origin/gitlab-配置SMTP邮件服务.html","title":"配置SMTP邮件服务","keywords":"","body":"1. 修改/etc/gitlab/gitlab.rb### Email Settings gitlab_rails['gitlab_email_enabled'] = true gitlab_rails['gitlab_email_from'] = 'rationalmonster@163.com' gitlab_rails['gitlab_email_display_name'] = 'Curiouser163SMTPServer' gitlab_rails['gitlab_email_reply_to'] = 'rationalmonster@163.com' gitlab_rails['gitlab_email_subject_suffix'] = 'Gitlab' gitlab_rails['smtp_enable'] = true gitlab_rails['smtp_address'] = \"smtp.163.com\" gitlab_rails['smtp_port'] = 25 gitlab_rails['smtp_user_name'] = \"******@163.com\" gitlab_rails['smtp_password'] = \"******\" gitlab_rails['smtp_domain'] = \"163.com\" gitlab_rails['smtp_authentication'] = \"login\" gitlab_rails['smtp_enable_starttls_auto'] = false gitlab_rails['smtp_tls'] = false 2. 测试发送邮件# gitlab-rails console Loading production environment (Rails 4.2.10) # irb(main):001:0> Notify.test_email('******@163.com','gitlab send mail test','gitlab test mail').deliver_now Notify#test_email: processed outbound mail in 539.6ms Sent mail to ******@163.com (397.8ms) Date: Thu, 04 Jul 2019 15:07:33 +0000 From: Curiouser163SMTPServer Reply-To: Curiouser163SMTPServer To:******@163.com Message-ID: Subject: gitlab send mail test Mime-Version: 1.0 Content-Type: text/html; charset=UTF-8 Content-Transfer-Encoding: 7bit Auto-Submitted: auto-generated X-Auto-Response-Suppress: All gitlab test mail=> #, >, >, , >, , , , , , > irb(main):002:0> Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:51:59 "},"origin/gitlab-配置代码仓库事件触发器Webhook.html":{"url":"origin/gitlab-配置代码仓库事件触发器Webhook.html","title":"代码仓库配置事件触发器Webhook","keywords":"","body":"Gitlab 代码仓库配置事件Webhoook触发器一、OverviewsWebhook与异步编程中\"订阅-发布模型\"非常类似，一端触发事件，一端监听执行。WebHook就是一个接收HTTP POST（或GET，PUT，DELETE）的URL通常来说，WebHook通过HTTP协议将请求数据发送到外部系统后，就不再处理响应数据啦！二、Gitlab事件触发器配置Webhook当Gitlab中代码仓库发生了一些事件后，可设置触发器发送http通知到外部系统。通常配置Jenkins上的Webhook配置Gitlab的网络限制 否则后续发送webhook请求时会报错\"Requests to the local network are not allowed\",原因详见：附录 配置代码代码仓库的Webhook 配置完可以模拟一些事件进行测试 查看详细的Webhook请求信息 附录：gitlab发送webhook请求时报错原因If you have non-GitLab web services running on your GitLab server or within its local network, these may be vulnerable to exploitation via Webhooks. With Webhooks, you and your project maintainers and owners can set up URLs to be triggered when specific things happen to projects. Normally, these requests are sent to external web services specifically set up for this purpose, that process the request and its attached data in some appropriate way. Things get hairy, however, when a Webhook is set up with a URL that doesn't point to an external, but to an internal service, that may do something completely unintended when the webhook is triggered and the POST request is sent. Because Webhook requests are made by the GitLab server itself, these have complete access to everything running on the server (http://localhost:123) or within the server's local network (http://192.168.1.12:345), even if these services are otherwise protected and inaccessible from the outside world. If a web service does not require authentication, Webhooks can be used to trigger destructive commands by getting the GitLab server to make POST requests to endpoints like \"http://localhost:123/some-resource/delete\". To prevent this type of exploitation from happening, starting with GitLab 10.6, all Webhook requests to the current GitLab instance server address and/or in a private network will be forbidden by default. That means that all requests made to 127.0.0.1, ::1 and 0.0.0.0, as well as IPv4 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16 and IPv6 site-local (ffc0::/10) addresses won't be allowed. This behavior can be overridden by enabling the option \"Allow requests to the local network from hooks and services\" in the \"Outbound requests\" section inside the Admin area under Settings (/admin/application_settings): Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:51:59 "},"origin/nexus-简介.html":{"url":"origin/nexus-简介.html","title":"Nexus","keywords":"","body":"一、Nexus简介Nexus是Sonatype公司出的一款目前最为流程的构件仓库管理软件，主要用于局域网内部的构件管理，代理访问外部仓库等。例如对于公司私有的Java制品Jar包，可上传至Nexus的Maven类型仓库中进行集中管理；代理访问阿里云Maven仓库，缓存加速获取互联网上的Java制品。Nexus使用Lucene提供了强大的构件搜索功能，拥有丰富的RestFul API接口用于管理控制，支持WebDAV和LDAP安全身份认证，基于RBAC的权限访问控制等功能。市面上同类产品有Apache的Archiva和JFrog的Artifatory。Nexus分为免费开源版OSS和收费商业版Professional二、仓库类型Proxy 类型仓库主要用于代理缓存访问外网上其他公开的仓库，将每次从代理仓库拉取的制品缓存到nexus文件系统中，下次再拉取相同版本制品时就不需再次从外网拉取，起到代理访问缓存的功能 Hosted 类型的仓库主要用于存放各个项目组产出的、用于共享、不能放到公网上、私有的制品。有两种版本策略，一种是Snapshots版本策略类型的，对于相同版本制品的上传，nexus会自动追加时间戳加以区分；一种是Release版本策略类型的，对于相同的制品，要明确版本，不能存放相同版本。可以理解为snapshots仓库存放一些内容变更频繁的制品，这样不管上传还是使用时不用频繁变更版本号就能拉取到最新版本。而release仓库存放一些内容稳定变更少的制品，使用时指定好版本就行，无需经常变动 Group 类型仓库主要用于组合其他仓库，统一对外使用方式。可设置组仓库组合其他仓库的顺序。例如组合顺序为先拉取maven格式aliyun代理仓库中的制品，如果其中没有想要的制品，再去拉取maven格式Central代理仓库中的制品。如果还没有，就去maven格式hosted类型仓库中拉取，直到遍历完所有的组合仓库。同时，拉取使用时不需要配置那么多的仓库地址，只需要配置group仓库地址就行 三、官方高可用方案官方收费版的高可用方案四、Kubernetes部署--- apiVersion: apps/v1 kind: Deployment metadata: labels: app: nexus3 name: nexus3 namespace: nexus3 spec: replicas: 1 selector: matchLabels: app: nexus3 strategy: type: Recreate template: metadata: labels: app: nexus3 spec: imagePullSecrets: - name: harbor-secrets initContainers: - name: init-scheduler image: busybox:latest imagePullPolicy: IfNotPresent command: ['sh', '-c', 'chmod -R 777 /nexus-data'] volumeMounts: - name: nexus3-data mountPath: /nexus-data containers: - env: - name: CONTEXT_PATH value: / - name: TZ value: 'Asia/Shanghai' image: docker.io/sonatype/nexus3:3.15.2 imagePullPolicy: IfNotPresent readinessProbe: failureThreshold: 1 initialDelaySeconds: 100 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 8081 timeoutSeconds: 1 livenessProbe: failureThreshold: 1 initialDelaySeconds: 100 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 8081 timeoutSeconds: 1 name: nexus3 ports: - containerPort: 8081 protocol: TCP resources: limits: memory: 4096Mi requests: memory: 2048Mi terminationMessagePath: /dev/termination-log volumeMounts: - mountPath: /nexus-data name: nexus3-data dnsPolicy: ClusterFirst restartPolicy: Always securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: nexus3-data persistentVolumeClaim: claimName: nexus3-pv --- apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: cephfs labels: app: nexus3 name: nexus3-pv namespace: nexus3 spec: accessModes: - ReadWriteMany resources: requests: storage: 1000Gi --- apiVersion: v1 kind: Service metadata: labels: app: nexus3 name: nexus3 namespace: nexus3 spec: ports: - name: 8081-tcp port: 8081 protocol: TCP targetPort: 8081 selector: app: nexus3 sessionAffinity: None type: ClusterIP --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nexus3 namespace: nexus3 spec: rules: - host: nexus.curiouser.com http: paths: - path: / backend: serviceName: nexus3 servicePort: 8081 五、常见仓库配置YUM格式制品 Group类型仓库 yum yum-ustc yum-ansible yum-cloudera5 Proxy类型仓库yum-ansible：https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ yum-ustc： http://mirrors.ustc.edu.cn/ yum-cloudera5：https://archive.cloudera.com/cdh5/ Hosted类型仓库yum-hosted Maven格式制品 Group类型仓库 maven maven-aliyun maven-central maven-releases maven-snapshots Proxy类型仓库maven-central：https://repo1.maven.org/maven2/ maven-aliyun：http://maven.aliyun.com/nexus/content/groups/public Hosted类型仓库maven-snapshots maven-releases NPM格式制品 Group类型仓库 npm npm-taobao npm-cnpm npm-hosted Proxy类型仓库npm-taobao：http://registry.npm.taobao.org/ npm-cnpm：http://registry.cnpmjs.org/ Hosted类型仓库npm-hosted（上传权限需修改realms) Docker格式制品Group类型仓库docker（设置http-port:8082） Proxy类型仓库docker-io： https://registry-1.docker.io Hosted类型仓库docker-hosted（设置http-port:8083） Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:07 "},"origin/nexus-使用OrientDB Console在DB层面修改配置.html":{"url":"origin/nexus-使用OrientDB Console在DB层面修改配置.html","title":"使用OrientDB Console在DB层面修改配置","keywords":"","body":"Nexus使用OrientDB Console在DB层面修改配置一、Context在配置Nexus对接LDAP过程中，原先的用户（admin和原来创建的）也是可以登录的，为了测试只让LDAP用户登录，将安全域中的Local Authorizing Realm和Local Authenticating Realm给去掉了，导致admin用户登陆不上，LDAP上的用户能登陆，但没有管理Nexus的权限。换句话说，这个Nexus成了僵尸。如果Nexus的admin密码忘了,情形也类似。都需要修改Nexus数据库中的用户数据才能修改相关配置（Nexus是将用户、配置等相关信息存放在OrientDB 数据库中，而不是在配置文件中，所以改密码或者在没有权限的情况下改配置都是要操作OrientDB 中的数据）相关信息Nexus是在Openshift(Kubernetes也一样)中容器化部署的 Nexus版本：3.15.2 Nexus的数据目录是单独使用NFS类型的PV挂载的 openshift上Nexus容器的执行用户没有权限操作除持久化目录之外的目录。（以root用户起的Nexus进程可直接使用Nexus容器中的”/opt/nexus/lib/support/nexus-orient-console.jar“连接OrientDB ） 问题解决思路将Nexus的PV数据目录挂载到物理节点一个临时目录下 使用Nexus相同版本安装包中的OrientDB Console连接到临时数据目录中的DB，然后修改相关信息Note: 如果容器中执行Nexus进程的用户和物理节点上的用户不一样导致权限不够的话，可暂时将临时数据目录中的所有文件权限设置为777，修改完再改回来 再将Nexus的PV数据目录挂载到openshift上Nexus容器中，重启Nexus 二、PreflightNexus的PV数据目录已挂载到/roor/test 下载相同版本的Nexus到/opt/目录下 /roor/test下的所有文件及文件夹权限临时修改为777 三、操作1. 使用官方的OrientDB Console连接到数据目录中的DBjava -jar /opt/nexus-3.15.2-01/lib/support/nexus-orient-console.jar 此时会进入OrientDB 控制台# OrientDB console v.2.2.36 (build d3beb772c02098ceaea89779a7afd4b7305d3788, branch 2.2.x) https://www.orientdb.com # Type 'help' to display all the supported commands. orientdb> #输入exit退出 2. 连接临时数据目录中的DBorientdb> connect plocal:/root/test/db/security admin admin 此时会连到一个security的Database3. 重置Realm如果从活动列表中删除了缺省安全域，则缺省管理员用户(即使用户名密码正确)也无法进行身份验证orientdb> delete from realm Note: 重置后。默认的安全域将被激活，任何自定义的安全域都将被删除。后续再UI界面进行添加4.(可选) 重置admin用户密码为默认值\"admin123\"orientdb> update user SET password=\"$shiro1$SHA-512$1024$NE+wqQq/TmjZMvfI7ENh/g==$V4yPw8T64UQ6GfJfxYq2hLsVrBY8D1v+bktfOxGdt4b/9BthpWPNUy/CBk6V9iA0nHpzYzJFWO8v/tZFtES8CA==\" UPSERT WHERE id=\"admin\" 5.(可选) 重置admin为管理员如果admin用户不在是“nx-admin”管理员角色orientdb> select * from user_role_mapping where userID = \"admin\" orientdb> update user_role_mapping set roles = [\"nx-admin\"] where userID = \"admin\" orientdb> select status from user where id = \"admin\" orientdb> update user set status=\"active\" upsert where id=\"admin\" 6. 退出OrientDB 控制台，修改回临时数据目录所有文件的原始权限，重新将数据目录挂载到容器上，然后重启NexusBazinga，admin用户能正常，又重新夺回Nexus的管理权限，所有的仓库配置和数据没有丢失，重新将LDAP的安全域加回去，一切恢复原样。附录：Nexus使用的OrientDB1. What is the OrientDB ConsoleNexus 3 uses several OrientDB databases. In very specific circumstances, these databases can be manipulated as advised by Sonatype support. This article describes how to open a special command-line interface for connecting to and working with the databases used by Nexus. This interface is known as the OrientDB Console.Caution: Using the console incorrectly can cause irreparable harm to the databases.2. Launching the OrientDB Console on Nexus 3.2.1 and NewerNexus 3.2.1+ includes a single jar executable which can launch the OrientDB console.As the operating system user account that typically owns the Nexus Repository Manager process, start a terminal session on the host where Nexus is installed. Change directories to your application directory. Launch the console using the same version of Java executable that Nexus is using : Unixjava -jar ./lib/support/nexus-orient-console.jar Windowsjava -jar lib\\support\\nexus-orient-console.jar Mac.install4j/jre.bundle/Contents/Home/jre/bin/java -jar ./lib/support/nexus-orient-console.jar You should be presented with a command-line interface such as this, ready to accept your commands:# OrientDB console v.2.2.16 www.orientdb.com #Type 'help' to display all the supported commands. orientdb> # When you are done your commands, type exit to quit the console. The nexus-orient-console.jar sets up the correct classpath to successfully launch the console. Launching the jar from another location is not supported参考连接https://support.sonatype.com/hc/en-us/articles/115002930827-Accessing-the-OrientDB-Console https://support.sonatype.com/hc/en-us/articles/213467158-How-to-reset-a-forgotten-admin-password-in-Nexus-3-x Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:07 "},"origin/nexus-设置SMTP邮件服务.html":{"url":"origin/nexus-设置SMTP邮件服务.html","title":"设置SMTP邮件服务","keywords":"","body":"Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:07 "},"origin/nexus-maven仓库的配置与使用.html":{"url":"origin/nexus-maven仓库的配置与使用.html","title":"Maven","keywords":"","body":"Maven仓库的配置与使用一、Overview有了Maven仓库之后，当 Maven 需要下载构件时，直接请求Nexus的maven仓库，仓库中存在则下载到本地仓库；否则，nexus请求外部的远程仓库，将构件下载到私服，再提供给本地仓库下载有了Maven仓库之后，当Maven需要下载构件时，直接请求Nexus，Maven仓库上存在则下载到本地仓库；Maven仓库上不存在的话，Nexus请求外部的远程仓库，将构件下载到Maven仓库，再提供给本地仓库下载。8e3e97dde794e5413f80df5a8b21234616159073Maven格式制品仓库配置三、四、=======Group类型仓库 maven maven-aliyun maven-central maven-releases maven-snapshots Proxy类型仓库maven-central：https://repo1.maven.org/maven2/ maven-aliyun：http://maven.aliyun.com/nexus/content/groups/public Hosted类型仓库maven-snapshots maven-releases maven格式的仓库有两种典型的使用场景，一个是顺序拉取group组仓库中组合仓库里的制品，一个是上传制品到hosted类型的仓库中，例如maven-snapshots仓库。而maven配置nexus中的仓库有三个地方，配置最终生效的顺序为全局配置文件--->用户配置文件---->POM文件：maven的全局配置文件settings.xml中,该配置文件在maven安装目录conf文件夹下 maven的用户配置文件settings.xml中,该配置文件在用户目录.m2文件夹下 项目的POM文件 二、代理仓库的使用在Maven用户配置文件setting.xml中添加maven格式group仓库的地址 .....上文省略...... curiouser-maven microservices microservices用户的密码 curiouser-maven * The Maven repository of curiouser http://nexus-ip地址:8081/repository/maven/ .....下文省略...... 三、发布制品到Maven的Hosted仓库1、mvn deploy在Maven用户配置文件setting.xml中添加snapshot、release仓库的id .....上文省略...... maven-releases microservices microservices用户的密码 maven-snapshots microservices microservices用户的密码 .....下文省略...... 在项目POM.xml文件中添加Snapshots仓库或Release仓库的地址 .....上文省略...... maven-releases User Project Release http://nexus-ip地址:8081/repository/maven-releases/ maven-snapshots User Project SNAPSHOTS http://nexus-ip地址:8081/repository/maven-snapshots/ .....下文省略...... 然后mvn deploy2、Curl手动上传 curl -v -u microservices:microservices用户的密码 --upload-file springboot2-0.0.0.jar http://nexus-ip地址:8081/repository/maven-releases/com/curiouser/demoeverything/springboot2/0.0.0/springboot2-0.0.0.jar curl -v -u microservices:microservices用户的密码 --upload-file pom.xml http://nexus-ip地址:8081/repository/maven-releases/com/curiouser/demoeverything/springboot2/0.0.0/springboot2-0.0.0.pom 3、mvn命令手动上传前提是Maven用户配置文件setting.xml中已经添加了snapshot、release仓库的idLinux mvn deploy:deploy-file \\ #要上传的Jar包路径 \\ -Dfile=/root/websocket-server-9.4.11.v20180605.jar \\ #Jar包Maven坐标的GroupID \\ -DgroupId=org.eclipse.jetty.websocket \\ #Jar包Maven坐标的ArtifactID \\ -DartifactId=websocket-server \\ #Jar包Maven坐标的Version \\ -Dversion=9.4 \\ #要上传到仓库的制品类型，该值还可以是pom -Dpackaging=jar \\ #maven私服hosted类型仓库的地址 -Durl=http://nexus-ip地址:8081/repository/maven-releases/ \\ #maven私服hosted类型仓库的repositoryid -DrepositoryId=maven-releases Windows mvn deploy:deploy-file ^ -Dfile=D:\\websocket-server-9.4.11.v20180605.jar ^ -DgroupId=org.eclipse.jetty.websocket ^ -DartifactId=websocket-server ^ -Dversion=9.4 ^ -Dpackaging=jar ^ -Durl=http://nexus-ip地址:8081/repository/maven-releases/ ^ -DrepositoryId=maven-releases 4、UI界面上传5、使用Postman上传 注意事项Jar包上传只能上传hosted类型的仓库中，Proxy和Group类型的无法上传。同时，注意有些仓库设置禁止了上传 hosted类型的Snapshot仓库默认设置的上传版本控制为只能上传以“-snapshot”结尾版本的制品。所以版本为非“-snapshot”结尾的Jar包无法上传到snapshot仓库中 如果Nexus禁止匿名用户访问时，匿名用户是被禁止拉取maven仓库的Jar包。所以可以创建一个对maven仓库类型有读取权限的用户，在settings.xml文件进行配置。 附录：如何在maven的配置文件settings.xml中使用加密过的用户密码在Maven的settings.xml中，往往要配置访问远程库所在的服务器的username/password。但是明文的密码总是显得那么扎眼，必欲除之而后快。Apache Maven项目提供了便捷的密码加密机制，该机制的最近更新时间为2018-03-06。该机制目前只支持在命令行下的操作，如生成密码的密文。此外，用户还需要在${user.home}/.m2目录下配置settings-security.xml文件，其中包含：用以加密其他密码的master password（此处也是密文） 或指向另一个保密文件的完整路径 在该加密机制中有两个概念，一个是master password，即用以加密其他密码的密码，另一个就是实际使用的服务器访问密码password。master password的密文配置在settings-security.xml文件中，而服务器访问密码password的密文就可以大大方方地配置在settings.xml中。具体用法如下：生成Master password的密文 mvn --encrypt-master-password 根据提示输入Master password: 就可以生成密文{iENT44//TgwH46wJQ0Go3et0u9PRZivf7LcAA9mY4LA=} 配置${user.home}/.m2/settings-security.xml文件(如果没有就手动创建) {iENT44//TgwH46wJQ0Go3et0u9PRZivf7LcAA9mY4LA=} # 如果settings-security.xml文件被保存到U盘，则配置${user.home}/.m2/settings-security.xml文件如下： /my_u_volume/my_path/settings-security.xml 加密访问服务器的密码 mvn --encrypt-password 根据提示输入Password: ​ 就可以生成密文{rZhmW6UmQw0HhRTeqSBchuMAgAoH6owP/hJjV3a/9Eg=} 配置settings.xml文件 my.server myfoo add_any_comment or \\{\\{rZhmW6UmQw0HhRTeqSBchuMAgAoH6owP/hJjV3a/9Eg=\\}\\} add_any_comment 参考链接https://blog.csdn.net/taiyangdao/article/details/79500507 8e3e97dde794e5413f80df5a8b21234616159073 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:07 "},"origin/nexus-npm仓库的配置与使用.html":{"url":"origin/nexus-npm仓库的配置与使用.html","title":"NPM","keywords":"","body":"NPM仓库的配置与使用一、npm仓库的配置信息Group类型仓库 NPM npm-taobao npm-cnpm npm-cnpm Proxy类型仓库npm-taobao：http://registry.npm.taobao.org/ npm-cnpm：http://registry.cnpmjs.org/ Hosted类型仓库npm-hosted 二、使用NPM仓库npm config set registry http://nexus-ip地址:8081/repository/NPM/ 三、发布制品到NPM的Hosted仓库echo \"hello\" >> test npm init # package name: (test) sadsada # version: (1.0.0) # description: # entry point: (index.js) test # test command: # git repository: # keywords: # author: # license: (ISC) # About to write to /root/test/package.json: # { # \"name\": \"sadsada\", # \"version\": \"1.0.0\", # \"description\": \"\", # \"main\": \"f\", # \"scripts\": { # \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" # }, # \"author\": \"\", # \"license\": \"ISC\" # } npm login --registry http://Nexus-IP地址:8081/repository/NPM-Hosted/ # Username: admin #Password: ***** # Email: (this IS public) asdad@sada.com npm publish -registry http://Nexus-IP地址:8081/repository/NPM-Hosted/ 四、使用nrm工具切换npm仓库Github地址：https://github.com/Pana/nrm 帮助快速切换npm仓库源。默认已经配置了npm、yarn、taobao、cnpm、nj、npmMirror、edunpm等常见的仓库源。1. 安装npm install nrm -g 2. 命令详解$ nrm -h Usage: nrm [options] [command] Options: -V, --version output the version number -h, --help output usage information Commands: ls List all the registries current Show current registry name use Change registry to registry add [home] Add one custom registry set-auth [options] [value] Set authorize information for a custom registry with a base64 encoded string or username and pasword set-email Set email for a custom registry set-hosted-repo Set hosted npm repository for a custom registry to publish packages del Delete one custom registry home [browser] Open the homepage of registry with optional browser publish [options] [|] Publish package to current registry if current registry is a custom registry. if you're not using custom registry, this command will run npm publish directly test [registry] Show response time for specific or all registries help Print this help 3. 常用命令查看默认支持的npm 仓库$ nrm ls * npm -------- https://registry.npmjs.org/ yarn ------- https://registry.yarnpkg.com/ cnpm ------- http://r.cnpmjs.org/ taobao ----- https://registry.npm.taobao.org/ nj --------- https://registry.nodejitsu.com/ npmMirror -- https://skimdb.npmjs.com/registry/ edunpm ----- http://registry.enpmjs.org/ # \"*\"编注的仓库代表当前使用的仓库 添加私有的npm仓库nrm add curiouser http://nexus.apps.okd311.curiouser.com/repository/NPM 切换npm仓库 nrm use 仓库名 删除仓库 nrm del 仓库名 测试仓库速度nrm test Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:07 "},"origin/nexus-yum仓库的配置与使用.html":{"url":"origin/nexus-yum仓库的配置与使用.html","title":"YUM","keywords":"","body":"YUM仓库的配置与使用一、Overviews之前搭建内网YUM源仓库都是使用HTTP和createrepo做的，每次还要手动去同步外网镜像源最新的RPM包，繁琐耗时。最近发现新版的Nexus已经有可以做YUM源的功能，能像Maven仓库那样，没有相关资源的时候去外网拉，有的话就不去外网啦。即节省存储，又加快速度，还能自动更新RPM版本。Group类型仓库 yum yum-ustc yum-ansible yum-cloudera5 Proxy类型仓库yum-ustc：http://mirrors.ustc.edu.cn/ yum-tuna：https://mirrors.tuna.tsinghua.edu.cn/ yum-163：http://mirrors.163.com/ yum-ansible：https://releases.ansible.com/ansible/rpm/releasepel-7-x86_64/ yum-cloudera5：https://archive.cloudera.com/cdh5/ Hosted类型仓库yum-hosted（\"Repodata Depth\"设置为“1”） 二、手动上传RPM包到Hosted仓库上传RPM包到Hosted仓库，相关的RPM元信息文件夹repodata会自动生成，不再需要createrepo生成啦（如果没有自动生成，点击仓库的Rebuild Index，然后稍等即可）$ ls -l | grep ^[^d] | awk '{print $9}' mysql-community-client-5.7.19-1.el7.x86_64.rpm mysql-community-common-5.7.19-1.el7.x86_64.rpm mysql-community-devel-5.7.19-1.el7.x86_64.rpm mysql-community-embedded-5.7.19-1.el7.x86_64.rpm mysql-community-embedded-compat-5.7.19-1.el7.x86_64.rpm mysql-community-embedded-devel-5.7.19-1.el7.x86_64.rpm mysql-community-libs-5.7.19-1.el7.x86_64.rpm mysql-community-libs-compat-5.7.19-1.el7.x86_64.rpm mysql-community-minimal-debuginfo-5.7.19-1.el7.x86_64.rpm mysql-community-server-5.7.19-1.el7.x86_64.rpm mysql-community-server-minimal-5.7.19-1.el7.x86_64.rpm mysql-community-test-5.7.19-1.el7.x86_64.rpm #一次只能上传一个RPM文件， #为了一个Host仓库存放不同软件的不同版本，上传时需要指定上传到Hosted仓库的哪个目录下，以文件夹名来区分当前上传的RPM包是哪个版本的。 $ for i in `ls *rpm` ;do curl -v --user 'admin:admin123' --upload-file $i http://nexus-ip地址:8081/repository/yum-nexus-MySQL/mysql5.7.19/;done 三、YUM仓库的使用[centos] name=Nexus Yum baseurl=http://nexus-ip地址:8081/repository/yum-nexus/centos/$releasever/os/$basearch/ enabled=1 gpgcheck=0 [centos-extras] name=Nexus Yum Extras baseurl=http://nexus-ip地址:8081/repository/yum-nexus/centos/$releasever/extras/$basearch/ enabled=1 gpgcheck=0 [epel] name=Nexus Epel baseurl=http://nexus-ip地址:8081/repository/yum-nexus/epel/$releasever/$basearch/ enabled=1 gpgcheck=0 [openshift] name=Nexus openshift 3.11 baseurl=http://nexus-ip地址:8081/repository/yum-nexus/centos/$releasever/paas/$basearch/openshift-origin311/ enabled=1 gpgcheck=0 [mysql5.7] name=Nexus MySQL 5.7.19 baseurl=http://nexus-ip地址:8081/repository/yum-nexus/mysql5.7.19 enabled=1 gpgcheck=0 注意不要代理多个外网的YUM源。例如同时代理缓存网易的163镜像源和清华大学的镜像源，这些外网镜像源中的RPM都大差无几，配置多个的话，使用group去代理拉RPM的时候，会优先选择group中靠前的外网代理镜像源。有时（偶尔其中一个外网镜像源不能用）会出现相同的RPM存在于每一个proxy仓库中，浪费存储。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:07 "},"origin/nexus-数据的备份恢复.html":{"url":"origin/nexus-数据的备份恢复.html","title":"数据备份恢复","keywords":"","body":"Nexus的数据备份与恢复Nexus的备份分为两个部分。一个是元信息和配置信息数据库的备份，一个是Blob存储的备份一、配置DB的备份操作 二、Blob存储的备份操作Nexus的blob stores可以简单的理解为一个文件夹，存放着各种制品的原始文件，例如原始的Java制品Jar包，POM文件，War包等文件。Blob的类型可以是文件系统的一个文件夹，也可以是S3对象存储的一个存储Buckets。（默认是文件系统类型，仅支持S3对象存储）一个blob存储可以被一个或者多个仓库组使用 三、Kubernetes上Nexus的数据备份由于nexus在Kubernetes上的实例使用了CephFS类型的PVC存储作为Volume挂载在到数据目录下。所以，只要将Nexus容器持久化卷的PV对应Ceph路径挂载到某个目录下，拷贝其中的数据库DB目录和Blob目录进行数据备份。mkdir test nexus3-k8s-2019-5-6-bak mount -t ceph jk1:/pvc-volumes/kubernetes/kubernetes-dynamic-pvc-25c75aeb-6f2b-11e9-ac5b-9209ee33fdea test -o name=admin,secret=AQCxFKtcDGd1AhAAvytw5KlaeuApSi1a3G2iwA== cp -r test/db test/blob nexus3-k8s-2019-5-6-bak umount test 四、数据恢复操作数据恢复时使用的版本与旧版本的差异仅限于小版本号暂停旧的Nexus POD 挂载旧的Nexus CephFS PV 到本地目录 将备份提拷贝新的CephFS PV中 修改备份目录的权限 重启新的Nexus POD 五、备份策略优化hosted类型的仓库可使用单独的Blob存储，备份时只备份该Blob。Proxy类型的仓库可不用备份。 可添加执行脚本类型的定时任务做备份，将新增Blob同步到其他Nexus实例中 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:07 "},"origin/nexus-api.html":{"url":"origin/nexus-api.html","title":"API","keywords":"","body":"Nexus API一、Context官网API文档：https://help.sonatype.com/repomanager3/rest-and-integration-api 二、Search APISearch API用于搜索component和asset。GET /service/rest/v1/search 1、Search Components例如在maven-central仓库中搜索\"group=org.osgi\"的component$ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/search?repository=maven-central&group=org.osgi' { \"items\" : [ { \"id\" : \"bWF2ZW4tY2VudHJhbDoyZTQ3ZGRhMGYxYjU1NWUwNzE1OWRjOWY5ZGQzZmVmNA\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"group\" : \"org.osgi\", \"name\" : \"org.osgi.core\", \"version\" : \"4.3.1\", \"assets\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1-sources.jar\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1-sources.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDplMDE4OGVkMDcyOGZhNjhmNDExNzU2OGU1MjQ2NjZiYg\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"80bfafcf783988442b3a58318face1d2132db33d\", \"md5\" : \"87ee0258b79dc852626b91818316b9c3\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.jar\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDpkMDY0ODA0YThlZDVhZDZlNjhmZGU5MWNmM2NiZTgzMw\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"5458ffe2ba049e76c29f2df2dc3ffccddf8b839e\", \"md5\" : \"8053bbc1b55d51f5abae005625209d08\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.pom\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDo2NTRiYjdkMGE1OTIxMzg1OWZhMTVkMzNmYWU1ZmY3OA\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"79391fc69dd72ad1fd983d01b4572f93f644882b\", \"md5\" : \"3d87a59bcdb4b131d9a63e87e0ed924a\" } } ] } ], \"continuationToken\" : null } 2、Search Assets例如在maven-central仓库中搜索\"group=org.osgi\"的assets$ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/search../assets?repository=maven-central&group=org.osgi' { \"items\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1-sources.jar\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1-sources.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDplMDE4OGVkMDcyOGZhNjhmNDExNzU2OGU1MjQ2NjZiYg\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"80bfafcf783988442b3a58318face1d2132db33d\", \"md5\" : \"87ee0258b79dc852626b91818316b9c3\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.jar\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDpkMDY0ODA0YThlZDVhZDZlNjhmZGU5MWNmM2NiZTgzMw\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"5458ffe2ba049e76c29f2df2dc3ffccddf8b839e\", \"md5\" : \"8053bbc1b55d51f5abae005625209d08\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.pom\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDo2NTRiYjdkMGE1OTIxMzg1OWZhMTVkMzNmYWU1ZmY3OA\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"79391fc69dd72ad1fd983d01b4572f93f644882b\", \"md5\" : \"3d87a59bcdb4b131d9a63e87e0ed924a\" } } ], \"continuationToken\" : null } 3、Search and Download Asset用于搜索一个资产，然后将请求重定向到该资产的downloadUrlGET /service/rest/v1/search../assets/download 例如获取一个maven坐标为\"groupId=com.curiosuer，artifactId=SpringBoot2，version=0.0.0\"Jar包的下载链接$ curl -v -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/search../assets/download?maven.groupId=com.curiosuer&maven.artifactId=SpringBoot2&maven.baseVersion=0.0.0&maven.extension=jar&maven.classifier' 浏览器中三、Repositories API1、List Repositories获取用户能访问到的Repository仓库列表GET /service/rest/v1/repositories $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/repositories' [ { \"name\" : \"YUM-Hosted\", \"format\" : \"yum\", \"type\" : \"hosted\", \"url\" : \"http://localhost:8081/repository/YUM-Hosted\" }, ... ] 此endpoint返回所有存储库，并且不允许分页。注意，存储库的顺序在多个查询之间是一致的，并且不是按字母顺序排列的。四、Assets API1、List Assets列出指定Repository仓库中包含的AssetsGET /service/rest/v1../assets 例如列出maven-central仓库中的Assets$ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1../assets?repository=Maven-Releases' { \"items\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/Maven-Releases/com/curiosuer/SpringBoot2/0.0.0/SpringBoot2-0.0.0.jar\", \"path\" : \"com/curiosuer/SpringBoot2/0.0.0/SpringBoot2-0.0.0.jar\", \"id\" : \"TWF2ZW4tUmVsZWFzZXM6MzZlM2RlYzhkZTUyOGM5YmRkYTdhZTNjZjlmYjFiNTY\", \"repository\" : \"Maven-Releases\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"c1ab61e9f407cbabaa8f3b377a76afa1f8afa4f1\", \"md5\" : \"5474cd7fc95a581eb6b6a3319c8aa6ba\" }, ... ], \"continuationToken\" : \"3f5cae01760233b6506547dc7be10e0b\" } 该endpoint使用分页策略，如果需要，可以使用该策略遍历所有资产。注意，资产的顺序在多个查询之间是一致的，并且不是按字母顺序排列的。2、Get AssetGET /service/rest/v1../assets/{id} This endpoint allows us to get the details of an individual asset.$ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1../assets/bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2MjRiOTEwMmMwMmNiYmU4YQ' { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/sonatype/nexus/buildsupport/nexus-buildsupport-metrics/2.9.1-02/nexus-buildsupport-metrics-2.9.1-02.pom\", \"path\" : \"org/sonatype/nexus/buildsupport/nexus-buildsupport-metrics/2.9.1-02/nexus-buildsupport-metrics-2.9.1-02.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2MjRiOTEwMmMwMmNiYmU4YQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"a3bf672b3ea844575acba3b84790e76ed86a7c66\", \"md5\" : \"49e439c814c3098450dc4bbee952463f\" }} 3、Delete AssetDELETE /service/rest/v1../assets/{id} This endpoint can be used to delete an individual asset.$ curl -u admin:admin123 -X DELETE 'http://localhost:8081/service/rest/v1../assets/bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2MjRiOTEwMmMwMmNiYmU4YQ' HTTP/1.1 204 No Content Date: Fri, 19 Jan 2018 20:41:47 GMT ... 五、Components API1、List Components遍历仓库中的ComponentsGET /service/rest/v1/components $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/components?repository=Maven-Central' { \"items\" : [ { \"id\" : \"bWF2ZW4tY2VudHJhbDo4ODQ5MWNkMWQxODVkZDEzNjMyNjhmMjIwZTQ1ZDdkZQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"group\" : \"com.google.guava\", \"name\" : \"guava\", \"version\" : \"21.0\", \"assets\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/com/google/guava/guava/21.0/guava-21.0.jar\", \"path\" : \"com/google/guava/guava/21.0/guava-21.0.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2MzA4OThiZjZmZTFkOTE2NA\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"3a3d111be1be1b745edfa7d91678a12d7ed38709\", \"md5\" : \"ddc91fd850fa6177c91aab5d4e4d1fa6\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/com/google/guava/guava/21.0/guava-21.0.jar.sha1\", \"path\" : \"com/google/guava/guava/21.0/guava-21.0.jar.sha1\", \"id\" : \"bWF2ZW4tY2VudHJhbDpmODk4YjM5MDNjYjk5YzU5MDc0MDFlYzRjNjVlNjU5OQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"a1ff60cb911e1f64801c03d03702044d10c9bdd3\", \"md5\" : \"e34b8695ede1677ba262411d757ea980\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/com/google/guava/guava/21.0/guava-21.0.pom\", \"path\" : \"com/google/guava/guava/21.0/guava-21.0.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDpkMDY0ODA0YThlZDVhZDZlOWJjNDgzOGE1MzM2OGZlZg\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"fe4fa08a8c0897f9896c7e278fb397ede4a2feed\", \"md5\" : \"5c10f97af2ce9db54fa6c2ea6997a8d7\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/com/google/guava/guava/21.0/guava-21.0.pom.sha1\", \"path\" : \"com/google/guava/guava/21.0/guava-21.0.pom.sha1\", \"id\" : \"bWF2ZW4tY2VudHJhbDplMDE4OGVkMDcyOGZhNjhmZDA3NDdkNjlhZDNmZjI5Nw\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"992b43ab7b3a061be47767e910cab58180325abc\", \"md5\" : \"33aed29aa0bb4e03ea7854066a5b4738\" } } ] }, ... ], \"continuationToken\" : \"88491cd1d185dd136f143f20c4e7d50c\" } 该endpoint使用分页策略，如果需要，可以使用该策略遍历所有资产。注意，资产的顺序在多个查询之间是一致的，并且不是按字母顺序排列的。2、Get Component获取仓库中component的详细信息GET /service/rest/v1/components/{id} $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/components/bWF2ZW4tY2VudHJhbDo4ODQ5MWNkMWQxODVkZDEzNjYwYmUwMjE1MjI2NGUwZQ' { \"id\" : \"bWF2ZW4tY2VudHJhbDo4ODQ5MWNkMWQxODVkZDEzNjYwYmUwMjE1MjI2NGUwZQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"group\" : \"org.apache.httpcomponents\", \"name\" : \"httpcomponents-client\", \"version\" : \"4.3.5\", \"assets\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/apache/httpcomponents/httpcomponents-client/4.3.5/httpcomponents-client-4.3.5.pom\", \"path\" : \"org/apache/httpcomponents/httpcomponents-client/4.3.5/httpcomponents-client-4.3.5.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2YTFhOGUxOGQxZmFkOGM3Mw\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"95d80a44673358a5dcbcc2f510770b9f93fe5eba\", \"md5\" : \"f4769c4e60799ede664414c26c6c5c9d\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/apache/httpcomponents/httpcomponents-client/4.3.5/httpcomponents-client-4.3.5.pom.sha1\", \"path\" : \"org/apache/httpcomponents/httpcomponents-client/4.3.5/httpcomponents-client-4.3.5.pom.sha1\", \"id\" : \"bWF2ZW4tY2VudHJhbDpmODk4YjM5MDNjYjk5YzU5ZDU3YjFlYjE0MzM1ZTcwMQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"6b98f5cef5d7102f8f45215bdcf48dc843d060af\", \"md5\" : \"f3b3ac640853fcb887621d13029a1747\" } } ] } 3、Upload Component上传Component到指定仓库中，一些格式的仓库允许上传的Component中包含多个Assets。 endpoint的上传参数取决于要上传Component到那个仓库的格式 POST /service/rest/v1/components $ curl -v -u admin:admin123 \\ -X POST 'http://localhost:8081/service/rest/v1/components?repository=maven-releases' \\ -F maven2.groupId=com.google.guava \\ -F maven2.artifactId=guava \\ -F maven2.version=24.0-jre \\ -F maven2.asset1=@guava-24.0-jre.jar \\ -F maven2.asset1.extension=jar \\ -F maven2.asset2=@guava-24.0-jre-sources.jar \\ -F maven2.asset2.classifier=sources \\ -F maven2.asset2.extension=jar HTTP/1.1 204 No Content Date: Fri, 19 Jan 2018 20:26:13 GMT ... ①MavenMaven format allows multiple assets to be uploaded as part of a single component. To upload multiple assets just follow the information from a table describing the given format and replace assetN with multiple instances of it (e.g. asset1, asset2, etc.): **Field name** **Field type** Required? Description maven2.groupId String Yes, unless a POM asset is included in the upload Group ID of the component maven2.artifactId String Yes, unless a POM asset is included in the upload Artifact ID of the component maven2.version String Yes, unless a POM asset is included in the upload Version of the component maven2.generate-pom Boolean No Whether the Nexus Repository Manager should generate a POM file based on above component coordinates provided maven2.packaging String No Define component packaging (e.g. jar, ear) maven2.assetN File Yes, at least one Binary asset maven2.assetN.extension String Yes Extension of the corresponding `assetN` asset maven2.assetN.classifier String No Classifier of the corresponding `assetN` asset Examples：Uploading a jar and Automatically Creating a pom File$ curl -v -u admin:admin123 \\ -F \"maven2.generate-pom=true\" \\ -F \"maven2.groupId=com.example\" \\ -F \"maven2.artifactId=commercial-product\" \\ -F \"maven2.packaging=jar\" \\ -F \"version=1.0.0\" \\ -F \"maven2.asset1=@/absolute/path/to/the/local/file/product.jar;type=application/java-archive\" \\ -F \"maven2.asset1.extension=jar\" \\ \"http://localhost:8081/service/rest/v1/components?repository=maven-third-party\" Upload a POM and associated JAR File$ curl -v -u admin:admin123 \\ -F \"maven2.generate-pom=false\" \\ -F \"maven2.asset1=@/absolute/path/to/the/local/file/pom.xml\" \\ -F \"maven2.asset1.extension=pom\" \\ -F \"maven2.asset2=@/absolute/path/to/the/local/file/product-1.0.0.jar;type=application/java-archive\" \\ -F \"maven2.asset2.extension=jar\" \\ \"http://localhost:8081/service/rest/v1/components?repository=maven-releases\" ②RawRaw supports multiple assets within a single component. **Field name** **Field type** Required? Description raw.directory String Yes Destination for upload files (e.g. /path/to/files) raw.assetN File Yes, at least one Binary asset raw.assetN.filename String Yes Filename to be used for the corresponding `assetN` asset ③PyPI **Field name** **Field type** **Required?** **Description** pypi.asset File Yes Binary asset ④RubyGems **Field name** **Field type** **Required?** **Description** rubygems.asset File Yes Binary asset ⑤NuGet **Field name** **Field type** **Required?** **Description** nuget.asset File Yes Binary asset ⑥NPM **Field name** **Field type** **Required?** **Description** npm.asset File Yes Binary asset 4、Delete Component：删除仓库中的componentDELETE /service/rest/v1/components/{id} $ curl -u admin:admin123 -X DELETE 'http://localhost:8081/service/rest/v1/components/bWF2ZW4tY2VudHJhbDo4ODQ5MWNkMWQxODVkZDEzNjYwYmUwMjE1MjI2NGUwZQ' HTTP/1.1 204 No Content Date: Fri, 19 Jan 2018 20:26:13 GMT ... 六、Metrics API 示例https://raw.githubusercontent.com/OpenShiftDemos/nexus/master/scripts/nexus-functions################################################################# # Functions for Managing Sonatype Nexus # # # # Authors: # # - Jorge Morales https://github.com/jorgemoralespou # # - Siamak Sadeghianfar https://github.com/siamaksade # # # ################################################################# ​ # # add_nexus2_repo [repo-id] [repo-url] [nexus-username] [nexus-password] [nexus-url] # ​ function add_nexus2_repo() { local _REPO_ID=$1 local _REPO_URL=$2 local _NEXUS_USER=$3 local _NEXUS_PWD=$4 local _NEXUS_URL=$5 ​ read -r -d '' _REPO_JSON Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:06 "},"origin/nexus-使用jenkins插件上传CI流程制品到Nexus仓库.html":{"url":"origin/nexus-使用jenkins插件上传CI流程制品到Nexus仓库.html","title":"Jenkins相关插件","keywords":"","body":"使用jenkins插件上传CI流程制品到Nexus仓库一、Overviews现在Nexus各个格式仓库中的制品大多数都是在Jenkins的持续集成CI流水线中生成的，每次流水线构建都需要其制品上传到Nexus中进行管理。Nexus针对Jenkins有Nexus Platform的插件来简化上传步骤，该插件主要用来上传Maven格式制品到Hosted类型的仓库中。同时，Jenkins CI Pipeline中除了可以使用该插件来上传Maven制品到Maven格式仓库，原始Curl也是可以的。插件Github：https://github.com/jenkinsci/nexus-platform-plugin 二、Jenkins使用Nexus Platform上传maven格式制品1、安装2、配置系统管理--> 系统设置--> Sonatype Nexus 3、Jenkins Job4、Jenkins Pipeline.....上文省略...... stage ('上传制品') { steps { script{ //读取源代码中的POM文件，获取生成制品的maven坐标信息（Jenkins需要安装pipeline-utility-steps插件） def pomfile = readMavenPom file: 'pom.xml' //使用Nexus Platform插件上传maven制品到Nexus的maven格式release仓库 nexusPublisher nexusInstanceId: 'curiouser-okd-nexus', \\ nexusRepositoryId: 'Maven-Releases', \\ packages: [[$class: 'MavenPackage', \\ mavenAssetList: [[classifier: '', extension: '', \\ filePath: \"target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging}\"]], \\ mavenCoordinate: [artifactId: \"${pomfile.artifactId}\", \\ groupId: \"${pomfile.groupId}\", \\ packaging: \"${pomfile.packaging}\", \\ version: \"${pomfile.version}\"]]] //拼接maven制品的搜索链接,该链接是以源代码POM文件中的maven制品坐标信息参数对nexus api进行搜索，返回的response会重定向到制品的下载链接 echo \"The Jar Format Asset of Maven have been pushed to Hosted Repository: Maven-Release. The Download URL of the Asset: http://Nexus-IP地址:8081/service/rest/v1/search../assets/download?maven.groupId=${pomfile.groupId}&maven.artifactId=${pomfile.artifactId}&maven.baseVersion=${pomfile.version}&maven.extension=jar&maven.classifier\" } } } .....下文省略...... 5、Jenkins使用Curl命令手动上传maven制品到Nexus仓库中stage(\"上传制品\"){ steps{ script{ //读取源代码中的POM文件，获取生成制品的maven坐标信息（Jenkins需要安装pipeline-utility-steps插件） def pomfile = readMavenPom file: 'pom.xml' //使用curl命令通过Nexus API接口上传制品到RAW仓库。下载URL既是上传URL sh \"curl -sL -w 'Upload the jar to the repository status code: %{http_code}\\n' -u admin:****** \" + \"--upload-file target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging} \" + \"http://Nexus-IP地址:8081/repository/jenkins-product-repository/${pomfile.artifactId}-${pomfile.version}-${params.BUILD_VERSION}-${params.BUILD_ID}.${pomfile.packaging}\" } } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:07 "},"origin/SonarQube静态代码扫描分析简介.html":{"url":"origin/SonarQube静态代码扫描分析简介.html","title":"SonarQube静态代码扫描分析","keywords":"","body":"一、静态代码分析Why：在软件开发过程中，开发团队往往要花费大量的时间和精力发现并修改代码缺陷。而发现BUG越晚，修复的成本越大 缺陷引入的大部分是在编码阶段，但发现更多的是在单元测试、集成测试、功能测试阶段 30% 至 70% 的代码逻辑设计和编码缺陷是可以通过静态代码分析来发现和修复的 How：在编码阶段，可以通过以下手段发现源代码问题，从源头及时规避，保证代码质量静态代码扫描工具 Code Review What：Code Review往往要求大量的时间消耗和相关知识的积累，因此对于软件开发团队来说，使用静态代码分析工具自动化执行代码检查和分析，能够极大地提高软件可靠性并节省软件开发和测试成本。帮助程序开发人员自动执行静态代码分析，快速定位代码隐藏错误和缺陷 助代码设计人员更专注于分析和解决代码设计缺陷 显著减少在代码逐行检查上花费的时间，提高软件可靠性并节省软件开发和测试成本 常见的一些静态分析工具Checkstyle：SourceForge 的开源项目，通过检查对代码编码格式，命名约定，Javadoc，类设计等方面进行代码规范和风格的检查，从而有效约束开发人员更好地遵循代码编写规范 FindBugs：由马里兰大学提供的一款开源 Java 静态代码分析工具。FindBugs 通过检查类文件或 JAR 文件，将字节码与一组缺陷模式进行对比从而发现代码缺陷，完成静态代码分析 PMD：由 DARPA 在 SourceForge 上发布的开源 Java 代码静态分析工具。PMD 通过其内置的编码规则对 Java 代码进行静态检查，主要包括对潜在的 bug，未使用的代码，重复的代码，循环体创建新对象等问题的检验 二、Sonar简介Sonar是一个用于代码质量管理的开源平台，可以从 七个维度检测代码质量 Sonar可以通过PMD、CheckStyle、Findbugs等代码规则检测工具来检测你的代码，帮助你发现代码的漏洞，Bug，异味等信息。 Sonar最大的特点就是插件化，可以根据不同的场景需求进行插件化安装，可以同时可以检测Python、C++等多种语言。 Sonar客户端可以采用IDE插件、Sonar-Scanner插件、Ant插件和Maven插件等多种方式，并通过各种不同的分析机制对项目源代码进行分析和扫描，并把分析扫描后的结果上传到sonar的数据库，通过sonar web界面对分析结果进行管理 Sonar的架构体系Project：是需要被分析的源码 SonarQube Scanner：用于执行代码分析的工具，SonarQube Scanner分析完毕之后，会将结果上报到指定的SonarQube Server。 SonarQube Server：显示分析结果的Web Server，在SonarQube Scanner第一次将一个工程的分析结果上报给SonarQube Server后，Server上会自动创建一个工程显示分析的结果，可以在Server上设置代码质量管理相关的各种配置，如设置代码检查规则（Rule）和质量门限（Quality Gate）等。SonarQube Server包含三个子进程（web服务（界面管理）、搜索服务、计算引擎服务（写入数据库）） SonarQube Database：保存SonarQube服务端的权限配置，插件配置，项目快照，项目视图等 三、自动化扫描分析源代码的流程官方推荐的自动化扫描流程自动化静态代码扫描流程本地开发：JetBrains Intellij IDEA 、Eclipse安装阿里巴巴的代码检查规范插件，可在编写代码时提示规范信息；安装使用sonarlint插件在本地运行代码扫描Gitlab：Gitlab代码仓库可设置事件监听器，例如PUSH事件、Merge Request事件等。发送Web-hook到外部系统Jenkins：Jenkins中可安装Gitlab插件，用于设置特定的Web-hook后端监听器来触发当前任务。Jenkins Pipeline：在Jenkins Pipeline中获取Web-hook信息来拉取代码，然后编译、执行Sonar Scanner扫描源代码文件或二进制文件，最后将扫描的结果发送SnarQube进行存储、展示、管理等操作SonarQube：四、SonarQube服务端配置1. 配置代码规则插件2. 配置全局参数3. 管理扫描结果4. 质量门禁五、Sonar体系中的配置参数生效优先级UI界面中的全局参数配置 项目UI界面中的参数配置 项目分析客户端全局配置文件中的参数例如sonar scanner的全局配置文件/opt/sonarscanner/conf/sonar.properties中的参数 例如sonar scanner Maven插件在settings.xml中配置的参数 项目分析客户端命令行运行时配置的参数，例如sonar-scanner二进制命令行运行时以“-D”开头的配置参数 六、扫描器SonarScanner当SonarQube服务端搭建配置好了，Sonar提供了各种插件形式的Sonar Scanner扫描器供你选择来扫描你的源代码。SonarScanner：下载二进制客户端进行扫描 SonarScanner for Maven：以Maven插件的形式扫描代码 SonarScanner for Jenkins：以Jenkins插件的形式配置扫描代码 SonarScanner for Gradle：以Gradle插件的形式配置扫描代码 SonarScanner for Ant：以Ant插件的形式配置扫描代码 SonarScanner项目扫描参数官方文档说明 参数 描述 默认值 是否必要 sonar.host.url SonarQube服务端地址 http://localhost:9000 是 sonar.projectKey 项目的唯一标识。以`字母`,`-`,`_`,`:`,至少有一个非数字 对于Maven插件的话,默认值是 `:`其他形式插件不提供默认值 是 sonar.projectName 在SonarQube Web UI上面显示的项目名 对于Maven插件形式,默认值是``其他形式插件不提供默认值 否 sonar.projectVersion 项目的扫描版本 对于Maven插件形式,默认值是``其他形式插件不提供默认值 否 sonar.login 发送扫描结果到SonarQube时的认证方式之一。值类型可为`用户生成的认证Token`，`用户名` 是 sonar.password 当`sonar.login`值类型为认证Token时，则不填 是 sonar.ws.timeout 等待服务端响应的最大秒数 60 否 sonar.projectDescription 项目描述。用于在项目Web UI中显示项目的描述 对于Maven插件形式,默认值是`` 否 sonar.links.homepage 项目地址。用于在项目Web UI中显示项目访问链接 对于Maven插件形式,默认值是`` 否 sonar.links.issue 项目代码Issue管理地址。用于在项目Web UI中显示Issue管理链接 对于Maven插件形式,默认值是`` 否 sonar.links.scm 项目源代码仓库地址。用于在项目Web UI中显示源代码仓库链接 `` for Maven projects 否 sonar.sources 以逗号分割的main源代码文件夹路径 否 sonar.tests 以逗号分割的测试源代码文件夹路径 否 sonar.sourceEncoding 源代码文件的编码格式，例如：`UTF-8`, `MacRoman`, `Shift_JIS` 系统的编码格式 否 sonar.externalIssuesReportPaths 否 sonar.projectDate 格式： `yyyy-MM-dd`, 例如: 2010-12-01 Current date 否 sonar.projectBaseDir 针对多模块项目时，指定要扫描源代码的目录路径 否 sonar.working.directory 指定Sonarscanner的工作空间。必须是不存在的，相对路径。针对MSBuild的插件，此参数不兼容 ~/.scannerwork 否 sonar.scm.provider 否 sonar.scm.forceReloadAll 否 sonar.scm.exclusions.disabled 否 sonar.scm.revision 否 sonar.buildString 100 否 sonar.analysis.[yourKey] 10 否 sonar.log.level 控制Sonarscanner输出日志的级别 INFO 否 sonar.verbose 输出更多Sonarscanner客户端和Sonarqube服务的扫描信息 false 否 sonar.showProfiling 否 sonar.scanner.dumpToFile 输出扫描期间所有的配置参数到文件中 否 sonar.scanner.metadataFilePath 指定report-task.txt文件的生成路径 等于sonar.working.directory的值 否 1. SonarScanner下载地址：https://docs.sonarqube.org/latest/analysis/scan/sonarscanner/配置文件全局配置文件路径：$安装目录/conf/sonar-scanner.properties 项目配置文件路径：$项目根目录/sonar-project.properties CLI命令参数$ sonar-scanner usage: sonar-scanner [options] 参数: -D,--define Define property -h,--help Display help information -v,--version Display version information -X,--debug Produce execution debug output If you need more debug information you can add one of the following to your command line: -X, --verbose, or -Dsonar.verbose=true. 2. SonarScanner for Maven官方文档：https://docs.sonarqube.org/latest/analysis/scan/sonarscanner-for-maven/注意：从maven-sonar-plugin 3.4.0.905开始，不再支持SonarQube 从maven-sonar-plugin 3.1开始，不再支持Maven 全局参数详见SonarQube服务端配置配置Maven的setting.xml org.sonarsource.scanner.maven sonar true http://myserver:9000 f6eedc3d8bfa850a15f2ffcd 在项目pom.xml中配置项目扫描参数 ....上文省略.... com.curiosuer springboot2 0.0.1 用于演示Spring Boot2的一些功能 Curiouser-Demo-SpringBoot2-${gitlabBranch} http://springboot2-demo.apps.okd311.curiouser.com/swagger-ui.html ${gitlabSourceRepoHomepage}/commit/${gitlabMergeRequestLastCommit} ${gitlabSourceRepoHomepage}/issues ${BUILD_URL} UTF-8 UTF-8 1.8 指定的值 --> ${gitlabMergeRequestLastCommit} src/main/ 1 ${gitlabMergeRequestLastCommit} ${gitlabMergeRequestLastCommit} reuseReports jacoco ....下文省略.... 注意：pom.xml中有些参数的值是可以在Jenkins CI流水线中通过环境变量获取的。执行扫描命令mvn test sonar:sonar -Dspring.profiles.active=local 默认参数sonar.projectKey ==> POM中的: sonar.projectName ==> POM中的 sonar.projectVersion ==> POM中的 sonar.projectDescription ==> POM中的 sonar.links.homepage ==> POM中的 sonar.links.ci ==> POM中的 sonar.links.issue ==> POM中的 sonar.links.scm ==> POM中的 七、扫描结果解析附录1、Sonar检查代码质量的七个维度复杂度分布（complexity）：代码复杂度过高将难以理解、难以维护 重复代码（duplications）：程序中包含大量复制粘度的代码是质量低下的表现 单元测试（unit tests）：统计并展示单元测试覆盖率 编码规范（coding rules）：通过Findbugs、PMD、CheckStyle等规范代码编写 注释（commments）：少了可读性差，多了看起来费劲 潜在的Bug（potential bugs）：通过Findbugs、PMD、CheckStyle等检测潜在bug 结构与设计（architecture & design）：依赖i、耦合等 2、常见检查分析工具的内置规范Checkstyle：分析源代码文件Javadoc 注释：检查类及方法的 Javadoc 注释 命名约定：检查命名是否符合命名规范 标题：检查文件是否以某些行开头 Import 语句：检查 Import 语句是否符合定义规范 代码块大小，即检查类、方法等代码块的行数 空白：检查空白符，如 tab，回车符等 修饰符：修饰符号的检查，如修饰符的定义顺序 块：检查是否有空块或无效块 代码问题：检查重复代码，条件判断，魔数等问题 类设计：检查类的定义是否符合规范，如构造函数的定义等问题 FindBugs：分析字节码文件Bad practice 坏的实践：常见代码错误，用于静态代码检查时进行缺陷模式匹配 Correctness 可能导致错误的代码，如空指针引用等 国际化相关问题：如错误的字符串转换 可能受到的恶意攻击，如访问权限修饰符的定义等 多线程的正确性：如多线程编程时常见的同步，线程调度问题。 运行时性能问题：如由变量定义，方法调用导致的代码低效问题。 PMD：：分析源代码文件可能的 Bugs：检查潜在代码错误，如空 try/catch/finally/switch 语句 未使用代码（Dead code）：检查未使用的变量，参数，方法 复杂的表达式：检查不必要的 if 语句，可被 while 替代的 for 循环 重复的代码：检查重复的代码 循环体创建新对象：检查在循环体内实例化新对象 资源关闭：检查 Connect，Result，Statement 等资源使用之后是否被关闭掉 Jtest可能的错误：如内存破坏、内存泄露、指针错误、库错误、逻辑错误和算法错误等 未使用代码：检查未使用的变量，参数，方法 初始化错误：内存分配错误、变量初始化错误、变量定义冲突 命名约定：检查命名是否符合命名规范 Javadoc 注释：检查类及方法的 Javadoc 注释 线程和同步：检验多线程编程时常见的同步，线程调度问题 国际化问题： 垃圾回收：检查变量及 JDBC 资源是否存在内存泄露隐患 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:51:58 "},"origin/sonarscanner-将扫描结果以comment的形式回写到gitlab.html":{"url":"origin/sonarscanner-将扫描结果以comment的形式回写到gitlab.html","title":"SonarScanner-将扫描结果以comment的形式回写到gitlab","keywords":"","body":"SonarScanner使用Sonarqube的Gitlab插件将扫描结果以gitlab comment的形式回写到Gitlab一、Context在Jenkins中做CI过程中,有一个步骤是代码编译完,使用sonar scanner扫描代码,检查静态代码中的语法错误等,然后将扫描结果发送到sonarqube,供项目经理查看代码质量. sonarqube可以安装插件gitlab,让sonarscanner扫描完代码,将结果以gitlab注释的方式回写到提交的commit中.方便开发人员排查代码.以下操作过程各组件的版本sonarqube: 7.3 (build 15553) sonarscanner: 3.3.0.1492 sonarqube gitlab插件: 4.0.0 gitlab: 10.8.4 ce jenkins: 2.150.2 Jenkins CI流水线是在使用Jenkins Slave(Kubernetes插件动态生成Slave POD)节点中来运行的,所以Sonarscanner,Maven等工具都是在Kubernetes Jenkins Slave镜像中已经安装好的.二、操作1、安装sonar-gitlab-plugin插件插件Github:https://github.com/gabrie-allaigre/sonar-gitlab-plugin/ 2、生成用户访问Token3、gitlab创建sonarscanner的用户,并生成AccessKey4、在gitlab中将sonarqube加入到对应项目仓库的Members中5、Sonarqube中编辑gitlab插件的全局配置扫描项目时，扫描参数生效优先级如下：UI界面中的全局参数配置 项目UI界面中的参数配置 项目分析客户端全局配置文件中的参数（例如sonar scanner的全局配置文件/opt/sonarscanner/conf/sonar.properties中的参数） 项目分析客户端命令行中配置的参数 所以可以在UI界面全局配置中配置一些通用、不经常变动的、由管理员控制的参数。例如：gitlab插件的通用配置、gitlab地址等参数6、Jenkins Pipeline中使用sonarscanner扫描代码stage(\"代码扫描\"){ steps{ sh \"sonar-scanner \\ -Dsonar.host.url=http://sonarqube.apps.okd311.curiouser.com \\ -Dsonar.login=6a6fa6f1702ae42f8d0a0fe14166d9a2 \\ -Dsonar.projectName=demo-springboot2-$GITLABSOURCEBRANCH \\ -Dsonar.projectKey=demo-springboot2-$GITLABSOURCEBRANCH \\ -Dsonar.projectVersion=$GIT_COMMIT \\ -Dsonar.sourceEncoding=UTF-8 \\ -Dsonar.sources=src/main \\ -Dsonar.test=src/test \\ -Dsonar.java.binaries=target/classes \\ -Dsonar.java.test.binaries='target/test-classes/*/*.class' \\ -Dsonar.java.source=8 \\ -Dsonar.gitlab.project_id=1 \\ -Dsonar.gitlab.commit_sha=$GIT_COMMIT \\ -Dsonar.gitlab.ref_name=$GIT_BRANCH \\ -Dsonar.java.coveragePlugin=jacoco \\ -Dsonar.dynamicAnalysis=reuseReports \" } } 三、效果 四、Soanarscanner Gitlab插件参数详解 Variable Comment Type Version sonar.gitlab.url GitLab url Administration, Variable >= 1.6.6 sonar.gitlab.max_global_issues Maximum number of anomalies to be displayed in the global comment Administration, Variable >= 1.6.6 sonar.gitlab.user_token Token of the user who can make reports on the project, either global or per project Administration, Project, Variable >= 1.6.6 sonar.gitlab.project_id Project ID in GitLab or internal id or namespace + name or namespace + path or url http or ssh url or url or web Project, Variable >= 1.6.6 sonar.gitlab.commit_sha SHA of the commit comment Variable >= 1.6.6 sonar.gitlab.ref Branch name or reference of the commit Variable sonar.gitlab.ref_name Branch name or reference of the commit Variable >= 1.6.6 sonar.gitlab.max_blocker_issues_gate Max blocker issue for build failed (default 0). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_critical_issues_gate Max critical issues for build failed (default 0). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_major_issues_gate Max major issues for build failed (default -1 no fail). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_minor_issues_gate Max minor issues for build failed (default -1 no fail). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_info_issues_gate Max info issues for build failed (default -1 no fail). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.ignore_certificate Ignore Certificate for access GitLab, use for auto-signing cert (default false) Administration, Variable >= 2.0.0 sonar.gitlab.comment_no_issue Add a comment even when there is no new issue (default false) Administration, Variable >= 2.0.0 sonar.gitlab.disable_inline_comments Disable issue reporting as inline comments (default false) Administration, Variable >= 2.0.0 sonar.gitlab.only_issue_from_commit_file Show issue for commit file only (default false) Variable >= 2.0.0 sonar.gitlab.only_issue_from_commit_line Show issue for commit line only (default false) Variable >= 2.1.0 sonar.gitlab.build_init_state State that should be the first when build commit status update is called (default pending) Administration, Variable >= 2.0.0 sonar.gitlab.disable_global_comment Disable global comment, report only inline (default false) Administration, Variable >= 2.0.0 sonar.gitlab.failure_notification_mode Notification is in current build (exit-code) or in commit status (commit-status) (default commit-status) Administration, Variable >= 2.0.0 sonar.gitlab.global_template Template for global comment in commit Administration, Variable >= 2.0.0 sonar.gitlab.ping_user Ping the user who made an issue by @ mentioning. Only for default comment (default false) Administration, Variable >= 2.0.0 sonar.gitlab.unique_issue_per_inline Unique issue per inline comment (default false) Administration, Variable >= 2.0.0 sonar.gitlab.prefix_directory Add prefix when create link for GitLab Variable >= 2.1.0 sonar.gitlab.api_version GitLab API version (default `v4` or `v3`) Administration, Variable >= 2.1.0 sonar.gitlab.all_issues All issues new and old (default false, only new) Administration, Variable >= 2.1.0 sonar.gitlab.json_mode Create a json report in root for GitLab EE (codeclimate.json or gl-sast-report.json) Project, Variable >= 3.0.0 sonar.gitlab.query_max_retry Max retry for wait finish analyse for publish mode Administration, Variable >= 3.0.0 sonar.gitlab.query_wait Max retry for wait finish analyse for publish mode Administration, Variable >= 3.0.0 sonar.gitlab.quality_gate_fail_mode Quality gate fail mode: error, warn or none (default error) Administration, Variable >= 3.0.0 sonar.gitlab.issue_filter Filter on issue, if MAJOR then show only MAJOR, CRITICAL and BLOCKER (default INFO) Administration, Variable >= 3.0.0 sonar.gitlab.load_rules Load rules for all issues (default false) Administration, Variable >= 3.0.0 sonar.gitlab.disable_proxy Disable proxy if system contains proxy config (default false) Administration, Variable >= 4.0.0 sonar.gitlab.merge_request_discussion Allows to post the comments as discussions (default false) Project, Variable >= 4.0.0 sonar.gitlab.ci_merge_request_iid The IID of the merge request if it’s pipelines for merge requests Project, Variable >= 4.0.0 五、问题1、当项目是私有仓库时2、获取项目仓库的ProjectID 3、gitlab插件4.0.0无法兼容Sonarqube 7.6-community至7.9-community的版本报错如下！插件GIthub的原始Issue：https://github.com/gabrie-allaigre/sonar-gitlab-plugin/issues/213[ERROR] Failed to execute goalorg.sonarsource.scanner.maven:sonar-maven-plugin:3.6.0.1398:sonar(default-cli) on project egsdloen-bc-facade:com.talanlabs.sonar.plugins.gitlab.CommitPublishPostJob hasunsatisfied dependency 'classcom.talanlabs.sonar.plugins.gitlab.ReporterBuilder' for constructor'public com.talanlabs.sonar.plugins.gitlab.CommitPublishPostJo(com.talanlabs.sonar.plugins.gitlab.GitLabPluginConfigurationcom.talanlabs.sonar.plugins.gitlab.SonarFacadecom.talanlabs.sonar.plugins.gitlab.CommitFacadecom.talanlabs.sonar.plugins.gitlab.ReporterBuilder)' fromorg.sonar.core.platformComponentContainer$ExtendedDefaultPicoContainer@7615666e:512[Immutable:org.sonar.core.platform.ComponentContainer$ExtendedDefaultPicoContaner@364adb24:56 [Help 1] org.apache.maven.lifecycle.LifecycleExecutionException: Failed toexecute goalorg.sonarsource.scanner.maven:sonar-maven-plugin:3.6.0.1398:sonar(default-cli) on project egsdloen-bc-facade:com.talanlabs.sonar.plugins.gitlab.CommitPublishPostJob hasunsatisfied dependency 'classcom.talanlabs.sonar.plugins.gitlab.ReporterBuilder' for constructor'public com.talanlabs.sonar.plugins.gitlab.CommitPublishPostJo(com.talanlabs.sonar.plugins.gitlab.GitLabPluginConfigurationcom.talanlabs.sonar.plugins.gitlab.SonarFacadecom.talanlabs.sonar.plugins.gitlab.CommitFacadecom.talanlabs.sonar.plugins.gitlab.ReporterBuilder)' fromorg.sonar.core.platformComponentContainer$ExtendedDefaultPicoContainer@7615666e:512[Immutable:org.sonar.core.platform.ComponentContainer$ExtendedDefaultPicoContaner@364adb24:56 原因解决方案已经修改编译好的插件Jar包：https://github.com/gabrie-allaigre/sonar-gitlab-plugin/releases/download/4.1.0-SNAPSHOT/sonar-gitlab-plugin-4.1.0-SNAPSHOT.jar参考链接https://gitlab.com/gitlab-org/gitlab-ce/issues/28342 https://www.cnblogs.com/amyzhu/p/8988519.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:12 "},"origin/ldap-Jenkins对接LDAP.html":{"url":"origin/ldap-Jenkins对接LDAP.html","title":"Jenkins","keywords":"","body":"一. ContextOpenLDAP的条目组织形式 二. Jenkins配置1. Jenkins安装LDAP插件安装插件有两种方法：方法一：后台插件管理里直接安装优点：简单方便，不需要考虑插件依赖问题 缺点：因为网络等各种问题安装不成功 安装方法：登录Jenkins --> 系统管理 --> 插件管理 --> 可选插件 --> 搜索LDAP --> 选中 --> 直接安装 --> 安装完成重启方法二：官网下载安装文件后台上传优点：一定可以安装成功的 缺点：麻烦，要去官网找插件并解决依赖 安装方法：官网下载插件 --> 登录Jenkins --> 系统管理 --> 插件管理 --> 高级 --> 上传插件 --> 选择文件 --> 上传 --> 安装完成后重启 LDAP插件下载地址：https://updates.jenkins.io/download/plugins/ldap/ 2. 登录Jenkins --> 系统管理 --> 全局安全配置root DN：这里的root DN只是指搜索的根，并非LDAP服务器的root dn。由于LDAP数据库的数据组织结构类似一颗大树，而搜索是递归执行的，理论上，我们如果从子节点（而不是根节点）开始搜索，因为缩小了搜索范围那么就可以获得更高的性能。这里的root DN指的就是这个子节点的DN，当然也可以不填，表示从LDAP的根节点开始搜索 User search base：这个配置也是为了缩小LDAP搜索的范围，例如Jenkins系统只允许ou为Admin下的用户才能登陆，那么你这里可以填写ou=Admin，这是一个相对的值，相对于上边的root DN，例如你上边的root DN填写的是dc=domain,dc=com，那么user search base这里填写了ou=Admin，那么登陆用户去LDAP搜索时就只会搜索ou=Admin,dc=domain,dc=com下的用户了 User search filter：这个配置定义登陆的“用户名”对应LDAP中的哪个字段，如果你想用LDAP中的uid作为用户名来登录，那么这里可以配置为uid={0}（{0}会自动的替换为用户提交的用户名），如果你想用LDAP中的mail作为用户名来登录，那么这里就需要改为mail={0}。在测试的时候如果提示你user xxx does not exist，而你确定密码输入正确时，就要考虑下输入的用户名是不是这里定义的这个值了 Group search base：参考上边User search base解释 Group search filter：这个配置允许你将过滤器限制为所需的objectClass来提高搜索性能，也就是说可以只搜索用户属性中包含某个objectClass的用户，这就要求你对你的LDAP足够了解，一般我们也不配置 Group membership：没配置，没有详细研究 Manager DN：这个配置在你的LDAP服务器不允许匿名访问的情况下用来做认证（详细的认证过程参考文章LDAP落地实战（二）：SVN集成OpenLDAP认证中关于LDAP服务器认证过程的讲解），通常DN为cn=admin,dc=domain,dc=com这样 Manager Password：上边配置dn的密码 Display Name LDAP attribute：配置用户的显示名称，一般为显示名称就配置为uid，如果你想显示其他字段属性也可以这里配置，例如mail Email Address LDAP attribute：配置用户Email对应的字段属性，一般没有修改过的话都是mail，除非你用其他的字段属性来标识用户邮箱，这里可以配置 3. 登录验证参考链接https://mp.weixin.qq.com/s/S5ozDJSh4yTSfP_glNoiOQ https://plugins.jenkins.io/ldap https://wiki.jenkins.io/display/JENKINS/LDAP+Plugin#LDAPPlugin-Groupmembership https://www.cnblogs.com/zhaojiedi1992/p/zhaojiedi_liunx_52_ldap_for_jenkins.html https://blog.csdn.net/wanglei_storage/article/details/52935312 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:02 "},"origin/ldap-SonarQube对接LDAP.html":{"url":"origin/ldap-SonarQube对接LDAP.html","title":"SonarQube","keywords":"","body":"一、ContextOpenLDAP的条目组织形式 Sonaeqube官方文档的操作步骤二、操作1、Sonarqube安装LDAP插件配置--> 应用市场2、修改配置文件/opt/sonarqube/conf/sonar.properties如果sonarqube的部署实例是使用Dockers的话，则可通过环境变量的方式注入以下配置sonar.security.realm=LDAP sonar.forceAuthentication=true ldap.authentication=simple ldap.url=ldap://openldap-service.openldap.svc:389 ldap.bindDn=cn=admin,dc=curiouser,dc=com ldap.bindPassword=****** # User Configuration ldap.user.baseDn=ou=employee,dc=curiouser,dc=com ldap.user.request=(&(memberOf=cn=sonarqube,ou=applications,dc=curiouser,dc=com)(cn={0})) ldap.user.realNameAttribute=sn ldap.user.emailAttribute=mail 相关配置 Property Description Default value Required Example sonar.security.realm Set this to LDAP authenticate first against the external sytem. If the external system is not reachable or if the user is not defined in the external system, authentication will be performed against SonarQube's internal database. none Yes LDAP (only possible value) sonar.authenticator.downcase Set to true when connecting to a LDAP server using a case-insensitive setup. false No ldap.url URL of the LDAP server. If you are using ldaps, you should install the server certificate into the Java truststore. none Yes ldap://localhost:10389 ldap.bindDn The username of an LDAP user to connect (or bind) with. Leave this blank for anonymous access to the LDAP directory. none No cn=sonar,ou=users,o=mycompany ldap.bindPassword The password of the user to connect with. Leave this blank for anonymous access to the LDAP directory. none No secret ldap.authentication Possible values: simple, CRAM-MD5, DIGEST-MD5, GSSAPI. See the tutorial on authentication mechanisms simple No ldap.realm See Digest-MD5 Authentication, CRAM-MD5 Authentication none No example.org ldap.contextFactoryClass Context factory class. com.sun.jndi.ldap.LdapCtxFactory No ldap.StartTLS Enable use of StartTLS false No ldap.followReferrals Follow referrals or not. See Referrals in the JNDI true 用户配置 Property Description Default value Required Example ldap.user.baseDn Distinguished Name (DN) of the root node in LDAP from which to search for users. None Yes cn=users,dc=example,dc=org ldap.user.request LDAP user request. (&(objectClass=inetOrgPerson)(uid={login})) No (&(objectClass=user)(sAMAccountName={login})) ldap.user.realNameAttribute Attribute in LDAP defining the user’s real name. cn No ldap.user.emailAttribute Attribute in LDAP defining the user’s email. mail No Group Mapping Only groups are supported (not roles). Only static groups are supported (not dynamic groups).For the delegation of authorization, groups must be first defined in SonarQube. Then, the following properties must be defined to allow SonarQube to automatically synchronize the relationships between users and groups. Property Description Default value Required Example for Active Directory ldap.group.baseDn Distinguished Name (DN) of the root node in LDAP from which to search for groups. none No cn=groups,dc=example,dc=org ldap.group.request LDAP group request. (&(objectClass=groupOfUniqueNames)(uniqueMember={dn})) No (&(objectClass=group)(member={dn})) ldap.group.idAttribute Property used to specifiy the attribute to be used for returning the list of user groups in the compatibility mode. cn No sAMAccountName 重启Sonarqube，启动过程中如果出现以下日志，则证明LDAP连接成功INFO org.sonar.INFO Security realm: LDAP ... INFO o.s.p.l.LdapContextFactory Test LDAP connection: OK 3、登录验证4、权限控制将admin用户的管理员权限删除，赋予另一个用户参考链接https://hub.docker.com/_/sonarqube?tab=description https://docs.sonarqube.org/latest/instance-administration/delegated-auth/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:02 "},"origin/ldap-Gitlab对接LDAP.html":{"url":"origin/ldap-Gitlab对接LDAP.html","title":"Gitlab","keywords":"","body":"一、ContextOpenLDAP的条目组织形式 二、配置1. 修改/etc/gitlab/gitlab.rb..................省略............................. gitlab_rails['ldap_enabled'] = true ###! **remember to close this block with 'EOS' below** gitlab_rails['ldap_servers'] = YAML.load 三、测试登录四、注意当用户第一次使用LDAP登录GitLab时，如果其LDAP电子邮件地址是现有GitLab用户的电子邮件地址时，那么LDAP DN用户将与现有gitlab用户相关联。如果在GitLab的数据库中没有找到LDAP电子邮件属性，就会创建一个新用户。换句话说，如果现有的GitLab用户希望自己启用LDAP登录，那么他们应该检查他们的GitLab电子邮件地址是否匹配LDAP电子邮件地址，然后通过他们的LDAP凭证登录GitLab。https://docs.gitlab.com/ee/administration/auth/ldap.html#enabling-ldap-sign-in-for-existing-gitlab-users参考链接https://blog.csdn.net/tongdao/article/details/52538365 https://docs.gitlab.com/ee/administration/auth/ldap.html#configuration https://docs.gitlab.com/ee/administration/auth/how_to_configure_ldap_gitlab_ce/index.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:02 "},"origin/ldap-Nexus对接LDAP.html":{"url":"origin/ldap-Nexus对接LDAP.html","title":"Nexus","keywords":"","body":"Preflight Nexus 3 OpenLDAP 3.15.2-01 1.2.4 一、ContextOpenLDAP的条目组织形式 二、Nexus设置1. Nexus开启认证Realm2. 配置LDAPName：Enter a unique name for the new configuration. LDAP server address：Enter Protocol, Hostname, and Port of your LDAP server. Protocol：Valid values in this drop-down are ldap and ldaps that correspond to the Lightweight Directory Access Protocol and the Lightweight Directory Access Protocol over SSL. Hostname：The hostname or IP address of the LDAP server. Port：The port on which the LDAP server is listening. Port 389 is the default port for the ldap protocol, and port 636 is the default port for the ldaps. Search base：The search base further qualifies the connection to the LDAP server. The search base usually corresponds to the domain name of an organization. For example, the search base could be dc=example,dc=com. Note: If the values in your search base contain spaces, escape them with \"%20\", as in \"dc=example%20corp,dc=com\" You can configure one of four authentication methods to be used when connecting to the LDAP Server with the Authentication method drop-down. Simple Authentication：Simple authentication consists of a Username and Password. Simple authentication is not recommended for production deployments not using the secure ldaps protocol as it sends a clear-text password over the network. Anonymous Authentication：The anonymous authentication uses the server address and search base without further authentication. Digest-MD5：This is an improvement on the CRAM-MD5 authentication method. For more information, see RFC-2831. CRAM-MD5：The Challenge-Response Authentication Method (CRAM) is based on the HMAC-MD5 MAC algorithm. In this authentication method, the server sends a challenge string to the client. The client responds with a username followed by a Hex digest that the server compares to an expected value. For more information, see RFC-2195.For a full discussion of LDAP authentication approaches, see RFC-2829 and RFC-2251. SASL Realm：The Simple Authentication and Security Layer (SASL) realm used to connect to the LDAP server. It is only available if the authentication method is Digest-MD5 or CRAM-MD5. Username or DN：Username or DN (Distinguished Name) of an LDAP user with read access to all necessary users and groups. It is used to connect to the LDAP server. Password：Password for the Username or DN configured above. Base DN：Corresponds to the collection of distinguished names used as the base for user entries. This DN is relative to the Search Base. For example, if your users are all contained in ou=users,dc=sonatype,dc=com and you specified a Search Base of dc=sonatype,dc=com, you use a value of ou=users. User subtree：Check the box if True. Uncheck if False. Values are true if there is a tree below the Base DN that can contain user entries and false if all users are contain within the specified Base DN. For example, if all users are in ou=users,dc=sonatype,dc=com this field should be False. If users can appear in organizational units within organizational units such as ou=development,ou=users,dc=sonatype,dc=com, this field should be True . Object class：This value is a standard object class defined in RFC-2798. It specifies the object class for users. Common values are inetOrgPerson, person, user, or posixAccount. User filter：This allows you to configure a filter to limit the search for user records. It can be used as a performance improvement. User ID attribute：This is the attribute of the object class specified above, that supplies the identifier for the user from the LDAP server. The repository manager uses this attribute as the User ID value. Real name attribute：This is the attribute of the Object class that supplies the real name of the user. The repository manager uses this attribute when it needs to display the real name of a user similar to usage of the internal First name and Last name attributes. Email attribute：This is the attribute of the Object class that supplies the email address of the user. The repository manager uses this attribute for the Email attribute of the user. It is used for email notifications of the user. Password attribute：It can be used to configure the Object class, which supplies the password (\"userPassword\"). If this field is blank the user will be authenticated against a bind with the LDAP server. The password attribute is optional. When not configured authentication will occur as a bind to the LDAP server. Otherwise this is the attribute of the Object class that supplies the password of the user. The repository manager uses this attribute when it is authenticating a user against an LDAP server. Group Base DN：This field is similar to the Base DN field described for User Element Mapping, but applies to groups instead of users. For example, if your groups were defined under ou=groups,dc=sonatype,dc=com, this field would have a value of ou=groups. Group subtree：This field is similar to the User subtree field described for User Element Mapping, but configures groups instead of users. If all groups are defined under the entry defined in Base DN, set the field to false. If a group can be defined in a tree of organizational units under the Base DN, set the field to true. Group object class：This value in this field is a standard object class defined in RFC-2307. The class is simply a collection of references to unique entries in an LDAP directory and can be used to associate user entries with a group. Examples are groupOfUniqueNames, posixGroup or custom values. Group ID attribute：Specifies the attribute of the object class that specifies the group identifier. If the value of this field corresponds to the ID of a role, members of this group will have the corresponding privileges. Group member attribute：Specifies the attribute of the object class which specifies a member of a group. An example value is uniqueMember. Group member format：This field captures the format of the Group Member Attribute, and is used by the repository manager to extract a username from this attribute. An example values is ${dn} . 3. 分配Nexus管理员的角色\"nx-admin\"给LDAP上的一个用户，作为nexus新的管理员。然后将admin用户禁用。 参考链接https://help.sonatype.com/repomanager3/security/ldap Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:02 "},"origin/ldap-Grafana对接LDAP.html":{"url":"origin/ldap-Grafana对接LDAP.html","title":"Grafana","keywords":"","body":"一、ContextOpenLDAP的条目组织形式 二、操作1、修改/etc/grafana/grafana.ini.............省略............. [auth.ldap] enabled = true config_file = /etc/grafana/ldap.toml allow_sign_up = true .............省略............. 2、修改/etc/grafana/ldap.toml.............省略............. # To troubleshoot and get more log info enable ldap debug logging in grafana.ini # [log] # filters = ldap:debug [[servers]] # Ldap server host (specify multiple hosts space separated) host = \"openldap-service.openldap.svc\" # Default port is 389 or 636 if use_ssl = true port = 389 # Set to true if ldap server supports TLS use_ssl = false # Set to true if connect ldap server with STARTTLS pattern (create connection in insecure, then upgrade to secure connection with TLS) start_tls = false # set to true if you want to skip ssl cert validation ssl_skip_verify = false # set to the path to your root CA certificate or leave unset to use system defaults # root_ca_cert = \"/path/to/certificate.crt\" # Authentication against LDAP servers requiring client certificates # client_cert = \"/path/to/client.crt\" # client_key = \"/path/to/client.key\" # Search user bind dn bind_dn = \"cn=admin,dc=curiouser,dc=com\" # Search user bind password，If the password contains # or ; you have to wrap it with triple quotes. Ex \"\"\"#password;\"\"\" bind_password = '*********' # User search filter, for example \"(cn=%s)\" or \"(sAMAccountName=%s)\" or \"(uid=%s)\" search_filter = \"(&(memberOf=cn=grafana,ou=applications,dc=curiouser,dc=com))\" # An array of base dns to search through search_base_dns = [\"ou=employee,dc=curiouser,dc=com\"] ## For Posix or LDAP setups that does not support member_of attribute you can define the below settings。Please check grafana LDAP docs for examples # group_search_filter = \"(&(objectClass=posixGroup)(memberUid=%s))\" # group_search_base_dns = [\"ou=groups,dc=grafana,dc=org\"] # group_search_filter_user_attribute = \"uid\" # Specify names of the ldap attributes your ldap uses [servers.attributes] name = \"sn\" username = \"cn\" member_of = \"memberOf\" email = \"mail\" # Map ldap groups to grafana org roles [[servers.group_mappings]] #group_dn = \"cn=admins,dc=grafana,dc=org\" #org_role = \"Admin\" # To make user an instance admin (Grafana Admin) uncomment line below # grafana_admin = true # The Grafana organization database id, optional, if left out the default org (id 1) will be used # org_id = 1 [[servers.group_mappings]] group_dn = \"cn=users,dc=grafana,dc=org\" org_role = \"Editor\" [[servers.group_mappings]] # If you want to match all (or no ldap groups) then you can use wildcard group_dn = \"*\" #org_role = \"Viewer\" .............省略............. Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:02 "},"origin/logging-日志系统技术概览简介.html":{"url":"origin/logging-日志系统技术概览简介.html","title":"日志系统技术概览简介","keywords":"","body":"日志系统概览简介Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:06 "},"origin/logging-日志系统数据在个组件中的流转格式.html":{"url":"origin/logging-日志系统数据在个组件中的流转格式.html","title":"日志系统数据在个组件中的流转格式","keywords":"","body":"原始日志2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com.curiouser.framework.common.aspect.ControllerAspect#around {\"business\":\"curiouser\",\"currentTime\":\"2019-09-24 09:12:39.052\",\"data\":\"{\"args\": {\"AuthQueryDTO\": {\"clientId\":\"ppush-platform\",\"clientSecret\":\"Jygv8V4TerC5rDxO\"},},\"result\": {\"expireTime\":-1,\"token\":\"77ff1cd2d1985b6d2d99bd54453bbc5f\",\"type\":\"1\"}}\",\"datatype\":0,\"interface1\":\"com.curiouser.auth.center.controller.ClientApiController\",\"level\":\"INFO\",\"method\":\"serverAuth\",\"module\":\"curiouser-auth-center\",\"reqTime\":8,\"requestId\":\"req-bf1bcc406dfa4d35b9062e06fbad78cd\",\"thread\":\"XNIO-1 task-14\",\"urlPath\":\"/client/server/token\"} This is a test log ! hahaha {\"datatype\":0,\"business\":\"alert\",\"module\":\"alert-rule\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"level\":\"WARN \",\"method\":\"isConnectionAlive\",\"thread\":\"XNIO-1 task-20\",\"requestId\":\"req-498fe711243b444e9b73ed6d5dc20a20\",\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\"} 经过\"原始日志+Filebeat\"处理过的日志{\"@timestamp\":\"2019-09-24T11:02:47.692Z\",\"@metadata\":{\"beat\":\"filebeat\",\"type\":\"_doc\",\"version\":\"7.2.0\"},\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"log\":{\"offset\":0,\"file\":{\"path\":\"/root/logs/test.log\"}},\"message\":\"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com.curiouser.framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouser\\\",\\\"currentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.center.controller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"-auth-center\\\",\\\"reqTime\\\":8,\\\"requestId\\\":\\\"req-bf1bcc406dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\",\"fields\":{\"ENV\":\"dev\",\"CANARY\":\"sit0\",\"TEMPLATE\":2019082110,\"NAMESPACE\":\"test\",\"PROJECTNAME\":\"test\",\"CLUSTER\":\"cluster_dev\"}} {\"@timestamp\":\"2019-09-24T11:02:47.692Z\",\"@metadata\":{\"beat\":\"filebeat\",\"type\":\"_doc\",\"version\":\"7.2.0\"},\"log\":{\"offset\":645,\"file\":{\"path\":\"/root/logs/test.log\"}},\"message\":\"This is a test log ! hahaha\",\"fields\":{\"CLUSTER\":\"cluster_dev\",\"ENV\":\"dev\",\"CANARY\":\"sit0\",\"TEMPLATE\":2019082110,\"NAMESPACE\":\"test\",\"PROJECTNAME\":\"test\"},\"host\":{\"name\":\"allinone.tools.curiouser.com\"}} {\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"message\":\"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\",\"@timestamp\":\"2019-09-24T12:58:49.062Z\",\"log\":{\"offset\":675,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"@version\":\"1\",\"fields\":{\"PROJECTNAME\":\"test\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"TEMPLATE\":2019082110,\"ENV\":\"dev\",\"CLUSTER\":\"cluster_dev\"},\"data\":{\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"method\":\"isConnectionAlive\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"thread\":\"XNIO-1 task-20\",\"module\":\"alert-rule\",\"level\":\"WARN \",\"business\":\"alert\",\"requestId\":\"req2fe711243b444e9b73ed6d5dc20a20\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\",\"datatype\":0}} 经过\"原始日志+Filebeat--->logstash-producer\"流程处理过的日志（内容没变，字段顺序变了，消息顺序变了）{\"fields\":{\"PROJECTNAME\":\"test\",\"TEMPLATE\":2019082110,\"NAMESPACE\":\"test\",\"CLUSTER\":\"cluster_dev\",\"ENV\":\"dev\",\"CANARY\":\"sit0\"},\"message\":\"This is a test log ! hahaha\",\"@version\":\"1\",\"@timestamp\":\"2019-09-24T11:24:50.336Z\",\"log\":{\"offset\":645,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"host\":{\"name\":\"allinone.tools.curiouser.com\"}} {\"fields\":{\"PROJECTNAME\":\"test\",\"TEMPLATE\":2019082110,\"NAMESPACE\":\"test\",\"CLUSTER\":\"cluster_dev\",\"ENV\":\"dev\",\"CANARY\":\"sit0\"},\"message\":\"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com..framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouserentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.cenoller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"curiouser-auth-center\\\",\\\"r8,\\\"requestId\\\":\\\"req-bf1bcc406dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\",\"@version\":\"1\",\"@timestamp\":\"2019-09-24T11:24:50.332Z\",\"log\":{\"offset\":0,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"host\":{\"name\":\"allinone.tools.curiouser.com\"}} {\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"message\":\"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\",\"@timestamp\":\"2019-09-24T12:58:49.062Z\",\"log\":{\"offset\":675,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"@version\":\"1\",\"fields\":{\"PROJECTNAME\":\"test\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"TEMPLATE\":2019082110,\"ENV\":\"dev\",\"CLUSTER\":\"cluster_dev\"},\"data\":{\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"method\":\"isConnectionAlive\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"thread\":\"XNIO-1 task-20\",\"module\":\"alert-rule\",\"level\":\"WARN \",\"business\":\"alert\",\"requestId\":\"reqfe711243b444e9b73ed6d5dc20a20\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\",\"datatype\":0}} 经过\"原始日志+Filebeat--->logstash-producer--->Kafka \"流程处理过的日志(内容没变，字段顺序变了，消息顺序变了){\"message\":\"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com..framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouserentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.cencuriouseroller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"curiouser-auth-cencuriousereqTime\\\":8,\\\"requestId\\\":\\\"req-bf1bcc406dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\",\"@timestamp\":\"2019-09-24T11:34:29.741Z\",\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"@version\":\"1\",\"fields\":{\"CANARY\":\"sit0\",\"ENV\":\"dev\",\"TEMPLATE\":2019082110,\"PROJECTNAME\":\"test\",\"NAMESPACE\":\"test\",\"CLUSTER\":\"cluster_dev\"},\"log\":{\"file\":{\"path\":\"/root/logs/test.log\"},\"offset\":0},\"tags\":[\"beats_input_codec_plain_applied\"]} {\"message\":\"This is a test log ! hahaha\",\"@timestamp\":\"2019-09-24T11:34:29.741Z\",\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"@version\":\"1\",\"fields\":{\"CANARY\":\"sit0\",\"ENV\":\"dev\",\"TEMPLATE\":2019082110,\"PROJECTNAME\":\"test\",\"NAMESPACE\":\"test\",\"CLUSTER\":\"cluster_dev\"},\"log\":{\"file\":{\"path\":\"/root/logs/test.log\"},\"offset\":645},\"tags\":[\"beats_input_codec_plain_applied\"]} {\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"message\":\"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\",\"@timestamp\":\"2019-09-24T12:58:49.062Z\",\"log\":{\"offset\":675,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"@version\":\"1\",\"fields\":{\"PROJECTNAME\":\"test\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"TEMPLATE\":2019082110,\"ENV\":\"dev\",\"CLUSTER\":\"cluster_dev\"},\"data\":{\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"method\":\"isConnectionAlive\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"thread\":\"XNIO-1 task-20\",\"module\":\"alert-rule\",\"level\":\"WARN \",\"business\":\"alert\",\"requestId\":\"reqfe711243b444e9b73ed6d5dc20a20\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\",\"datatype\":0}} 经过\"原始日志+Filebeat--->logstash-producer--->Kafka--->logstash-consumer\"流程处理过的日志(内容没变，字段顺序变了，消息顺序变了){\"@version\":\"1\",\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"log\":{\"offset\":0,\"file\":{\"path\":\"/root/logs/test.log\"}},\"@timestamp\":\"2019-09-24T11:34:29.741Z\",\"fields\":{\"PROJECTNAME\":\"test\",\"TEMPLATE\":2019082110,\"CLUSTER\":\"cluster_dev\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"ENV\":\"dev\"},\"message\":\"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com.curiouser.framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouser\\\",\\\"currentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.center.controller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"curiouser-auth-center\\\",\\\"reqTime\\\":8,\\\"requestId\\\":\\\"req-bf1bcc406dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\",\"tags\":[\"beats_input_codec_plain_applied\"]} {\"@version\":\"1\",\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"log\":{\"offset\":645,\"file\":{\"path\":\"/root/logs/test.log\"}},\"@timestamp\":\"2019-09-24T11:34:29.741Z\",\"fields\":{\"PROJECTNAME\":\"test\",\"TEMPLATE\":2019082110,\"CLUSTER\":\"cluster_dev\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"ENV\":\"dev\"},\"message\":\"This is a test log ! hahaha\",\"tags\":[\"beats_input_codec_plain_applied\"]} {\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"message\":\"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\",\"@timestamp\":\"2019-09-24T12:58:49.062Z\",\"log\":{\"offset\":675,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"@version\":\"1\",\"fields\":{\"PROJECTNAME\":\"test\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"TEMPLATE\":2019082110,\"ENV\":\"dev\",\"CLUSTER\":\"cluster_dev\"},\"data\":{\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"method\":\"isConnectionAlive\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"thread\":\"XNIO-1 task-20\",\"module\":\"alert-rule\",\"level\":\"WARN \",\"business\":\"alert\",\"requestId\":\"reqfe711243b444e9b73ed6d5dc20a20\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\",\"datatype\":0}} 经过\"原始日志+Filebeat--->logstash-producer--->Kafka--->logstash-consumer--->elasticsearch\"流程处理过的日志（将logstash-consumer发送的日志数据放在”_source“字段下，同时） # =========================================================================== { \"_index\": \"test-test-2019.09.24\", \"_type\": \"_doc\", \"_id\": \"s_ZaY20BY8hT6jLicmK4\", \"_version\": 1, \"_score\": null, \"_source\": { \"message\": \"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\", \"log\": { \"file\": { \"path\": \"/root/logs/test.log\" }, \"offset\": 675 }, \"@version\": \"1\", \"data\": { \"business\": \"alert\", \"interface\": \"com.zaxxer.hikari.pool.PoolBase\", \"method\": \"isConnectionAlive\", \"requestId\": \"reqfe711243b444e9b73ed6d5dc20a20\", \"data\": \"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\", \"datatype\": 0, \"thread\": \"XNIO-1 task-20\", \"level\": \"WARN \", \"module\": \"alert-rule\", \"currentTime\": \"2019-09-24 20:50:00,056\" }, \"tags\": [ \"beats_input_codec_plain_applied\" ], \"@timestamp\": \"2019-09-24T12:58:49.062Z\", \"host\": { \"name\": \"allinone.tools.curiouser.com\" }, \"fields\": { \"PROJECTNAME\": \"test\", \"CANARY\": \"sit0\", \"NAMESPACE\": \"test\", \"TEMPLATE\": 2019082110, \"ENV\": \"dev\", \"CLUSTER\": \"cluster_dev\" } }, \"fields\": { \"@timestamp\": [ \"2019-09-24T12:58:49.062Z\" ] }, \"sort\": [ 1569329929062 ] } # =========================================================================== { \"_index\": \"test-test-2019.09.24\", \"_type\": \"_doc\", \"_id\": \"1fUgY20BY8hT6jLiJAfF\", \"_version\": 1, \"_score\": null, \"_source\": { \"message\": \"This is a test log ! hahaha\", \"tags\": [ \"beats_input_codec_plain_applied\" ], \"host\": { \"name\": \"allinone.tools.curiouser.com\" }, \"@timestamp\": \"2019-09-24T11:34:29.741Z\", \"fields\": { \"CANARY\": \"sit0\", \"NAMESPACE\": \"test\", \"CLUSTER\": \"cluster_dev\", \"TEMPLATE\": 2019082110, \"ENV\": \"dev\", \"PROJECTNAME\": \"test\" }, \"@version\": \"1\", \"log\": { \"file\": { \"path\": \"/root/logs/test.log\" }, \"offset\": 645 } }, \"fields\": { \"@timestamp\": [ \"2019-09-24T11:34:29.741Z\" ] }, \"sort\": [ 1569324869741 ] } # =========================================================================== { \"_index\": \"test-test-2019.09.24\", \"_type\": \"_doc\", \"_id\": \"DOlaY20B_ehr23pid9GM\", \"_version\": 1, \"_score\": null, \"_source\": { \"message\": \"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com.curiouser.framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouser\\\",\\\"currentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.center.controller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"curiouser-auth-center\\\",\\\"reqTime\\\":8,\\\"requestId\\\":\\\"req-bf106dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\", \"log\": { \"file\": { \"path\": \"/root/logs/test.log\" }, \"offset\": 0 }, \"@version\": \"1\", \"tags\": [ \"beats_input_codec_plain_applied\" ], \"@timestamp\": \"2019-09-24T12:58:49.062Z\", \"host\": { \"name\": \"allinone.tools.curiouser.com\" }, \"fields\": { \"PROJECTNAME\": \"test\", \"CANARY\": \"sit0\", \"NAMESPACE\": \"test\", \"ENV\": \"dev\", \"TEMPLATE\": 2019082110, \"CLUSTER\": \"cluster_dev\" } }, \"fields\": { \"@timestamp\": [ \"2019-09-24T12:58:49.062Z\" ] }, \"sort\": [ 1569329929062 ] } 附录1、filebeat配置 filebeat.inputs: - type: log enabled: true paths: - /root/logs/test.log exclude_files: [\"/root/logs/_filebeat\", \".gz$\"] recursive_glob.enabled: true setup.template.settings: index.number_of_shards: 3 processors: - decode_json_fields: fields: [\"message\"] process_array: false max_depth: 1 target: \"data\" overwrite_keys: false - drop_fields: fields: [\"agent\", \"tags\", \"input\", \"ecs\"] fields: NAMESPACE: \"test\" PROJECTNAME: \"test\" CLUSTER: cluster_dev ENV: dev CANARY: sit0 TEMPLATE: 2019082110 output.logstash: hosts: [\"localhost:5044\"] #output.file: # path: \"/root/logs/output\" # filename: filebeat.log 2、logstash_producer配置 input { beats { id => \"logstash_producer_input_beats\" port => 5044 } } output { #file{ # path => \"/root/logs/output/logstah-producer.log\" #} kafka { id => \"logstash_producer_output_kafka\" codec => json topic_id => \"logs\" bootstrap_servers => \"localhost:9092\" compression_type => \"snappy\" } } 3、logstash_consumer配置 input { kafka { id => \"logstash_consumer_input_kafka\" bootstrap_servers => \"localhost:9092\" topics => \"logs\" group_id => \"applications_logs_group\" codec => \"json\" auto_offset_reset => \"earliest\" } } output { #file{ # path => \"/root/logs/output/logstah-consumer.log\" #} elasticsearch { id => \"logstash_consumer_output_elasticsearch\" hosts => [\"localhost:9092\"] index=>\"%{[fields][NAMESPACE]}-%{[fields][PROJECTNAME]}-%{+YYYY.MM.dd}\" document_type => \"_doc\" http_compression => true user => \"logstash-pipeline\" password => \"logstash-pipeline\" } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:06 "},"origin/logging-kafka基础知识.html":{"url":"origin/logging-kafka基础知识.html","title":"基础知识","keywords":"","body":"一、Kafka 与 Zookeeper1、Zookeeper在Kafka集群分布式消息中的作用1.1、选举ControllerKafka是高可用的分布式消息系统，首先要解决的就是资源协调分配和多副本状态维护的问题。解决这些问题通常就是两种思路，一是依靠Zookeeper来协调，二是设定一个中心节点，让这个中心节点来协调。如果依靠Zookeeper来协调，会存在大量的竞争条件，对Zookeeper的访问压力增大，而且如果Zookeeper出现了问题（比如网络抖动），系统很容易出现紊乱。Kafka采用的是第二种思路，即选举一个中心节点来进行资源协调与多副本状态维护，这个中心节点被称作Controller（一个特殊的Broker），这个选举过程依靠Zookeeper来完成。 Broker启动时，会竞争创建临时\"/controller\"。如果创建成功，则成为Controller，并把Broker的id等信息写入这个节点。同时会全程监控\"/controller\"的数据变化，如果旧的Controller挂掉，则开启新一轮的竞争过程。1.2、注册BrokerKafka要进行资源协调，第一件需要知道的事情就是各个Broker的存活状态，这个问题利用Zookeeper可以很容易做到。 假设某个Broker，id为0，它启动时，会创建\"/brokers/ids/0\"临时节点，并把端口等信息写进去。Controller会监控\"/brokers/ids\"的节点变化，以实时感知各broker的状态，进行资源协调。1.3、协调topic的创建、调整与销毁在Kafka这个多副本分区的消息系统里，创建一个topic，至少需要以下3个步骤：持久化topic的多副本分区信息 为每个分区挑选一个副本leader 将上述信息发送给对应的Broker，以完成实际的日志文件创建过程 Controller的存在，可以很容易完成上面的b和c步骤，但a步骤不行，如果Controller挂掉，则这些信息会不可用。Kafka把这些信息保存在Zookeeper中，依靠其高可用特性来保证这些信息的高可用。假设某个topic名字为mytopic，创建时，其分区信息保存在\"/brokers/topics/mytopic\"中。Controller全程监控\"/brokers/topics\"的孩子节点变动，实时感知这些信息，以完成后续步骤。创建完成之后，后续往往会有分区调整和topic删除等需求。普通青年可能会觉得这两个问题很简单，给Controller发个相关请求就可以了。事实远非如此！拿分区调整来说，假设某分区有三个副本，分别位于Broker-1、Broker-2和Broker-3，leader为1，现在扩容增加了Broker-4、Broker-5、Broker-6，为了平衡机器间压力，需要将副本1 2 3移到4 5 6，至少经历以下步骤：修改该分区的副本信息为1 2 3 4 5 6，leader为1 等待4 5 6副本追赶1 2 3的进度直至大家都同步(in sync) 从4 5 6中挑选一个新的副本leader，假设为4 修改该分区的副本信息为4 5 6，leader为4 以上每个步骤都有可能失败，如何才能保证这次调整顺利进行呢？首先，我们不能直接修改该分区的副本信息为 4 5 6，原因很简单，需要等待4 5 6的追赶过程以便产生新leader。其次，操作未完全成功的命令需要保存下来，如果操作过程中，Controller挂掉，则新的Controller可以从头开始直至成功。Kafka怎么做的呢？通常是Admin控制台）把调整命令写入\"/admin/reassign_partitions\"节点 Controller监控\"/admin/reassign_partitions\"，拿到调整命令，执行上述步骤 如果操作成功则删除该节点；如果Controller挂掉，新的Controller还会拿到这个命令并从头开始执行 当然，这里一次只能有一个调整命令，但一个调整命令可以同时调整多个topic的多个分区。 在这个过程中，Zookeeper的作用是：持久化操作命令并实时通知操作者，是不是只有Zookeeper可以做这个事情呢，不是，但Zookeeper可以做得很好，保证命令高可用。 类似的操作还有topic删除，副本的leader变更等，都是沿用上面的套路。1.4. 保存topic级别和client级别的配置信息Broker的集群中有全局配置信息，但如果想针对某个topic或者某个client进行配置呢，Kafka把这些信息保存在Zookeeper中，各个Broker实时监控以更新。1.5、脑裂问题脑裂问题是指，在一个设有中心节点的系统中，出现了两个中心节点。两个中心同时传达命令，自然会造成系统的紊乱。Kafka利用Zookeeper所做的第一件也是至关重要的一件事情是选举Controller，那么自然就有疑问，有没有可能产生两个Controller呢？首先，Zookeeper也是有leader的，它有没有可能产生两个leader呢？答案是不会。 quorum机制可以保证，不可能同时存在两个leader获得大多数支持。假设某个leader假死，其余的followers选举出了一个新的leader。这时，旧的leader复活并且仍然认为自己是leader，这个时候它向其他followers发出写请求也是会被拒绝的。因为每当新leader产生时，会生成一个epoch，这个epoch是递增的，followers如果确认了新的leader存在，知道其epoch，就会拒绝epoch小于现任leader epoch的所有请求。那有没有follower不知道新的leader存在呢，有可能，但肯定不是大多数，否则新leader无法产生。Zookeeper的写也遵循quorum机制，因此，得不到大多数支持的写是无效的，旧leader即使各种认为自己是leader，依然没有什么作用。Kafka的Controller也采用了epoch，具体机制如下:所有Broker监控\"/controller\"，节点被删除则开启新一轮选举，节点变化则获取新的epoch Controller会注册SessionExpiredListener，一旦因为网络问题导致Session失效，则自动丧失Controller身份，重新参与选举 收到Controller的请求，如果其epoch小于现在已知的controller_epoch，则直接拒绝 理论上来说，如果Controller的SessionExpired处理成功，则可以避免双leader，但假设SessionExpire处理意外失效的情况：旧Controller假死，新的Controller创建。旧Controller复活，SessionExpired处理意外失效，仍然认为自己是leader。 这时虽然有两个leader，但没有关系，leader只会发信息给存活的broker（仍然与Zookeeper在Session内的），而这些存活的broker则肯定能感知到新leader的存在，旧leader的请求会被拒绝。1.6、如果Zookeeper挂了会怎样每个Broker有一个metaDataCache，缓存有topic和partition的基本信息，可以正常的生产和消费信息，但不能进行topic的创建、调整和删除等操作。 此外，Broker会不断重试连接。1.7、Zookeeper用量估计假设Broker数目为B，topic数目为T，所有topic总partition数目为P，Client数目为C，以下数值均为峰值：qps: 100以内 连接数: B watcher数目：3 * B + 2 * T + 6 Zookeeper节点数（叶子节点）: B + P + T + C + 8 2、kafka注册到zookeeper中的数据存储结构Zookeeper路径的创建者与监听者 路径 创建者 监听者 类型 /controller 各个broker竞争创建 所有broker全程监控data change 临时节点 /controller_epoch controller 无 永久节点 /brokers/ids broker启动时检查并确保存在 controller全程监控child change 永久节点 /brokers/ids/{id} id对应的broker 无 临时节点 /brokers/topics broker启动时检查确保存在 controller全程监控child change 永久节点 /brokers/topics/{topic} controller收到创建请求，或者broker启用自动创建topic时，或admin工具 controller全程监控data change 永久节点 /brokers/topics/{topic}/{partition}/state partiton的leader partition reassign时，controller临时监控data change 永久节点 /config/changes broker启动时检查并确保存在 所有broker全程监控child change 永久节点 /config/topics broker启动时检查并确保存在 无 永久节点 /config/clients broker启动时检查并确保存在 无 永久节点 /brokers/seqid broker启动时检查并确保存在 待确认 永久节点 /admin/delete_topics broker启动时检查并确保存在 controller全程监控child change 永久节点 /isr_change_notification broker启动时检查并确保存在 controller全程监控child change 永久节点 /admin/reassign_partitions admin 工具 controller全程监控data change 永久节点，reassign结束后会删除 /admin/preferred_replica_election admin 工具 controller全程监控data change 永久节点，replica election结束后会删除 2.1、Topic注册信息/brokers/topics/[topic]存储某个topic的partitions所有分配信息{ \"version\": \"版本编号目前固定为数字1\", \"partitions\": { \"partitionId编号\": [ 同步副本组brokerId列表 ], \"partitionId编号\": [ 同步副本组brokerId列表 ], ....... } } 2.2、Partition状态信息/brokers/topics/[topic]/partitions/[partition-Id]/stateSchema: { \"controller_epoch\": 表示kafka集群中的中央控制器选举次数, \"leader\": 表示该partition选举leader的brokerId, \"version\": 版本编号默认为1, \"leader_epoch\": 该partition leader选举次数, \"isr\": [同步副本组brokerId列表] } 2.3、Broker注册信息/brokers/ids/[0...N]每个broker的配置文件中都需要指定一个数字类型的id(全局不可重复),此节点为临时znode(EPHEMERAL) Schema: { \"jmx_port\": jmx端口号, \"timestamp\": kafka broker初始启动时的时间戳, \"host\": 主机名或ip地址, \"version\": 版本编号默认为1, \"port\": kafka broker的服务端端口号,由server.properties中参数port确定 } 2.4、Controller epoch/controller_epoch -> int (epoch)此值为一个数字,kafka集群中第一个broker第一次启动时为1，以后只要集群中center controller中央控制器所在broker变更或挂掉，就会重新选举新的center controller，每次center controller变更controller_epoch值就会 + 12.5、Controller注册信息/controller -> int (broker id of the controller)存储center controller中央控制器所在kafka broker的信息{ \"version\": 版本编号默认为1, \"brokerid\": kafka集群中broker唯一编号, \"timestamp\": kafka broker中央控制器变更时的时间戳 } 2.6、Consumer注册信息/consumers/[groupId]/ids/[consumerIdString]每个consumer都有一个唯一的ID(consumerId可以通过配置文件指定,也可以由系统生成),此id用来标记消费者信息.这是一个临时的znode,此节点的值为请看consumerIdString产生规则,即表示此consumer目前所消费的topic + partitions列表.Schema: { \"version\": 版本编号默认为1, \"subscription\": { //订阅topic列表 \"topic名称\": consumer中topic消费者线程数 }, \"pattern\": \"static\", \"timestamp\": \"consumer启动时的时间戳\" } 2.7、Consumer offset/consumers/[groupId]/offsets/[topic]/[partitionId] -> long (offset)用来跟踪每个consumer目前所消费的partition中最大的offset.此znode为持久节点,可以看出offset跟group_id有关,以表明当消费者组(consumer group)中一个消费者失效,重新触发balance,其他consumer可以继续消费2.8、admin管理信息二、Kafka中的消费者与消费者组1、消费者组里面的消费者消费Topic Partition的消息时流程每个consumer客户端被创建时,会向zookeeper注册自己的信息.主要是为了\"负载均衡\" 同一个Consumer Group中的Consumers，Kafka将相应Topic中的每个消息只发送给其中一个Consumer。 Consumer Group中的每个Consumer读取Topic的一个或多个Partitions，并且是唯一的Consumer； 一个Consumer group的多个consumer的所有线程依次有序地消费一个topic的所有partitions,如果Consumer group中所有consumer总线程大于partitions数量，则会出现空闲情况举例说明： kafka集群中创建一个topic为report-log，4个partitions 索引编号为0,1,2,3。假如有目前有三个消费者node（注意：一个consumer中一个消费线程可以消费一个或多个partition） 如果每个consumer创建一个consumer thread线程,各个node消费情况如下，node1消费索引编号为0,1分区，node2费索引编号为2,node3费索引编号为3 如果每个consumer创建2个consumer thread线程，各个node消费情况如下(是从consumer node先后启动状态来确定的)，node1消费索引编号为0,1分区；node2费索引编号为2,3；node3为空闲状态 总结：从以上可知，Consumer Group中各个consumer是根据先后启动的顺序有序消费一个topic的所有partitions的。如果Consumer Group中所有consumer的总线程数大于partitions数量，则可能consumer thread或consumer会出现空闲状态。 2、Consumer均衡算法当一个group中,有consumer加入或者离开时,会触发partitions均衡(均衡的最终目的,是提升topic的并发消费能力)假如topic1,具有如下partitions: P0,P1,P2,P3 加入group中,有如下consumer: C0,C1 首先根据partition索引号对partitions进行排序，假设排序: P0,P1,P2,P3 根据(consumer.id + '-'+ thread序号)对消费者进行排序,假设排序: C0,C1 计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整) 然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i M),P((i + 1) M -1)] 3、Consumer启动流程首先进行\"Consumer Id注册\"; 然后在\"Consumer id 注册\"节点下注册一个watch用来监听当前group中其他consumer的\"退出\"和\"加入\";只要此znode path下节点列表变更,都会触发此group下consumer的负载均衡.(比如一个consumer失效,那么其他consumer接管partitions). 在\"Broker id 注册\"节点下,注册一个watch用来监听broker的存活情况;如果broker列表变更,将会触发所有的groups下的consumer重新balance. 参考链接http://blog.csdn.net/lizhitao/article/details/23744675 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:06 "},"origin/logging-kafka常用操作.html":{"url":"origin/logging-kafka常用操作.html","title":"kafka常用操作","keywords":"","body":"Apache Kafka常用操作一、Topic管理1. 列出所有Topickafka-topics.sh --zookeeper 127.0.0.1:2181 --list kafka-topics.sh --bootstrap-server 127.0.0.1:9092 --list 2. 创建一个topickafka-topics.sh --create --zookeeper 127.0.0.1:2181 --replication-factor 2 --partitions 3 --topic Test #--replication-factor参数指定Topic的数据副本个数 #--partitions参数指定Topic的分区个数 3. 删除Topickafka-topics.sh --delete --zookeeper 127.0.0.1:2181 --topic Test 或者 #只会删除zookeeper中的元数据，消息文件须手动删除 kafka-run-class.sh kafka.admin.DeleteTopicCommand --zookeeper 172.16.3.12:2181/kafka/q-35aw0fye --topic Test 4. 查看Topic的详细信息kafka-topics.sh --describe --zookeeper 127.0.0.1:2181 --topic Test Topic:Test PartitionCount:2 ReplicationFactor:1 Configs: Topic: Test Partition: 0 Leader: 2 Replicas: 2 Isr: 2 Topic: Test Partition: 1 Leader: 3 Replicas: 3 Isr: 3 #第一行，列出了topic的名称，分区数(PartitionCount),副本数(ReplicationFactor)以及其他的配置(Config.s) #Leader:1 表示为做为读写的broker的节点编号 #Replicas:表示该topic的每个分区在那些borker中保存 #Isr:表示当前有效的broker, Isr是Replicas的子集 5. 增加Topic分区个数（只能增加扩容）kafka-topics.sh --zookeeper 127.0.0.1:2181 --alter --topic Test --partitions 2 6. 给Topic增加配置kafka-topics.sh --zookeeper 127.0.0.1:2181 --alter --topic Test --config flush.messages=1 7. 删除Topic的配置kafka-topics.sh --zookeeper 127.0.0.1:2181 --alter --topic Test --delete-config flush.messages=1 8. 查看消费者组kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --list 9. 查看Topic各个分区的消息偏移量最大（小）值kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list 127.0.0.1:9092 --time -1 --topic Test # time为-1时表示最大值，time为-2时表示最小值 10. 查看Topic中指定consumer组内消息消费的offsetkafka的offset保存位置分为两种情况 0.9.0.0版本之前默认保存在zookeeper当中 ，0.9.0.0版本之后保存在broker对应的topic当中kafka-consumer-offset-checker.sh --zookeeper 127.0.0.1:2181 --group logstash-group --topic Test GROUP TOPIC PID OFFSET LOGSIZE LAG Ower 消费者组 话题id 分区id 当前已消费的条数 总条数 未消费的条数 所有者 console-consumer-98995 Test 0 112 318084 317972 none console-consumer-98995 Test 1 -1 318088 unknown none 方式二：$ kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --describe --offsets --group Group-Name TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID basic_log_k8s 1 127509 333334 205825 logstash-0-490058d7-154f-4111-b514-57de254ecae8 /192.168.3.72 logstash-0 basic_log_k8s 0 127317 333333 206016 logstash-0-2bd85bcc-282e-41a0-a9c3-6d0dbefd547f /192.168.0.40 logstash-0 11. 修改指定消费者分组对应topic的offset第一种情况offset信息保存在topic中$ bin/kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --group test-consumer-group --topic test --execute --reset-offsets --to-offset 10000 #参数解析： #--bootstrap-server 代表你的kafka集群 你的offset保存在topic中 #--group 代表你的消费者分组 #--topic 代表你消费的主题 #--execute 代表支持复位偏移 #--reset-offsets 代表要进行偏移操作 #--to-offset 代表你要偏移到哪个位置 是long类型数值，只能比前面查询出来的小 #还有其他的--to- ** 方式可以自己验证 本人验证过--to-datetime 没有成功 第二种方式offset信息保存在zookeeper当中$ bin/kafka-consumer-groups.sh --zookeeper kafka_zk1:2181 --group test-consumer-group --topic test --execute --reset-offsets --to-offset 10000 12. 修改topic副本因子数官方文档：https://kafka.apache.org/21/documentation.html#replication① 先查看Topic的信息$ kafka-topics.sh --zookeeper localhost:2181 --topic test --describe Topic:test PartitionCount:2 ReplicationFactor:1 Configs: Topic: test Partition: 0 Leader: 2 Replicas: 2 Isr: 2 Topic: test Partition: 1 Leader: 3 Replicas: 3 Isr: 3 ② 准备JSON文件{ \"version\": 1, \"partitions\": [ { \"topic\": \"test\", \"partition\": 0, \"replicas\": [2, 1, 3] }, { \"topic\": \"test\", \"partition\": 1, \"replicas\": [3, 2, 1] }] } ③ kafka-reassign-partitions命令增加topic分区副本数$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file replication.json --execute Current partition replica assignment {\"version\":1,\"partitions\":[{\"topic\":\"test\",\"partition\":0,\"replicas\":[2]},{\"topic\":\"test\",\"partition\":1,\"replicas\":[3]}]} Save this to use as the --reassignment-json-file option during rollback Successfully started reassignment of partitions {\"version\":1,\"partitions\":[{\"topic\":\"test\",\"partition\":0,\"replicas\":[2,1,3]},{\"topic\":\"test\",\"partition\":1,\"replicas\":[3,2,1]}]} ④ 使用verify参数来检查副本数据是否复制分配完成$ kafka-reassign-partitions.sh --zookeeper www.iteblog.com:2181 --reassignment-json-file replication.json --verify Status of partition reassignment: Reassignment of partition [test,0] is still in progress Reassignment of partition [test,1] is still in progress $ kafka-reassign-partitions.sh --zookeeper www.iteblog.com:2181 --reassignment-json-file replication.json --verify Status of partition reassignment: Reassignment of partition [test,0] completed successfully Reassignment of partition [test,1] completed successfully $ kafka-topics.sh --zookeeper localhost:2181 --topic test --describe Topic:iteblog PartitionCount:2 ReplicationFactor:3 Configs: Topic: test Partition: 0 Leader: 2 Replicas: 2,1,3 Isr: 2,1,3 Topic: test Partition: 1 Leader: 3 Replicas: 3,2,1 Isr: 3,2,1 13. 均衡Topic分区到新增Broker节点重新分配官方文档地址：http://kafka.apache.org/documentation/#basic_ops_cluster_expansion翻译官方文档中文地址：http://orchome.com/36参考文章：https://blog.csdn.net/forrest_ou/article/details/79141391① 确定要重启分配分区的主题，新建topics-to-move.json json文件{ \"topics\": [ {\"topic\": \"foo1\"}, {\"topic\": \"foo2\"} ], \"version\":1 } // foo1 foo2 为要重新分配的主题 ② 使用 bin/kafka-reassign-partitions.sh重新分配工具生成分配规则的json语句分配到 5，6机器kafka-reassign-partitions.sh --zookeeper localhost:2181 --topics-to-move-json-file topics-to-move.json --broker-list \"5,6\" –generate ③ 有分配规则的json语句输出到控制台，复制到新建的json文件expand-cluster-reassignment.json中，例如：{\"version\":1, \"partitions\":[{\"topic\":\"foo1\",\"partition\":0,\"replicas\":[5,6]}, {\"topic\":\"foo1\",\"partition\":1,\"replicas\":[5,6]}, {\"topic\":\"foo1\",\"partition\":2,\"replicas\":[5,6]}, {\"topic\":\"foo2\",\"partition\":0,\"replicas\":[5,6]}, {\"topic\":\"foo2\",\"partition\":1,\"replicas\":[5,6]}, {\"topic\":\"foo2\",\"partition\":2,\"replicas\":[5,6]}] } //描述分配之后分区的分布情况 ④ 执行命令，开始分区重新分配kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json –execute ⑤ 验证是否完成kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json –verify //当输出全部都是completed successfully表明移动已经完成. 注意kafka新建主题时的分区分配策略：随机选取第一个分区节点，然后往后依次增加。例如第一个分区选取为1，第二个分区就 是2，第三个分区就是3. 1，2，3是brokerid。不会负载均衡，所以要手动重新分配分区操作，尽量均衡。 在生产的同时进行数据迁移会出现重复数据。所以迁移的时候避免重复生产数据，应该停止迁移主题的生产。同时消费不会，同时消费之后出现短暂的leader报错，会自动恢复。 新增了broker节点，如果有主题的分区在新增加的节点上，生产和消费的客户端都应该在hosts配置文件中增加新增的broker节点，否则无法生产消费，但是也不报错。 可以不需要第一步和第二步，自己手动新建分配的json文件 14. 查询Topic不可用的分区kafka-topics.sh --describe --unavailable-partitions --zookeeper localhost:2181 二、其他1. 自带测试生产者kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic Test 2. 自带测试消费者kafka-console-consumer.sh --zookeeper 127.0.0.1:2181 --from-beginning --topic Test 3. 自带性能测试位于bin/kafka-producer-perf-test.sh.主要参数有以下:messages 生产者发送总的消息数量 message-size 每条消息大小（单位为b） batch-size 每次批量发送消息的数量 topics 生产者发送的topic threads 生产者使用几个线程同时发送 例如 kafka-producer-perf-test.sh --messages 100000 --message-size 1000 --batch-size 10000 --topics test --threads 4 --broker-list 127.0.0.1:9092 start.time, end.time, compression, message.size, batch.size, total.data.sent.in.MB, MB.sec, total.data.sent.in.nMsg, nMsg.sec 2015-10-15 18:56:27:542, 2015-10-15 18:56:30:880, 0, 1000, 10000, 95.37, 28.5702, 100000, 29958.0587 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:06 "},"origin/filebeat-简介安装配置.html":{"url":"origin/filebeat-简介安装配置.html","title":"简介安装配置","keywords":"","body":"Filebeat的简介、安装、配置、Pipeline一. 简介Filebeat由两个主要组件组成：Inputs：负责管理harvester并找到所有要读取的文件来源。如果输入类型为日志，则查找器将查找路径匹配的所有文件，并为每个文件启动一个harvester。每个Inputs都在自己的Go协程中运行 每个prospector类型可以定义多次 Harvesters：一个harvester负责读取一个单个文件的内容，每个文件启动一个harvester。harvester逐行读取每个文件（一行一行地读取每个文件），并把这些内容发送到输出。在harvester正在读取文件内容的时候，文件被删除或者重命名了，那么Filebeat会续读这个文件。这就有一个问题了，就是只要负责这个文件的harvester没用关闭，那么磁盘空间就不会释放。默认情况下，Filebeat保存文件打开的状态直到close_inactive到达。 关闭harvester会产生以下结果： 如果在harvester仍在读取文件时文件被删除，则关闭文件句柄，释放底层资源。 文件的采集只会在scan_frequency过后重新开始 如果在harvester关闭的情况下移动或移除文件，则不会继续处理文件 二. 安装默认的安装文件路径 Type Description Default Location Config Option **home** Home of the Filebeat installation. `path.home` **bin** The location for the binary files. `{path.home}/bin` **config** The location for configuration files. `{path.home}` `path.config` **data** The location for persistent data files. `{path.home}/data` `path.data` **logs** The location for the logs created by Filebeat. `{path.home}/logs` `path.logs` YUM/RPM[elastic-7.x] name=Elastic repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md yum install filebeat-7.4.0 RPM下载地址：https://www.elastic.co/cn/downloads/beats/filebeatyum localinstall -y filebeat-7*.rpm 安装文件路径 Type Description Location **home** Home of the Filebeat installation. `/usr/share/filebeat` **bin** The location for the binary files. `/usr/share/filebeat/bin` **config** The location for configuration files. `/etc/filebeat` **data** The location for persistent data files. `/var/lib/filebeat` **logs** The location for the logs created by Filebeat. `/var/log/filebeat` 二进制文件zip, tar.gz, tgz 压缩格式的二进制安装包，下载地址：https://www.elastic.co/cn/downloads/beats/filebeat安装文件路径 Type Description Location **home** Home of the Filebeat installation. `{extract.path}` **bin** The location for the binary files. `{extract.path}` **config** The location for configuration files. `{extract.path}` **data** The location for persistent data files. `{extract.path}/data` **logs** The location for the logs created by Filebeat. `{extract.path}/logs` Filebeat命令行启动/usr/share/filebeat/bin/filebeat Commands SUBCOMMAND [FLAGS] Commands 描述 [`export`](https://www.elastic.co/guide/en/beats/filebeat/current/command-line-options.html#export-command) 导出配置到控制台，包括index template, ILM policy, dashboard [`help`](https://www.elastic.co/guide/en/beats/filebeat/current/command-line-options.html#help-command) 显示帮助文档 [`keystore`](https://www.elastic.co/guide/en/beats/filebeat/current/command-line-options.html#keystore-command) 管理[secrets keystore](https://www.elastic.co/guide/en/beats/filebeat/current/keystore.html). [`modules`](https://www.elastic.co/guide/en/beats/filebeat/current/command-line-options.html#modules-command) 管理配置Modules [`run`](https://www.elastic.co/guide/en/beats/filebeat/current/command-line-options.html#run-command) Runs Filebeat. This command is used by default if you start Filebeat without specifying a command. [`setup`](https://www.elastic.co/guide/en/beats/filebeat/current/command-line-options.html#setup-command) 设置初始环境。包括index template, ILM policy, write alias, Kibana dashboards (when available), machine learning jobs (when available). [`test`](https://www.elastic.co/guide/en/beats/filebeat/current/command-line-options.html#test-command) 测试配置文件 [`version`](https://www.elastic.co/guide/en/beats/filebeat/current/command-line-options.html#version-command) 显示版本信息 Global Flags 描述 `-E \"SETTING_NAME=VALUE\"` 覆盖配置文件中的配置项 `--M \"VAR_NAME=VALUE\"` 覆盖Module配置文件的中配置项 `-c FILE` 指定filebeat的配置文件路径。路径要相对于`path.config `-d SELECTORS` `-e` `--path.config` `--path.data` `--path.home` ` --path.logs` `--strict.perms` 示例：/usr/share/filebeat/bin/filebeat --modules mysql -M \"mysql.slowlog.var.paths=[/root/slow.log]\" -e /usr/share/filebeat/bin/filebeat -e -E output.console.pretty=true --modules mysql -M \"mysql.slowlog.var.paths=[\"/root/mysql-slow-sql-log/mysql-slowsql.log\"]\" -M \"mysql.error.enabled=false\" -E output.elasticsearch.enabled=false SystemD启动systemctl enable filebeat systemctl start filebeat systemctl stop filebeat systemctl status filebeat journalctl -u filebeat.service systemctl daemon-reload systemctl restart filebeat Filebeat的SystemD配置文件$ /usr/lib/systemd/system/filebeat.service [Unit] Description=Filebeat sends log files to Logstash or directly to Elasticsearch. Documentation=https://www.elastic.co/products/beats/filebeat Wants=network-online.target After=network-online.target [Service] Environment=\"BEAT_LOG_OPTS=-e\" Environment=\"BEAT_CONFIG_OPTS=-c /etc/filebeat/filebeat.yml\" Environment=\"BEAT_PATH_OPTS=-path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat -path.logs /var/log/filebeat\" ExecStart=/usr/share/filebeat/bin/filebeat $BEAT_LOG_OPTS $BEAT_CONFIG_OPTS $BEAT_PATH_OPTS Restart=always [Install] WantedBy=multi-user.target Variable Description Default value **BEAT_LOG_OPTS** Log options `-e` **BEAT_CONFIG_OPTS** Flags for configuration file path `-c /etc/filebeat/filebeat.yml` **BEAT_PATH_OPTS** Other paths `-path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat -path.logs /var/log/filebeat` 三. Docker镜像docker pull docker.elastic.co/beats/filebeat:7.4.0 docker pull filebeat:7.4.0 镜像中的安装文件路径 Type Description Location **home** Home of the Filebeat installation. `/usr/share/filebeat` **bin** The location for the binary files. `/usr/share/filebeat` **config** The location for configuration files. `/usr/share/filebeat` **data** The location for persistent data files. `/usr/share/filebeat/data` **logs** The location for the logs created by Filebeat. `/usr/share/filebeat/logs` Kubernetes部署默认部署到kube-system命名空间 部署类型是Daemonset，会部署到每一个Node上 每个Node上的/var/lib/docker/containers目录会挂载到filebeat容器中 默认Filebeat会将日志吐到kube-system命名空间下的elasticsearch中，如果需要指定吐到其他elasticsearch中，修改环境变量- name: ELASTICSEARCH_HOST value: elasticsearch - name: ELASTICSEARCH_PORT value: \"9200\" - name: ELASTICSEARCH_USERNAME value: elastic - name: ELASTICSEARCH_PASSWORD value: changeme curl -L -O https://raw.githubusercontent.com/elastic/beats/7.4/deploy/kubernetes/filebeat-kubernetes.yaml kubectl create -f filebeat-kubernetes.yaml kubectl --namespace=kube-system get ds/filebeat OKD部署curl -L -O https://raw.githubusercontent.com/elastic/beats/7.4/deploy/kubernetes/filebeat-kubernetes.yaml 修改部署文件 securityContext: runAsUser: 0 privileged: true oc adm policy add-scc-to-user privileged system:serviceaccount:kube-system:filebeat 四. 配置Filebeat的配置文件路径：/etc/filebeat/filebeat.yml 配置语法为YAML 配置项 描述 示例 processors.* Processors配置 processors:- include_fields: fields: [\"cpu\"]- drop_fields: fields: [\"cpu.user\", \"cpu.system\"] filebeat.modules: Module配置 filebeat.modules:- module: mysql error: enabled: true filebeat.inputs: Input配置 filebeat.inputs:- type: log enabled: false paths: - /var/log/*.log output.*: Output配置 output.console: enabled: true path.* 组件产生文件的位置配置 path.home: /usr/share/filebeatpath.data: \\${path.home}/datapath.logs: \\${path.home}/logs setup.template.* Template配置 logging.* 日志配置 logging.level: infologging.to_stderr: falselogging.to_files: true monitoring.* X-Pack监控配置 monitoring.enabled: falsemonitoring.elasticsearch.hosts: [\"localhost:9200\"] http.* HTTP Endpoint配置 http.enabled: falsehttp.port: 5066http.host: localhost filebeat.autodiscover.* Filebeat自动发现配置 通用配置 全局配置项 queue.* 缓存队列设置 全局配置项 配置项 默认值 描述 示例 registry.path ${path.data}/registry 注册表文件的根路径 filebeat.registry.path: registry registry.file_permissions 0600 注册表文件的权限。Window下该配置项无效 filebeat.registry.file_permissions: 0600 registry.flush 0s filebeat.registry.flush: 5s registry.migrate_file filebeat.registry.migrate_file: /path/to/old/registry_file config_dir filebeat.config_dir: path/to/configs shutdown_timeout 5s filebeat.shutdown_timeout: 5s 通用配置项 配置项 默认值 描述 示例 name name: \"my-shipper\" tags tags: [\"service-X\", \"web-tier\"] fields fields: {project: \"myproject\", instance-id: \"57452459\"} fields_under_root 如果该选项设置为true，则新增fields会放在根路径下，而不是放在fields路径下。自定义的field会覆盖filebeat默认的field。 fields_under_root: true processors 该配置项可配置以下Processors，详见 max_procs 配置示例# Modules配置项 filebeat.modules: - module: system # 通用配置项 fields: level: debug review: 1 fields_under_root: false # Processors配置项 processors: - decode_json_fields: # Input配置项 filebeat.inputs: - type: log # Output配置项 output.elasticsearch: output.logstash: 五. Input插件类型Input类型 类型 描述 配置示例 [Log](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-log.html) 从日志文件中读取每一行 filebeat.inputs: - type: log paths: - /var/log/messages - /var/log/*.log [Stdin](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-stdin.html) filebeat.inputs: - type: stdin [Container](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-container.html) filebeat.inputs: - type: container paths: - '/var/lib/docker/containers/*/*.log' [Kafka](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-kafka.html) filebeat.inputs: - type: kafka hosts: - kafka-broker-1:9092 - kafka-broker-2:9092 topics: [\"my-topic\"] group_id: \"filebeat\" [Redis](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-redis.html) filebeat.inputs: - type: redis hosts: [\"localhost:6379\"] password: \"${redis_pwd}\" [UDP](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-udp.html) filebeat.inputs: - type: udp max_message_size: 10KiB host: \"localhost:8080\" [ Docker](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-docker.html) filebeat.inputs: - type: docker containers.ids: - 'e067b58476dc57d6986dd347' [TCP](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-tcp.html) filebeat.inputs: - type: tcp max_message_size: 10MiB host: \"localhost:9000\" [Syslog](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-syslog.html) filebeat.inputs: - type: syslog protocol.udp: host: \"localhost:9000\" [s3](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-s3.html) filebeat.inputs: - type: s3 queue_url: https://test.amazonaws.com/12/test access_key_id: my-access-key secret_access_key: my-secret-access-key [NetFlow](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-netflow.html) [Google Pub/Sub](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-google-pubsub.html) 六. Output插件类型 类型 描述 配置样例 [Elasticsearch](https://www.elastic.co/guide/en/beats/filebeat/current/elasticsearch-output.html) output.elasticsearch: hosts: [\"https://localhost:9200\"] protocol: \"https\" index: \"filebeat-%{[agent.version]}-%{+yyyy.MM.dd}\" ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] ssl.certificate: \"/etc/pki/client/cert.pem\" ssl.key: \"/etc/pki/client/cert.key\" username: \"filebeat_internal\" password: \"YOUR_PASSWORD\" [Logstash](https://www.elastic.co/guide/en/beats/filebeat/current/logstash-output.html) output.logstash: hosts: [\"127.0.0.1:5044\"] [Kafka](https://www.elastic.co/guide/en/beats/filebeat/current/kafka-output.html) output.kafka: hosts: [\"kafka1:9092\", \"kafka2:9092\", \"kafka3:9092\"] topic: '%{[fields.log_topic]}' partition.round_robin: reachable_only: false required_acks: 1 compression: gzip max_message_bytes: 1000000 [Redis](https://www.elastic.co/guide/en/beats/filebeat/current/redis-output.html) output.redis: hosts: [\"localhost\"] password: \"my_password\" key: \"filebeat\" db: 0 timeout: 5 [File](https://www.elastic.co/guide/en/beats/filebeat/current/file-output.html) output.file: path: \"/tmp/filebeat\" filename: filebeat #rotate_every_kb: 10000 #number_of_files: 7 #permissions: 0600 [Console](https://www.elastic.co/guide/en/beats/filebeat/current/console-output.html) output.console: pretty: true [Cloud](https://www.elastic.co/guide/en/beats/filebeat/current/configure-cloud-id.html) 七. Processors插件配置语法processors: - if: then: - : - : ... else: - : - : 可以再Input中添加Processor- type: processors: - : when: 条件语法equalsequals: http.response.code: 200 contains contains: status: \"Specific error\" regexp regexp: system.process.name: \"foo.*\" range：The condition supports lt, lte, gt and gte. The condition accepts only integer or float values. range: http.response.code: gte: 400 network network: source.ip: private destination.ip: '192.168.1.0/24' destination.ip: ['192.168.1.0/24', '10.0.0.0/8', loopback] has_fields has_fields: ['http.response.code'] or or: - - - ... ----------------------------- or: - equals: http.response.code: 304 - equals: http.response.code: 404 and and: - - - ... ----------------------------- and: - equals: http.response.code: 200 - equals: status: OK ----------------------------- or: - - and: - - not not: -------------- not: equals: status: OK 支持的Processors 类型 作用 配置样例 [`add_cloud_metadata`](https://www.elastic.co/guide/en/beats/filebeat/current/add-cloud-metadata.html) [`add_docker_metadata`](https://www.elastic.co/guide/en/beats/filebeat/current/add-docker-metadata.html) processors: - add_docker_metadata: host: \"unix:///var/run/docker.sock\" [`add_fields`](https://www.elastic.co/guide/en/beats/filebeat/current/add-fields.html) processors:- add_fields: target: project fields: name: myproject id: '574734885120952459' [`add_host_metadata`](https://www.elastic.co/guide/en/beats/filebeat/current/add-host-metadata.html) processors: - add_host_metadata: netinfo.enabled: false cache.ttl: 5m geo: name: nyc-dc1-rack1 location: 40.7128, -74.0060 continent_name: North America country_iso_code: US region_name: New York region_iso_code: NY city_name: New York [`add_kubernetes_metadata`](https://www.elastic.co/guide/en/beats/filebeat/current/add-kubernetes-metadata.html) processors: - add_kubernetes_metadata: host: kube_config: ~/.kube/config default_indexers.enabled: false default_matchers.enabled: false indexers: - ip_port: matchers: - fields: lookup_fields: [\"metricset.host\"] [`add_labels`](https://www.elastic.co/guide/en/beats/filebeat/current/add-labels.html) processors:- add_labels: labels: number: 1 with.dots: test nested: with.dots: nested array: - do - re - with.field: mi [`add_locale`](https://www.elastic.co/guide/en/beats/filebeat/current/add-locale.html) processors:- add_locale: ~processors:- add_locale: format: abbreviation [`add_observer_metadata`](https://www.elastic.co/guide/en/beats/filebeat/current/add-observer-metadata.html) [`add_process_metadata`](https://www.elastic.co/guide/en/beats/filebeat/current/add-process-metadata.html) [`add_tags`](https://www.elastic.co/guide/en/beats/filebeat/current/add-tags.html) processors:- add_tags: tags: [web, production] target: \"environment\" [`community_id`](https://www.elastic.co/guide/en/beats/filebeat/current/community-id.html) [`convert`](https://www.elastic.co/guide/en/beats/filebeat/current/convert.html) processors: - convert: fields: - {from: \"src_ip\", to: \"source.ip\", type: \"ip\"} - {from: \"src_port\", to: \"source.port\", type: \"integer\"} ignore_missing: true fail_on_error: false [`decode_base64_field`](https://www.elastic.co/guide/en/beats/filebeat/current/decode-base64-field.html) [`decode_cef`](https://www.elastic.co/guide/en/beats/filebeat/current/processor-decode-cef.html) [`decode_csv_fields`](https://www.elastic.co/guide/en/beats/filebeat/current/decode-csv-fields.html) [`decode_json_fields`](https://www.elastic.co/guide/en/beats/filebeat/current/decode-json-fields.html) [`decompress_gzip_field`](https://www.elastic.co/guide/en/beats/filebeat/current/decompress-gzip-field.html) [`dissect`](https://www.elastic.co/guide/en/beats/filebeat/current/dissect.html) processors:- dissect: tokenizer: \"%{key1} %{key2}\" field: \"message\" target_prefix: \"dissect\" [`dns`](https://www.elastic.co/guide/en/beats/filebeat/current/processor-dns.html) [`drop_event`](https://www.elastic.co/guide/en/beats/filebeat/current/drop-event.html) processors:- drop_event: when: condition [`drop_fields`](https://www.elastic.co/guide/en/beats/filebeat/current/drop-fields.html) processors:- drop_fields: when: condition fields: [\"field1\", \"field2\", ...] ignore_missing: false [`extract_array`](https://www.elastic.co/guide/en/beats/filebeat/current/extract-array.html) processors: - extract_array: field: my_array mappings: source.ip: 0 destination.ip: 1 network.transport: 2 [`include_fields`](https://www.elastic.co/guide/en/beats/filebeat/current/include-fields.html) processors: - include_fields: when: condition fields: [\"field1\", \"field2\", ...] [`registered_domain`](https://www.elastic.co/guide/en/beats/filebeat/current/processor-registered-domain.html) [`rename`](https://www.elastic.co/guide/en/beats/filebeat/current/rename-fields.html) processors: - rename: fields: - from: \"a.g\" to: \"e.d\" ignore_missing: false fail_on_error: true [`script`](https://www.elastic.co/guide/en/beats/filebeat/current/processor-script.html) [`timestamp`](https://www.elastic.co/guide/en/beats/filebeat/current/processor-timestamp.html) 八. 采集注册文件解析采集注册文件路径：/var/lib/filebeat/registry/filebeat/data.json[{\"source\":\"/root/mysql-slow-sql-log/mysql-slowsql.log\",\"offset\":1365442,\"timestamp\":\"2019-10-11T09:29:35.185399057+08:00\",\"ttl\":-1,\"type\":\"log\",\"meta\":null,\"FileStateOS\":{\"inode\":2360926,\"device\":2051}}] source # 记录采集日志的完整路径 offset # 已经采集的日志的字节数;已经采集到日志的哪个字节位置 timestamp # 日志最后一次发生变化的时间戳 ttl # 采集失效时间，-1表示只要日志存在，就一直采集该日志 type: meta filestateos # 操作系统相关 　　inode # 日志文件的inode号 　　device # 日志所在磁盘的磁盘编号 硬盘格式化的时候，操作系统自动将硬盘分成了两个区域。一个是数据区，用来存放文件的数据信息一个是inode区，用来存放文件的元信息，比如文件的创建者、创建时间、文件大小等等每一个文件都有对应的inode，里边包含了与该文件有关的一些信息，可以用stat命令查看文件的inode信息> stat /var/log/messages File: ‘/var/log/messages’ Size: 56216339 Blocks: 109808 IO Block: 4096 regular file Device: 803h/2051d Inode: 1053379 Links: 1 Access: (0600/-rw-------) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2019-10-06 03:20:01.528781081 +0800 Modify: 2019-10-12 13:59:13.059112545 +0800 Change: 2019-10-12 13:59:13.059112545 +0800 Birth: - 2051为十进制数，对应十六进制数803参考链接https://www.cnblogs.com/micmouse521/p/8085229.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-16 17:20:54 "},"origin/filebeat-多实例部署.html":{"url":"origin/filebeat-多实例部署.html","title":"多实例安装部署","keywords":"","body":"Filebeat的多实例部署一、上下文一台服务器为了充分利用资源使用，通常安装了多个系统的组件。例如既是MySQL集群的节点，又是Redis集群的节点。如果该服务器之前已经部署了一个FIlebeat实例用来采集MySQL的慢查询日志，输出日志到指定的Logstash进行处理。而此时，有需求要收集该服务器上另外一个系统的组件日志数据。FIlebeat的配置中无法使用条件判断设置多个不同的输出目的地。此时可以直接部署多实例的filebeat。二、部署配置以采集API网关Kong的日志为例.创建新的filebeat配置文件/etc/filebeat/filebeat-kong.yml 创建Filebeat系统服务启动配置文件 /usr/lib/systemd/system/filebeat-kong.service[Unit] Description=Filebeat sends kong log files to Logstash or directly to Elasticsearch. Documentation=https://www.elastic.co/products/beats/filebeat Wants=network-online.target After=network-online.target [Service] Environment=\"BEAT_LOG_OPTS=-e\" # 指定Filebeat配置文件 Environment=\"BEAT_CONFIG_OPTS=-c /etc/filebeat/filebeat-kong.yml\" # 不同实例filebeat的'path.data'一定要不一样 Environment=\"BEAT_PATH_OPTS=-path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat-kong -path.logs /var/log/filebeat-kong\" ExecStart=/usr/share/filebeat/bin/filebeat $BEAT_LOG_OPTS $BEAT_CONFIG_OPTS $BEAT_PATH_OPTS Restart=always [Install] WantedBy=multi-user.target 启动filebeat服务 sudo systemctl daemon-reload sudo systemctl start filebeat-kong.service sudo systemctl status filebeat-kong.service -l Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-16 17:20:54 "},"origin/filebeat-modules模块.html":{"url":"origin/filebeat-modules模块.html","title":"Filebeat Modules","keywords":"","body":"Filebeat的Modules模块一、简介Filebeat采集日志文件除了可以自定义配置Input采集器、Processor处理器、输出目的地等，还提供大量的模板配置Modules来快速地配置采集通用格式的日志文件。例如Nginx标准格式的日志文件。Filebeat的Module简化了常见日志格式的收集、解析和可视化。一个典型的Module(例如，对于Nginx日志的Module)由一个或多个Fileset组成(对于Nginx，则是access和error日志文件)。Fileset包含以下内容:Filebeat输入配置：其中包含查找日志文件的默认路径，而这些默认路径取决于操作系统。Filebeat配置还负责在需要时将多行事件拼接在一起。 Elasticsearch Ingest节点Pipeline定义：用于解析日志行。 字段定义：用于为每个字段配置Elasticsearch的正确类型。它们还包含每个字段的简短描述。 Kibana表盘样本：可以用来可视化日志文件。 Filebeat支持的ModulesFilebeat模块需要Elasticsearch 5.2或更高版本。 类型 [*Modules overview*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-modules-overview.html) [*Apache module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-apache.html) [*Auditd module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-auditd.html) [*AWS module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-aws.html) [*CEF module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-cef.html) [*Cisco module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-cisco.html) [*Coredns Module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-coredns.html) [*Elasticsearch module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-elasticsearch.html) [*Envoyproxy Module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-envoyproxy.html) [*Google Cloud module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-googlecloud.html) [*haproxy module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-haproxy.html) [*IBM MQ module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-ibmmq.html) [*Icinga module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-icinga.html) [*IIS module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-iis.html) [*Iptables module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-iptables.html) [*Kafka module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-kafka.html) [*Kibana module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-kibana.html) [*Logstash module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-logstash.html) [*MongoDB module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-mongodb.html) [*MSSQL module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-mssql.html) [*MySQL module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-mysql.html) [*nats module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-nats.html) [*NetFlow module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-netflow.html) [*Nginx module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-nginx.html) [*Osquery module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-osquery.html) [*Palo Alto Networks module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-panw.html) [*PostgreSQL module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-postgresql.html) [*RabbitMQ module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-rabbitmq.html) [*Redis module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-redis.html) [*Santa module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-santa.html) [*Suricata module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-suricata.html) [*System module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-system.html) [*Traefik module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-traefik.html) [*Zeek (Bro) Module*](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-zeek.html) 二、Module配置1. 加载Modules在/etc/filebeat/filebeat.yml中配置加载Modulesfilebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: true reload.period: 10s 指定特殊的filebeat全局配置文件来配置加载Modulesfilebeat -c /etc/filebeat/filebeat-kong.yaml modules enable nginx 2. 查看所有Modulefilebeat modules list 3. 启用ModuleFilebeat的Modules配置文件通常在/etc/filebeat/modules.d下Filebeat提供了几种启用模块的不同方法：命令行启用module filebeat modules enable module名 在配置文件/etc/filebeat/filebeat.yml中启用Modules filebeat.modules: - module: nginx - module: mysql - module: system 在运行时使用Modules filebeat --modules nginx,mysql,system 4. 配置Module变量参数Filebeat的Modules配置文件通常在/etc/filebeat/modules.d下。当module不启用时，自带的Module配置文件是以.disabled后缀的。启用后，会自动去掉.disabled后缀，此时可以修改module的默认变量参数在运行时配置Module变量参数filebeat -e --modules 模块名 -M \"nginx.access.var.paths=[/var/log/nginx/access.log*]\" 在Modules的配置文件中配置变量参数 以Nginx模块的配置文件为例/etc/filebeat/modules.d/nginx.yml- module: nginx # 设置Nginx访问日志fileset access: input: close_eof: true enabled: true # 日志文件路径。如果为空，默认根据操作系统版本自行选择日志文件路径. var.paths: [ \"/var/log/nginx/access.log\", \"/var/log/nginx/admin_access.log\" ] # 设置Nginx错误日志fileset error: enabled: true # 日志文件路径。如果为空，默认根据操作系统版本自行选择日志文件路径. var.paths: [\"/var/log/nginx/error.log\"] 注意：例如想要给Nginx模块的fileset添加一个参数close_eof: true，可使用以下参数配置文件- module: nginx access: input: close_eof: true 命令行 filebeat -e --modules nginx -M \"*.*.input.close_eof=true\" # 或者 filebeat -e --modules nginx -M \"nginx.*.input.close_eof=true\" # 或者 filebeat -e --modules nginx -M \"nginx.access.input.close_eof=true\" Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-16 17:20:54 "},"origin/filbeat-nginx-module.html":{"url":"origin/filbeat-nginx-module.html","title":"Nginx Module","keywords":"","body":"Filebeat的Nginx模块一、简介Filebeat的Nginx Module模块可直接用来处理Nginx标准格式的访问日志和错误日志。二、启用Nginx模块1. 加载Modules在/etc/filebeat/filebeat.yml中配置加载Modulesfilebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: true reload.period: 10s 指定特殊的filebeat全局配置文件来配置加载Modulesfilebeat -c /etc/filebeat/filebeat-kong.yaml modules enable nginx 2. 启用Nginx 模块filebeat modules enable nginx 三、配置Nginx模块变量参数1. 配置Nginx 模块变量参数的方式Nginx module的配置文件/etc/filebeat/modules.d/nginx.yml- module: nginx # 设置Nginx访问日志 access: input: close_eof: true enabled: true # 日志文件路径。如果为空，默认根据操作系统版本自行选择日志文件路径. var.paths: [ \"/var/log/nginx/access.log\", \"/var/log/nginx/admin_access.log\" ] # 设置Nginx错误日志 error: enabled: true # 日志文件路径。如果为空，默认根据操作系统版本自行选择日志文件路径. var.paths: [\"/var/log/nginx/error.log\"] 在运行时 filebeat -e \\ --modules nginx \\ -M \"nginx.access.var.paths=[ \"/var/log/nginx/access.log\", \"/var/log/nginx/admin_access.log\" ]\" \\ -M \"nginx.error.var.paths=[\"/var/log/nginx/error.log\"]\" \\ -M \"nginx.access.input.close_eof=true\" 四、Nginx模块配置文件详解1. Nginx模块配置目录结构Nginx模块中所有的配置文件在/usr/share/filebeat/module/nginx路径下/usr/share/filebeat/module/nginx ├── access │ ├── config │ │ └── nginx-access.yml │ ├── ingest │ │ └── default.json │ └── manifest.yml ├── error │ ├── config │ │ └── nginx-error.yml │ ├── ingest │ │ └── pipeline.json │ └── manifest.yml └── module.yml 2. module.ymldashboards: - id: 55a9e6e0-a29e-11e7-928f-5dbe6f6f5519 file: Filebeat-nginx-overview.json - id: 046212a0-a2a1-11e7-928f-5dbe6f6f5519 file: Filebeat-nginx-logs.json - id: ML-Nginx-Access-Remote-IP-Count-Explorer file: ml-nginx-access-remote-ip-count-explorer.json - id: ML-Nginx-Remote-IP-URL-Explorer file: ml-nginx-remote-ip-url-explorer.json 3. access/manifest.ymlmodule_version: \"1.0\" var: - name: paths default: - /var/log/nginx/access.log* os.darwin: - /usr/local/var/log/nginx/access.log* os.windows: - c:/programdata/nginx/logs/*access.log* ingest_pipeline: ingest/default.json input: config/nginx-access.yml machine_learning: - name: response_code job: machine_learning/response_code.json datafeed: machine_learning/datafeed_response_code.json min_version: 5.5.0 - name: low_request_rate job: machine_learning/low_request_rate.json datafeed: machine_learning/datafeed_low_request_rate.json min_version: 5.5.0 - name: remote_ip_url_count job: machine_learning/remote_ip_url_count.json datafeed: machine_learning/datafeed_remote_ip_url_count.json min_version: 5.5.0 - name: remote_ip_request_rate job: machine_learning/remote_ip_request_rate.json datafeed: machine_learning/datafeed_remote_ip_request_rate.json min_version: 5.5.0 - name: visitor_rate job: machine_learning/visitor_rate.json datafeed: machine_learning/datafeed_visitor_rate.json min_version: 5.5.0 requires.processors: - name: user_agent plugin: ingest-user-agent - name: geoip plugin: ingest-geoip 4. access/config/nginx-access.ymltype: log paths: {{ range $i, $path := .paths }} - {{$path}} {{ end }} exclude_files: [\".gz$\"] processors: - add_locale: ~ 5. access/ingest/default.json{ \"description\": \"Pipeline for parsing Nginx access logs. Requires the geoip and user_agent plugins.\", \"processors\": [ { \"grok\": { \"field\": \"message\", \"patterns\": [ \"\\\"?(?:%{IP_LIST:nginx.access.remote_ip_list}|%{DATA:source.address}) - %{DATA:user.name} \\\\[%{HTTPDATE:nginx.access.time}\\\\] \\\"%{DATA:nginx.access.info}\\\" %{NUMBER:http.response.status_code:long} %{NUMBER:http.response.body.bytes:long} \\\"%{DATA:http.request.referrer}\\\" \\\"%{DATA:user_agent.original}\\\"\" ], \"pattern_definitions\": { \"IP_LIST\": \"%{IP}(\\\"?,?\\\\s*%{IP})*\" }, \"ignore_missing\": true } }, { \"grok\": { \"field\": \"nginx.access.info\", \"patterns\": [ \"%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}\", \"\" ], \"ignore_missing\": true } }, { \"remove\": { \"field\": \"nginx.access.info\" } }, { \"split\": { \"field\": \"nginx.access.remote_ip_list\", \"separator\": \"\\\"?,?\\\\s+\", \"ignore_missing\": true } }, { \"split\": { \"field\": \"nginx.access.origin\", \"separator\": \"\\\"?,?\\\\s+\", \"ignore_missing\": true } }, { \"set\": { \"field\": \"source.ip\", \"value\": \"\" } }, { \"script\": { \"lang\": \"painless\", \"source\": \"boolean isPrivate(def dot, def ip) { try { StringTokenizer tok = new StringTokenizer(ip, dot); int firstByte = Integer.parseInt(tok.nextToken()); int secondByte = Integer.parseInt(tok.nextToken()); if (firstByte == 10) { return true; } if (firstByte == 192 && secondByte == 168) { return true; } if (firstByte == 172 && secondByte >= 16 && secondByte 6. error/manifest.ymlmodule_version: \"1.0\" var: - name: paths default: - /var/log/nginx/error.log* os.darwin: - /usr/local/var/log/nginx/error.log* os.windows: - c:/programdata/nginx/logs/error.log* ingest_pipeline: ingest/pipeline.json input: config/nginx-error.yml 7. error/config/nginx-error.ymltype: log paths: {{ range $i, $path := .paths }} - {{$path}} {{ end }} exclude_files: [\".gz$\"] processors: - add_locale: ~ 8. error/ingest/pipeline.json{ \"description\": \"Pipeline for parsing the Nginx error logs\", \"processors\": [{ \"grok\": { \"field\": \"message\", \"patterns\": [ \"%{DATA:nginx.error.time} \\\\[%{DATA:log.level}\\\\] %{NUMBER:process.pid:long}#%{NUMBER:process.thread.id:long}: (\\\\*%{NUMBER:nginx.error.connection_id:long} )?%{GREEDYDATA:message}\" ], \"ignore_missing\": true } }, { \"rename\": { \"field\": \"@timestamp\", \"target_field\": \"event.created\" } }, { \"date\": { \"field\": \"nginx.error.time\", \"target_field\": \"@timestamp\", \"formats\": [\"yyyy/MM/dd H:m:s\"], \"ignore_failure\": true } }, { \"date\": { \"if\": \"ctx.event.timezone != null\", \"field\": \"nginx.error.time\", \"target_field\": \"@timestamp\", \"formats\": [\"yyyy/MM/dd H:m:s\"], \"timezone\": \"{{ event.timezone }}\", \"on_failure\": [{\"append\": {\"field\": \"error.message\", \"value\": \"{{ _ingest.on_failure_message }}\"}}] } }, { \"remove\": { \"field\": \"nginx.error.time\" } }], \"on_failure\" : [{ \"set\" : { \"field\" : \"error.message\", \"value\" : \"{{ _ingest.on_failure_message }}\" } }] } 五、示例：1. Filebeat Nginx模块配置采集API网关Kong日志Kong日志数据采集处理流程：kong节点 + filbeat ----> Kubernetes上的Logstash ----> Kubernetes上的ElasticsearchKong使用了Nginx作为基础组件，它的日志也主要是Nginx格式的日志，分为两种：访问日志和错误日志。它的Nginx是安装了Lua模块的，而Lua模块的错误日志和Nginx的错误日志混合在一起。Lua的错误日志格式有的是多行。这就造成整个Nginx错误日志中既有单行错误日志，又有多行错误日志。直接使用Filebeat的Nginx模块采集日志文件。对于标准格式的Kong访问日志是没有问题的，关键点是错误日志，要修改filebeat的Nginx模块对错误日志文件进行多行采集，设置过滤关键词，将关键词之间的多行合并为一个采集事件。Kong的日志输出目录：/usr/local/kong/logs。目录下有两种格式的日志文件Nginx标准日志格式的访问日志文件：/usr/local/kong/logs/admin_access.log /usr/local/kong/logs/access.log 172.17.18.169 - - [21/Oct/2019:11:47:42 +0800] \"GET /oalogin.php HTTP/1.1\" 494 46 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\" 带有Lua模块Nginx的错误日志文件 ：/usr/local/kong/logs/error.log ```bash 2019/10/21 10:58:56 [warn] 14716#0: _17345670 [lua] reports.lua:70: log(): [reports] unknown request scheme: http while logging request, client: 172.17.18.169, server: kong, request: \"GET / HTTP/1.1\", host: \"172.17.18.169\" 2019/10/21 10:59:05 [warn] 14717#0: _17346563 [lua] reports.lua:70: log(): [reports] unknown request scheme: http while logging request, client: 172.17.18.169, server: kong, request: \"GET /routes HTTP/1.1\", host: \"172.17.18.169\" 2019/10/21 11:00:09 [error] 14716#0: *17348732 lua coroutine: runtime error: don't know how to respond to POST stack traceback: coroutine 0: [C]: ? coroutine 1: [C]: in function 'resume' /usr/local/share/lua/5.1/lapis/application.lua:397: in function 'handler' /usr/local/share/lua/5.1/lapis/application.lua:130: in function 'resolve' /usr/local/share/lua/5.1/lapis/application.lua:167: in function [C]: in function 'xpcall' /usr/local/share/lua/5.1/lapis/application.lua:173: in function 'dispatch' /usr/local/share/lua/5.1/lapis/nginx.lua:230: in function 'serve' /usr/local/share/lua/5.1/kong/init.lua:1113: in function 'admin_content' content_by_lua(nginx-kong.conf:190):2: in main chunk, client: 172.17.18.169, server: kong_admin, request: \"POST /routes/smsp-route HTTP/1.0\", host: \"local.api.kong.curouser.com:80\" 2019/10/21 11:06:38 [warn] 14713#0: *17362982 [lua] reports.lua:70: log(): [reports] unknown request scheme: http while logging request, client: 172.17.18.169, server: kong, request: \"GET /upstream HTTP/1.1\", host: \"172.17.18.169\" ``` 修改Nginx模块采集错误日志文件的方式Filebeat的安装，Nignx模块启用，模块参数配置等操作步骤省略。这里只写针对Nginx错误日志配置进行的修改。编辑/usr/share/filebeat/module/nginx/error/config/nginx-error.yml# ===========================修改前的====================================== type: log paths: {{ range $i, $path := .paths }} - {{$path}} {{ end }} exclude_files: [\".gz$\"] processors: - add_locale: ~ # ===========================修改后的====================================== type: log paths: {{ range $i, $path := .paths }} - {{$path}} {{ end }} exclude_files: [\".gz$\"] multiline.pattern: '^[0-9]{4}/[0-9]{2}/[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}' multiline.negate: true multiline.match: after Logstash针对FIlebeat发送过来的日志事件进行分割处理的Pipelines#=======================接收Filebeat发送过来的日志事件==================== input { beats { id => \"logstash_kong_beats\" port => 5044 } } #=======================过滤、拆分、转换日志事件============================== filter { if [fileset][name] == \"access\" { grok { match => { \"message\" => [\"%{IPORHOST:[nginx][access][remote_ip]} - %{DATA:[nginx][access][user_name]} \\[%{HTTPDATE:[nginx][access][time]}\\] \\\"%{WORD:[nginx][access][method]} %{DATA:[nginx][access][url]} HTTP/%{NUMBER:[nginx][access][http_version]}\\\" %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \\\"%{DATA:[nginx][access][referrer]}\\\" \\\"%{DATA:[nginx][access][agent]}\\\"\"] } remove_field => \"message\" } mutate { add_field => { \"read_timestamp\" => \"%{@timestamp}\" } } date { match => [ \"[nginx][access][time]\", \"dd/MMM/YYYY:H:m:s Z\" ] remove_field => \"[nginx][access][time]\" } useragent { source => \"[nginx][access][agent]\" target => \"[nginx][access][user_agent]\" remove_field => \"[nginx][access][agent]\" } geoip { source => \"[nginx][access][remote_ip]\" target => \"[nginx][access][geoip]\" } } else if [fileset][name] == \"error\" { grok { match => { \"message\" => [\"%{DATA:[nginx][error][time]} \\[%{DATA:[nginx][error][level]}\\] %{NUMBER:[nginx][error][pid]}#%{NUMBER:[nginx][error][tid]}: (\\*%{NUMBER:[nginx][error][connection_id]} )?%{GREEDYDATA:[nginx][error][message]}\"] } remove_field => \"message\" } mutate { rename => { \"@timestamp\" => \"read_timestamp\" } } date { match => [ \"[nginx][error][time]\", \"YYYY/MM/dd H:m:s\" ] remove_field => \"[nginx][error][time]\" } } } #=======================根据日志事件类型的不同输出到不同elasticsearch索引中==================== output { if [fileset][name] == \"access\" { elasticsearch { id => \"logstash_kong_access_log\" hosts => [\"elasticsearch.elk.svc\"] index => \"kong-accesslog-%{+YYYY.MM.dd}\" document_type => \"_doc\" http_compression => true template_name => \"logstash-logger\" user => \"logstash-user\" password => \"logstash-password\" } }else if [fileset][name] == \"error\"{ elasticsearch { id => \"logstash_kong_error_log\" hosts => [\"elasticsearch.elk.svc\"] index => \"kong-errorlog-%{+YYYY.MM.dd}\" document_type => \"_doc\" http_compression => true template_name => \"logstash-curiouser\" user => \"logstash-user\" password => \"logstash-password\" } } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-16 17:20:54 "},"origin/logstash-简介安装配置Pipeline.html":{"url":"origin/logstash-简介安装配置Pipeline.html","title":"简介安装配置Pipeline","keywords":"","body":"Logastash的简介、安装、配置、Pipeline、插件一. 简介官方文档：https://www.elastic.co/guide/en/logstash/current/getting-started-with-logstash.htmlLogstash是一个开源数据收集引擎，具有实时管道功能。 Logstash可以动态地将来自不同数据源的数据统一起来，并将数据标准化到你所选择的目的地 Logstash 是一款强大的数据处理工具，它可以实现数据传输，格式处理，格式化输出，还有强大的插件功能，常用于日志处理。 Logstash耗资源较大，运行占用CPU和内存高。另外没有消息队列缓存，存在数据丢失隐患 Logstash使用Ruby语言编写的运行在Java虚拟机上的具有收集、分析和转发数据流功能的工具 Logstash使用Pipeline方式进行日志的搜集，处理和输出 Event：logstash将数据流中的每一条数据在input处被转换为event，在output处event再被转换为目标格式的数据Inputs：用于从数据源获取Event。每个Input启动一个线程，从对应数据源获取数据，将数据写入一个队列 Filters：用于过滤、修改Event Outputs：负责输出Event到其他系统中 Logstash使用Pipeline流水线的形式来处理数据Event事件，大致流程如下其中inputs和outputs支持codecs（coder&decoder）在1.3.0 版之前，logstash 只支持纯文本形式输入，然后用filter处理它。但现在，我们可以在输入期间处理不同类型的数据。所以现在的数据处理流程箭头代表数据流向。可以有多个input。中间的queue负责将数据分发到不通的pipline中，每个pipline由batcher，filter和output构成。batcher的作用是批量从queue中取数据（可配置）。logstash数据流历程首先有一个输入数据，例如是一个web.log文件，其中每一行都是一条数据。file imput会从文件中取出数据，然后通过json codec将数据转换成logstash event。 这条event会通过queue流入某一条pipline处理线程中，首先会存放在batcher中。当batcher达到处理数据的条件（如一定时间或event一定规模）后，batcher会把数据发送到filter中，filter对event数据进行处理后转到output，output就把数据输出到指定的输出位置。 输出后还会返回ACK给queue，包含已经处理的event，queue会将已处理的event进行标记。 queue分类In Memory： 在内存中，固定大小，无法处理进程crash. 机器宕机等情况，会导致数据丢失。 Persistent Queue：可处理进程crash情况，保证数据不丢失。保证数据至少消费一次；充当缓冲区，可代替kafka等消息队列作用。 Dead Letter Queues：存放logstash因数据类型错误等原因无法处理的Event Persistent Queue（PQ）处理流程一条数据经由input进入PQ，PQ将数据备份在disk，然后PQ响应input表示已收到数据； 数据从PQ到达filter/output，其处理到事件后返回ACK到PQ； PQ收到ACK后删除磁盘的备份数据； 二. 安装1. 安装Java环境在一些Linux环境下，必须设置JAVA_HOME环境变量，否则Logstash在安装期间没有检测到JAVA_HOME环境变量，会报错并且启动不起来服务。如果JDK目录在/opt下，则 在/usr/bin/下建立软连接指向JAVA_HOME/bin路径下的java2. 安装LogstashYUM/RPM[elasticsearch-7.x] name=Elasticsearch repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md yum install -y logstash-7.2.0 手动下载RPM安装，官方下载链接：https://www.elastic.co/downloads/logstashyum localinstall -y logstash-7*.rpm RPM包安装后各个配置文件的位置 Type Description Default Location Setting **home** Home directory of the Logstash installation. `/usr/share/logstash` **bin** Binary scripts including `logstash` to start Logstash and `logstash-plugin` to install plugins `/usr/share/logstash/bin` **settings** Configuration files, including `logstash.yml`, `jvm.options`, and `startup.options` `/etc/logstash` `path.settings` **conf** Logstash pipeline configuration files `/etc/logstash/conf.d/*.conf` `See /etc/logstash/pipelines.yml` **logs** Log files `/var/log/logstash` `path.logs` **plugins** Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only. `/usr/share/logstash/plugins` `path.plugins` **data** Data files used by logstash and its plugins for any persistence needs. `/var/lib/logstash` `path.data` 二进制包二进制包中各个配置文件的位置 Type Description Default Location Setting **home** Home directory of the Logstash installation. `{extract.path}- Directory created by unpacking the archive` **bin** Binary scripts, including `logstash` to start Logstash and `logstash-plugin` to install plugins `{extract.path}/bin` **settings** Configuration files, including `logstash.yml` and `jvm.options` `{extract.path}/config` `path.settings` **logs** Log files `{extract.path}/logs` `path.logs` **plugins** Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only. `{extract.path}/plugins` `path.plugins` **data** Data files used by logstash and its plugins for any persistence needs. `{extract.path}/data` `path.data` 3. 启动以服务形式或命令启动Logstashsystemctl start logstash #后台会起一个名叫org.jruby.Main的Java后台进程，用jps -l查看 jps -l 使用二进制执行文件启动/user/share/logstash/bin/logstash -f logstash.conf --config.reload.automatic #-f 指定配置文件路径 #--config.reload.automatic 自动检测加载配置文件，该参数在有-e参数是不生效 #--config.reload.interval 设置多少秒检测一次配置文件 如果Logstash启动时没有配置自动加载配置文件，重启进程时加上。 4. 验证/usr/share/logstash/bin/logstash -e 'input { stdin { } } output { stdout {} }' #参数-e：直接从命令行定义配置信息 #配置从标准输入读取输入，然后输出到标准输出 stdin > hello world stdout> 2013-11-21T01:22:14.405+0000 0.0.0.0 hello world #Logstash会在消息上添加时间戳和IP地址 #Ctrl+D 退出Logstash 5. 命令行参数 参数 描述 默认值 -r, --config.reload.automatic Monitor configuration changes and reload whenever it is changed. NOTE: use SIGHUP to manually reload the config false -n, --node.name NAME Specify the name of this logstash instance, if no value is given it will default to the current hostname. 当前主机名 -f, --path.config CONFIG_PATH Load the logstash config from a specific file or directory. If a directory is given, all files in that directory will be concatenated in lexicographical order and then parsed as a single config file. You can also specify wildcards (globs) and any matched files will be loaded in the order described above. -e, --config.string CONFIG_STRING Use the given string as the configuration data. Same syntax as the config file. If no input is pecified, then the following is used as the default input: \"input { stdin { type => stdin } }\" and if no output is specified, then the following is used as the default output: \"output { stdout { codec => rubydebug } }\" If you wish to use both defaults, please use the empty string for the '-e' flag. nil --log.level LEVEL Set the log level for logstash. Possible values are: `fatal` `error` `warn` `info` `debug` `trace` (default: \"info\") -l, --path.logs PATH Write logstash internal logs to the given file. Without this flag, logstash will emit logs to standard output. /usr/share/logstash/logs -t, --config.test_and_exit Check configuration for valid syntax and then exit. false --config.reload.interval RELOAD_INTERVAL How frequently to poll the configuration location for changes, in seconds 3000000000 --http.host HTTP_HOST Web API binding host 127.0.0.1 --http.port HTTP_PORT Web API http port 9600..9700 --log.format FORMAT Specify if Logstash should write its own logs in JSON form (one event per line) or in plain text (using Ruby's Object#inspect) plain --path.settings SETTINGS_DIR Directory containing logstash.yml file. This can also be set through the LS_SETTINGS_DIR environment variable /usr/share/logstash/config -p, --path.plugins PATH A path of where to find plugins. This flag can be given multiple times to include multiple paths. Plugins are expected to be in a specific directory hierarchy: 'PATH/logstash/TYPE/NAME.rb' where TYPE is 'inputs' 'filters', 'outputs' or 'codecs' and NAME is the name of the plugin. [] --path.data PATH This should point to a writable directory. Logstash will use this directory whenever it needs to store data. Plugins will also have access to this path. /usr/share/logstash/data -u, --pipeline.batch.delay DELAY_IN_MS When creating pipeline batches, how long to wait while polling for the next event. 50 --pipeline.id ID Sets the ID of the pipeline. main -b, --pipeline.batch.size SIZE Size of batches the pipeline is to work in. 125 -V, --version Emit the version of logstash and its friends, then exit. -M, --modules.variable MODULES_VARIABLE Load variables for module template. Multiple instances of '-M' or '--modules.variable' are supported. Ignored if '--modules' flag is not used. Should be in the format of '-M \"MODULE_NAME.var.PLUGIN_TYPE.PLUGIN_NAME.VARIABLE_NAME=VALUE\"' as in '-M \"example.var.filter.mutate.fieldname=fieldvalue\"' --modules MODULES Load Logstash modules. Modules can be defined using multiple instances '--modules module1 --modules module2', or comma-separated syntax '--modules=module1,module2' Cannot be used in conjunction with '-e' or '-f' Use of '--modules' will override modules declared in the 'logstash.yml' file. --setup Load index template into Elasticsearch, and saved searches, index-pattern, visualizations, and dashboards into Kibana when running modules. false -w, --pipeline.workers COUNT Sets the number of pipeline workers to run. 20 --config.debug Print the compiled config ruby code out as a debug log (you must also have --log.level=debug enabled). WARNING: This will include any 'password' options passed to plugin configs as plaintext, and may result in plaintext passwords appearing in your logs! false --pipeline.unsafe_shutdown Force logstash to exit during shutdown even if there are still inflight events in memory. By default, logstash will refuse to quit until all received events have been pushed to the outputs. false --java-execution Use Java execution engine. true -i, --interactive SHELL Drop to shell instead of running as normal. Valid shells are \"irb\" and \"pry\" --verbose Set the log level to info. 三. Docker镜像docker pull docker.elastic.co/logstash/logstash:7.4.0 docker pull logstash:7.4.0 镜像中各个配置文件的位置 Type Description Default Location Setting **home** Home directory of the Logstash installation. `/usr/share/logstash` **bin** Binary scripts, including `logstash` to start Logstash and `logstash-plugin` to install plugins `/usr/share/logstash/bin` **settings** Configuration files, including `logstash.yml` and `jvm.options` `/usr/share/logstash/config` `path.settings` **conf** Logstash pipeline configuration files `/usr/share/logstash/pipeline` `path.config` **plugins** Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only. `/usr/share/logstash/plugins` `path.plugins` **data** Data files used by logstash and its plugins for any persistence needs. `/usr/share/logstash/data` `path.data` Note：基于该镜像启动的容器，日志是直接输出到控制台的，无法直接输出到日志文件中docker镜像是基于.tar.gz格式的二进制包创建的 将pipeline文件挂载到/usr/share/logstash/pipeline/下启动docker run --rm -it \\ -v ./test.conf:/usr/share/logstash/pipeline/test.conf \\ docker.elastic.co/logstash/logstash:7.4.0 默认pipeline文件：/usr/share/logstash/pipeline/logstash.conf input { beats { port => 5044 } } output { stdout { codec => rubydebug } } 也就是说如果不配置挂载pipeline文件就直接启动容器，logstash将启动一个最小化的pipeline：Beat Input ---> Stdout Output 可通过设置环境变量的形式配置logstash。docker run --rm -it -e PIPELINE_WORKERS:2 docker.elastic.co/logstash/logstash:7.4.0。例如以下环境变量对应的logstash配置 **Environment Variable** **Logstash Setting** `PIPELINE_WORKERS` `pipeline.workers` `LOG_LEVEL` `log.level` `XPACK_MONITORING_ENABLED` `xpack.monitoring.enabled` logstash docker 镜像中的默认配置 `http.host` `0.0.0.0` `xpack.monitoring.elasticsearch.hosts` `http://elasticsearch:9200` 四. 配置Logstash配置文件中配置项的格式是基于YAML语法，例如：pipeline: batch: size: 125 delay: 50 也可以使用平级格式pipeline.batch.size: 125 pipeline.batch.delay: 50 配置项的值可以引用系统级别的环境变量pipeline.batch.size: ${BATCH_SIZE} pipeline.batch.delay: ${BATCH_DELAY:50} node.name: \"node_${LS_NODE_NAME}\" path.queue: \"/tmp/${QUEUE_DIR:queue}\" 如果设置多个自定义的配置项时，推荐使用以下格式modules: - name: MODULE_NAME1 var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY1: VALUE var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY2: VALUE var.PLUGIN_TYPE2.PLUGIN_NAME2.KEY1: VALUE var.PLUGIN_TYPE3.PLUGIN_NAME3.KEY1: VALUE - name: MODULE_NAME2 var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY1: VALUE var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY2: VALUE 常见的logstash配置 Setting Description Default value `node.name` A descriptive name for the node. Machine’s hostname `path.data` The directory that Logstash and its plugins use for any persistent needs. `LOGSTASH_HOME/data` `pipeline.id` The ID of the pipeline. `main` `pipeline.java_execution` Use the Java execution engine. true `pipeline.workers` The number of workers that will, in parallel, execute the filter and output stages of the pipeline. If you find that events are backing up, or that the CPU is not saturated, consider increasing this number to better utilize machine processing power. Number of the host’s CPU cores `pipeline.batch.size` The maximum number of events an individual worker thread will collect from inputs before attempting to execute its filters and outputs. Larger batch sizes are generally more efficient, but come at the cost of increased memory overhead. You may need to increase JVM heap space in the `jvm.options` config file. See [Logstash Configuration Files](https://www.elastic.co/guide/en/logstash/7.4/config-setting-files.html) for more info. `125` `pipeline.batch.delay` When creating pipeline event batches, how long in milliseconds to wait for each event before dispatching an undersized batch to pipeline workers. `50` `pipeline.unsafe_shutdown` When set to `true`, forces Logstash to exit during shutdown even if there are still inflight events in memory. By default, Logstash will refuse to quit until all received events have been pushed to the outputs. Enabling this option can lead to data loss during shutdown. `false` `pipeline.plugin_classloaders` (Beta) Load Java plugins in independent classloaders to isolate their dependencies. `false` `path.config` The path to the Logstash config for the main pipeline. If you specify a directory or wildcard, config files are read from the directory in alphabetical order. Platform-specific. See [Logstash Directory Layout](https://www.elastic.co/guide/en/logstash/7.4/dir-layout.html). `config.string` A string that contains the pipeline configuration to use for the main pipeline. Use the same syntax as the config file. None `config.test_and_exit` When set to `true`, checks that the configuration is valid and then exits. Note that grok patterns are not checked for correctness with this setting. Logstash can read multiple config files from a directory. If you combine this setting with `log.level: debug`, Logstash will log the combined config file, annotating each config block with the source file it came from. `false` `config.reload.automatic` When set to `true`, periodically checks if the configuration has changed and reloads the configuration whenever it is changed. This can also be triggered manually through the SIGHUP signal. `false` `config.reload.interval` How often in seconds Logstash checks the config files for changes. `3s` `config.debug` When set to `true`, shows the fully compiled configuration as a debug log message. You must also set `log.level: debug`. WARNING: The log message will include any *password* options passed to plugin configs as plaintext, and may result in plaintext passwords appearing in your logs! `false` `config.support_escapes` When set to `true`, quoted strings will process the following escape sequences: `\\n` becomes a literal newline (ASCII 10). `\\r` becomes a literal carriage return (ASCII 13). `\\t` becomes a literal tab (ASCII 9). `\\\\` becomes a literal backslash ``. `\\\"` becomes a literal double quotation mark. `\\'` becomes a literal quotation mark. `false` `modules` When configured, `modules` must be in the nested YAML structure described above this table. None `queue.type` The internal queuing model to use for event buffering. Specify `memory` for legacy in-memory based queuing, or `persisted` for disk-based ACKed queueing ([persistent queues](https://www.elastic.co/guide/en/logstash/7.4/persistent-queues.html)). `memory` `path.queue` The directory path where the data files will be stored when persistent queues are enabled (`queue.type: persisted`). `path.data/queue` `queue.page_capacity` The size of the page data files used when persistent queues are enabled (`queue.type: persisted`). The queue data consists of append-only data files separated into pages. 64mb `queue.max_events` The maximum number of unread events in the queue when persistent queues are enabled (`queue.type: persisted`). 0 (unlimited) `queue.max_bytes` The total capacity of the queue in number of bytes. Make sure the capacity of your disk drive is greater than the value you specify here. If both `queue.max_events` and `queue.max_bytes` are specified, Logstash uses whichever criteria is reached first. 1024mb (1g) `queue.checkpoint.acks` The maximum number of ACKed events before forcing a checkpoint when persistent queues are enabled (`queue.type: persisted`). Specify `queue.checkpoint.acks: 0` to set this value to unlimited. 1024 `queue.checkpoint.writes` The maximum number of written events before forcing a checkpoint when persistent queues are enabled (`queue.type: persisted`). Specify `queue.checkpoint.writes: 0` to set this value to unlimited. 1024 `queue.checkpoint.retry` When enabled, Logstash will retry once per attempted checkpoint write for any checkpoint writes that fail. Any subsequent errors are not retried. This is a workaround for failed checkpoint writes that have been seen only on filesystems with non-standard behavior such as SANs and is not recommended except in those specific circumstances. `false` `queue.drain` When enabled, Logstash waits until the persistent queue is drained before shutting down. `false` `dead_letter_queue.enable` Flag to instruct Logstash to enable the DLQ feature supported by plugins. `false` `dead_letter_queue.max_bytes` The maximum size of each dead letter queue. Entries will be dropped if they would increase the size of the dead letter queue beyond this setting. `1024mb` `path.dead_letter_queue` The directory path where the data files will be stored for the dead-letter queue. `path.data/dead_letter_queue` `http.host` The bind address for the metrics REST endpoint. `\"127.0.0.1\"` `http.port` The bind port for the metrics REST endpoint. `9600` `log.level` 设置Logstash日志输出级别 可用值：`fatal error warn info debug trace` `info` `log.format` The log format. Set to `json` to log in JSON format, or `plain` to use `Object#.inspect`. `plain` `path.logs` The directory where Logstash will write its log to. `LOGSTASH_HOME/logs` `path.plugins` Where to find custom plugins. You can specify this setting multiple times to include multiple paths. Plugins are expected to be in a specific directory hierarchy: `PATH/logstash/TYPE/NAME.rb` where `TYPE` is `inputs`, `filters`, `outputs`, or `codecs`, and `NAME` is the name of the plugin. Platform-specific. See [Logstash Directory Layout](https://www.elastic.co/guide/en/logstash/7.4/dir-layout.html). 五. Pipeline1. 配置项结构Logstash Pipeline文件的配置项分为三个部分：input{ input插件{ 插件配置项 } } filter{ filter插件{ 插件配置项 } } output{ output插件{ 插件配置项 } } Note:如果在filter中添加了多种处理规则，则按照它的顺序一一处理，但是有一些插件并不是线程安全的。 如果在filter中指定了两个一样的的插件，这两个任务并不能保证准确的按顺序执行，因此官方也推荐避免在filter中重复使用插件。 2. 插件的条件控制官方文档：https://www.elastic.co/guide/en/logstash/6.7/event-dependent-configuration.html#conditionals有时需要在特定条件下过滤或输出事件。为此，您可以使用条件（conditional）来决定filter和output处理特定的事件。比如在elk系统中想要添加一个type类型的关键字来根据不同的条件赋值，最后好做统计。条件语支持if，else if和else语句并且可以嵌套。条件语法if EXPRESSION { ... } else if EXPRESSION { ... } else { ... } 操作符比较操作：相等: ==, !=, , >, , >= 正则: `=~(匹配正则), !~(不匹配正则) 包含:in(包含), not in(不包含) 布尔操作：and(与), or(或), nand(非与), xor(非或) 一元运算符：!(取反) ()(复合表达式), !()(对复合表达式结果取反) 示例 filter { if [foo] in [foobar] { mutate { add_tag => \"field in field\" } } if [foo] in \"foo\" { mutate { add_tag => \"field in string\" } } if \"hello\" in [greeting] { mutate { add_tag => \"string in field\" } } if [foo] in [\"hello\", \"world\", \"foo\"] { mutate { add_tag => \"field in list\" } } if [missing] in [alsomissing] { mutate { add_tag => \"shouldnotexist\" } } if !(\"foo\" in [\"hello\", \"world\"]) { mutate { add_tag => \"shouldexist\" } } if [message] =~ /\\w+\\s+\\/\\w+(\\/learner\\/course\\/)/ { mutate { add_field => { \"learner_type\" => \"course\" } } } mutate { add_field => { \"show\" => \"This data will be in the output\" } } mutate { add_field => { \"[@metadata][test]\" => \"Hello\" } } mutate { add_field => { \"[@metadata][no_show]\" => \"This data will not be in the output\" } } } output { if \"_grokparsefailure\" not in [tags] { elasticsearch { ... } } if [@metadata][test] == \"Hello\" { stdout { codec => rubydebug } } if [loglevel] == \"ERROR\" and [deployment] == \"production\" { pagerduty { ... } } } 注意：如果if[foo] in \"String\"在执行这样的语句时无法把该字段值转化成String类型。所以最好要加field if exist判断if [\"foo\"] { mutate { add_field => \"bar\" => \"%{foo}\" } } 3. 引用event中的字段直接引用字段，使用[],嵌套字段使用多层[][]即可{ \"a\": \"1\", \"b\": \"2\", \"c\": { \"c1\": \"3\" } } ----------Pipeline中引用Event中的字段-------------- if [b] =~ \"2\" { .......... } if [c][c1] == \"3\" { ........... } 在字符串中以sprintf方式引用,使用%{} { \"a\": \"1\", \"b\": \"2\", \"c\": { \"c1\": \"3\" } } ----------Pipeline中引用Event中的字段-------------- add_field => { \"test\" => \"test: %{b}\" } add_field => { \"test\" => \"test: %{[c][c1]}\" } 六. Input插件插件一览表 Plugin Description Github repository [azure_event_hubs](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-azure_event_hubs.html) Receives events from Azure Event Hubs [azure_event_hubs](https://github.com/logstash-plugins/logstash-input-azure_event_hubs) [beats](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-beats.html) Receives events from the Elastic Beats framework [logstash-input-beats](https://github.com/logstash-plugins/logstash-input-beats) [cloudwatch](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-cloudwatch.html) Pulls events from the Amazon Web Services CloudWatch API [logstash-input-cloudwatch](https://github.com/logstash-plugins/logstash-input-cloudwatch) [couchdb_changes](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-couchdb_changes.html) Streams events from CouchDB’s `_changes` URI [logstash-input-couchdb_changes](https://github.com/logstash-plugins/logstash-input-couchdb_changes) [dead_letter_queue](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-dead_letter_queue.html) read events from Logstash’s dead letter queue [logstash-input-dead_letter_queue](https://github.com/logstash-plugins/logstash-input-dead_letter_queue) [elasticsearch](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-elasticsearch.html) Reads query results from an Elasticsearch cluster [logstash-input-elasticsearch](https://github.com/logstash-plugins/logstash-input-elasticsearch) [exec](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-exec.html) Captures the output of a shell command as an event [logstash-input-exec](https://github.com/logstash-plugins/logstash-input-exec) [file](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-file.html) Streams events from files [logstash-input-file](https://github.com/logstash-plugins/logstash-input-file) [ganglia](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-ganglia.html) Reads Ganglia packets over UDP [logstash-input-ganglia](https://github.com/logstash-plugins/logstash-input-ganglia) [gelf](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-gelf.html) Reads GELF-format messages from Graylog2 as events [logstash-input-gelf](https://github.com/logstash-plugins/logstash-input-gelf) [generator](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-generator.html) Generates random log events for test purposes [logstash-input-generator](https://github.com/logstash-plugins/logstash-input-generator) [github](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-github.html) Reads events from a GitHub webhook [logstash-input-github](https://github.com/logstash-plugins/logstash-input-github) [google_cloud_storage](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-google_cloud_storage.html) Extract events from files in a Google Cloud Storage bucket [logstash-input-google_cloud_storage](https://github.com/logstash-plugins/logstash-input-google_cloud_storage) [google_pubsub](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-google_pubsub.html) Consume events from a Google Cloud PubSub service [logstash-input-google_pubsub](https://github.com/logstash-plugins/logstash-input-google_pubsub) [graphite](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-graphite.html) Reads metrics from the `graphite` tool [logstash-input-graphite](https://github.com/logstash-plugins/logstash-input-graphite) [heartbeat](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-heartbeat.html) Generates heartbeat events for testing [logstash-input-heartbeat](https://github.com/logstash-plugins/logstash-input-heartbeat) [http](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-http.html) Receives events over HTTP or HTTPS [logstash-input-http](https://github.com/logstash-plugins/logstash-input-http) [http_poller](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-http_poller.html) Decodes the output of an HTTP API into events [logstash-input-http_poller](https://github.com/logstash-plugins/logstash-input-http_poller) [imap](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-imap.html) Reads mail from an IMAP server [logstash-input-imap](https://github.com/logstash-plugins/logstash-input-imap) [irc](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-irc.html) Reads events from an IRC server [logstash-input-irc](https://github.com/logstash-plugins/logstash-input-irc) [java_generator](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-java_generator.html) Generates synthetic log events [core plugin](https://github.com/elastic/logstash/blob/7.4/logstash-core/src/main/java/org/logstash/plugins/inputs/Generator.java) [java_stdin](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-java_stdin.html) Reads events from standard input [core plugin](https://github.com/elastic/logstash/blob/7.4/logstash-core/src/main/java/org/logstash/plugins/inputs/Stdin.java) [jdbc](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-jdbc.html) Creates events from JDBC data [logstash-input-jdbc](https://github.com/logstash-plugins/logstash-input-jdbc) [jms](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-jms.html) Reads events from a Jms Broker [logstash-input-jms](https://github.com/logstash-plugins/logstash-input-jms) [jmx](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-jmx.html) Retrieves metrics from remote Java applications over JMX [logstash-input-jmx](https://github.com/logstash-plugins/logstash-input-jmx) [kafka](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-kafka.html) Reads events from a Kafka topic [logstash-input-kafka](https://github.com/logstash-plugins/logstash-input-kafka) [kinesis](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-kinesis.html) Receives events through an AWS Kinesis stream [logstash-input-kinesis](https://github.com/logstash-plugins/logstash-input-kinesis) [log4j](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-log4j.html) Reads events over a TCP socket from a Log4j `SocketAppender` object [logstash-input-log4j](https://github.com/logstash-plugins/logstash-input-log4j) [lumberjack](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-lumberjack.html) Receives events using the Lumberjack protocl [logstash-input-lumberjack](https://github.com/logstash-plugins/logstash-input-lumberjack) [meetup](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-meetup.html) Captures the output of command line tools as an event [logstash-input-meetup](https://github.com/logstash-plugins/logstash-input-meetup) [pipe](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-pipe.html) Streams events from a long-running command pipe [logstash-input-pipe](https://github.com/logstash-plugins/logstash-input-pipe) [puppet_facter](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-puppet_facter.html) Receives facts from a Puppet server [logstash-input-puppet_facter](https://github.com/logstash-plugins/logstash-input-puppet_facter) [rabbitmq](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-rabbitmq.html) Pulls events from a RabbitMQ exchange [logstash-input-rabbitmq](https://github.com/logstash-plugins/logstash-input-rabbitmq) [redis](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-redis.html) Reads events from a Redis instance [logstash-input-redis](https://github.com/logstash-plugins/logstash-input-redis) [relp](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-relp.html) Receives RELP events over a TCP socket [logstash-input-relp](https://github.com/logstash-plugins/logstash-input-relp) [rss](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-rss.html) Captures the output of command line tools as an event [logstash-input-rss](https://github.com/logstash-plugins/logstash-input-rss) [s3](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-s3.html) Streams events from files in a S3 bucket [logstash-input-s3](https://github.com/logstash-plugins/logstash-input-s3) [salesforce](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-salesforce.html) Creates events based on a Salesforce SOQL query [logstash-input-salesforce](https://github.com/logstash-plugins/logstash-input-salesforce) [snmp](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-snmp.html) Polls network devices using Simple Network Management Protocol (SNMP) [logstash-input-snmp](https://github.com/logstash-plugins/logstash-input-snmp) [snmptrap](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-snmptrap.html) Creates events based on SNMP trap messages [logstash-input-snmptrap](https://github.com/logstash-plugins/logstash-input-snmptrap) [sqlite](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-sqlite.html) Creates events based on rows in an SQLite database [logstash-input-sqlite](https://github.com/logstash-plugins/logstash-input-sqlite) [sqs](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-sqs.html) Pulls events from an Amazon Web Services Simple Queue Service queue [logstash-input-sqs](https://github.com/logstash-plugins/logstash-input-sqs) [stdin](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-stdin.html) Reads events from standard input [logstash-input-stdin](https://github.com/logstash-plugins/logstash-input-stdin) [stomp](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-stomp.html) Creates events received with the STOMP protocol [logstash-input-stomp](https://github.com/logstash-plugins/logstash-input-stomp) [syslog](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-syslog.html) Reads syslog messages as events [logstash-input-syslog](https://github.com/logstash-plugins/logstash-input-syslog) [tcp](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-tcp.html) Reads events from a TCP socket [logstash-input-tcp](https://github.com/logstash-plugins/logstash-input-tcp) [twitter](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-twitter.html) Reads events from the Twitter Streaming API [logstash-input-twitter](https://github.com/logstash-plugins/logstash-input-twitter) [udp](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-udp.html) Reads events over UDP [logstash-input-udp](https://github.com/logstash-plugins/logstash-input-udp) [unix](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-unix.html) Reads events over a UNIX socket [logstash-input-unix](https://github.com/logstash-plugins/logstash-input-unix) [varnishlog](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-varnishlog.html) Reads from the `varnish` cache shared memory log [logstash-input-varnishlog](https://github.com/logstash-plugins/logstash-input-varnishlog) [websocket](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-websocket.html) Reads events from a websocket [logstash-input-websocket](https://github.com/logstash-plugins/logstash-input-websocket) [wmi](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-wmi.html) Creates events based on the results of a WMI query [logstash-input-wmi](https://github.com/logstash-plugins/logstash-input-wmi) [xmpp](https://www.elastic.co/guide/en/logstash/7.4/plugins-inputs-xmpp.html) Receives events over the XMPP/Jabber protocol [logstash-input-xmpp](https://github.com/logstash-plugins/logstash-input-xmpp) 插件通用配置项 **参数** **参数值类型** **必须** **默认值** **详解** `add_field` hash No {} 向事件添加字段。 `codec` codec No plain 用于输入数据的编解码器，在输入数据之前，输入编解码器是一种方便的解码方法，不需要在你的Logstash管道中使用单独的过滤器 `enable_metric` boolean No true 禁用或启用这个特定插件实例的指标日志，默认情况下，我们记录所有我们可以记录的指标，但是你可以禁用特定插件的指标集合。 `id` string No 向插件配置添加唯一的ID，如果没有指定ID，则Logstash将生成一个，强烈建议在配置中设置此ID，当你有两个或多个相同类型的插件时，这一点特别有用。例如，如果你有两个log4j输入，在本例中添加一个命名ID将有助于在使用监视API时监视Logstash。input { kafka { id => \"my_plugin_id\" }} `tags` array No 向事件添加任意数量的标记，这有助于以后的处理。 `type` string No 向该输入处理的所有事件添加type字段，类型主要用于过滤器激活，该type作为事件本身的一部分存储，因此你也可以使用该类型在Kibana中搜索它。如果你试图在已经拥有一个type的事件上设置一个type（例如，当你将事件从发送者发送到索引器时），那么新的输入将不会覆盖现有的type，发送方的type集在其生命周期中始终与该事件保持一致，甚至在发送到另一个Logstash服务器时也是如此。 七. Filter插件插件一览表 Plugin Description Github repository [aggregate](https://www.elastic.co/guide/en/logstash/current/plugins-filters-aggregate.html) Aggregates information from several events originating with a single task [logstash-filter-aggregate](https://github.com/logstash-plugins/logstash-filter-aggregate) [alter](https://www.elastic.co/guide/en/logstash/current/plugins-filters-alter.html) Performs general alterations to fields that the `mutate` filter does not handle [logstash-filter-alter](https://github.com/logstash-plugins/logstash-filter-alter) [bytes](https://www.elastic.co/guide/en/logstash/current/plugins-filters-bytes.html) Parses string representations of computer storage sizes, such as \"123 MB\" or \"5.6gb\", into their numeric value in bytes [logstash-filter-bytes](https://github.com/logstash-plugins/logstash-filter-bytes) [cidr](https://www.elastic.co/guide/en/logstash/current/plugins-filters-cidr.html) Checks IP addresses against a list of network blocks [logstash-filter-cidr](https://github.com/logstash-plugins/logstash-filter-cidr) [cipher](https://www.elastic.co/guide/en/logstash/current/plugins-filters-cipher.html) Applies or removes a cipher to an event [logstash-filter-cipher](https://github.com/logstash-plugins/logstash-filter-cipher) [clone](https://www.elastic.co/guide/en/logstash/current/plugins-filters-clone.html) Duplicates events [logstash-filter-clone](https://github.com/logstash-plugins/logstash-filter-clone) [csv](https://www.elastic.co/guide/en/logstash/current/plugins-filters-csv.html) Parses comma-separated value data into individual fields [logstash-filter-csv](https://github.com/logstash-plugins/logstash-filter-csv) [date](https://www.elastic.co/guide/en/logstash/current/plugins-filters-date.html) Parses dates from fields to use as the Logstash timestamp for an event [logstash-filter-date](https://github.com/logstash-plugins/logstash-filter-date) [de_dot](https://www.elastic.co/guide/en/logstash/current/plugins-filters-de_dot.html) Computationally expensive filter that removes dots from a field name [logstash-filter-de_dot](https://github.com/logstash-plugins/logstash-filter-de_dot) [dissect](https://www.elastic.co/guide/en/logstash/current/plugins-filters-dissect.html) Extracts unstructured event data into fields using delimiters [logstash-filter-dissect](https://github.com/logstash-plugins/logstash-filter-dissect) [dns](https://www.elastic.co/guide/en/logstash/current/plugins-filters-dns.html) Performs a standard or reverse DNS lookup [logstash-filter-dns](https://github.com/logstash-plugins/logstash-filter-dns) [drop](https://www.elastic.co/guide/en/logstash/current/plugins-filters-drop.html) Drops all events [logstash-filter-drop](https://github.com/logstash-plugins/logstash-filter-drop) [elapsed](https://www.elastic.co/guide/en/logstash/current/plugins-filters-elapsed.html) Calculates the elapsed time between a pair of events [logstash-filter-elapsed](https://github.com/logstash-plugins/logstash-filter-elapsed) [elasticsearch](https://www.elastic.co/guide/en/logstash/current/plugins-filters-elasticsearch.html) Copies fields from previous log events in Elasticsearch to current events [logstash-filter-elasticsearch](https://github.com/logstash-plugins/logstash-filter-elasticsearch) [environment](https://www.elastic.co/guide/en/logstash/current/plugins-filters-environment.html) Stores environment variables as metadata sub-fields [logstash-filter-environment](https://github.com/logstash-plugins/logstash-filter-environment) [extractnumbers](https://www.elastic.co/guide/en/logstash/current/plugins-filters-extractnumbers.html) Extracts numbers from a string [logstash-filter-extractnumbers](https://github.com/logstash-plugins/logstash-filter-extractnumbers) [fingerprint](https://www.elastic.co/guide/en/logstash/current/plugins-filters-fingerprint.html) Fingerprints fields by replacing values with a consistent hash [logstash-filter-fingerprint](https://github.com/logstash-plugins/logstash-filter-fingerprint) [geoip](https://www.elastic.co/guide/en/logstash/current/plugins-filters-geoip.html) Adds geographical information about an IP address [logstash-filter-geoip](https://github.com/logstash-plugins/logstash-filter-geoip) [grok](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html) Parses unstructured event data into fields [logstash-filter-grok](https://github.com/logstash-plugins/logstash-filter-grok) [http](https://www.elastic.co/guide/en/logstash/current/plugins-filters-http.html) Provides integration with external web services/REST APIs [logstash-filter-http](https://github.com/logstash-plugins/logstash-filter-http) [i18n](https://www.elastic.co/guide/en/logstash/current/plugins-filters-i18n.html) Removes special characters from a field [logstash-filter-i18n](https://github.com/logstash-plugins/logstash-filter-i18n) [java_uuid](https://www.elastic.co/guide/en/logstash/current/plugins-filters-java_uuid.html) Generates a UUID and adds it to each processed event [core plugin](https://github.com/elastic/logstash/blob/7.4/logstash-core/src/main/java/org/logstash/plugins/filters/Uuid.java) [jdbc_static](https://www.elastic.co/guide/en/logstash/current/plugins-filters-jdbc_static.html) Enriches events with data pre-loaded from a remote database [logstash-filter-jdbc_static](https://github.com/logstash-plugins/logstash-filter-jdbc_static) [jdbc_streaming](https://www.elastic.co/guide/en/logstash/current/plugins-filters-jdbc_streaming.html) Enrich events with your database data [logstash-filter-jdbc_streaming](https://github.com/logstash-plugins/logstash-filter-jdbc_streaming) [json](https://www.elastic.co/guide/en/logstash/current/plugins-filters-json.html) Parses JSON events [logstash-filter-json](https://github.com/logstash-plugins/logstash-filter-json) [json_encode](https://www.elastic.co/guide/en/logstash/current/plugins-filters-json_encode.html) Serializes a field to JSON [logstash-filter-json_encode](https://github.com/logstash-plugins/logstash-filter-json_encode) [kv](https://www.elastic.co/guide/en/logstash/current/plugins-filters-kv.html) Parses key-value pairs [logstash-filter-kv](https://github.com/logstash-plugins/logstash-filter-kv) [memcached](https://www.elastic.co/guide/en/logstash/current/plugins-filters-memcached.html) Provides integration with external data in Memcached [logstash-filter-memcached](https://github.com/logstash-plugins/logstash-filter-memcached) [metricize](https://www.elastic.co/guide/en/logstash/current/plugins-filters-metricize.html) Takes complex events containing a number of metrics and splits these up into multiple events, each holding a single metric [logstash-filter-metricize](https://github.com/logstash-plugins/logstash-filter-metricize) [metrics](https://www.elastic.co/guide/en/logstash/current/plugins-filters-metrics.html) Aggregates metrics [logstash-filter-metrics](https://github.com/logstash-plugins/logstash-filter-metrics) [mutate](https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html) Performs mutations on fields [logstash-filter-mutate](https://github.com/logstash-plugins/logstash-filter-mutate) [prune](https://www.elastic.co/guide/en/logstash/current/plugins-filters-prune.html) Prunes event data based on a list of fields to blacklist or whitelist [logstash-filter-prune](https://github.com/logstash-plugins/logstash-filter-prune) [range](https://www.elastic.co/guide/en/logstash/current/plugins-filters-range.html) Checks that specified fields stay within given size or length limits [logstash-filter-range](https://github.com/logstash-plugins/logstash-filter-range) [ruby](https://www.elastic.co/guide/en/logstash/current/plugins-filters-ruby.html) Executes arbitrary Ruby code [logstash-filter-ruby](https://github.com/logstash-plugins/logstash-filter-ruby) [sleep](https://www.elastic.co/guide/en/logstash/current/plugins-filters-sleep.html) Sleeps for a specified time span [logstash-filter-sleep](https://github.com/logstash-plugins/logstash-filter-sleep) [split](https://www.elastic.co/guide/en/logstash/current/plugins-filters-split.html) Splits multi-line messages into distinct events [logstash-filter-split](https://github.com/logstash-plugins/logstash-filter-split) [syslog_pri](https://www.elastic.co/guide/en/logstash/current/plugins-filters-syslog_pri.html) Parses the `PRI` (priority) field of a `syslog` message [logstash-filter-syslog_pri](https://github.com/logstash-plugins/logstash-filter-syslog_pri) [threats_classifier](https://www.elastic.co/guide/en/logstash/current/plugins-filters-threats_classifier.html) Enriches security logs with information about the attacker’s intent [logstash-filter-threats_classifier](https://github.com/empow/logstash-filter-threats_classifier) [throttle](https://www.elastic.co/guide/en/logstash/current/plugins-filters-throttle.html) Throttles the number of events [logstash-filter-throttle](https://github.com/logstash-plugins/logstash-filter-throttle) [tld](https://www.elastic.co/guide/en/logstash/current/plugins-filters-tld.html) Replaces the contents of the default message field with whatever you specify in the configuration [logstash-filter-tld](https://github.com/logstash-plugins/logstash-filter-tld) [translate](https://www.elastic.co/guide/en/logstash/current/plugins-filters-translate.html) Replaces field contents based on a hash or YAML file [logstash-filter-translate](https://github.com/logstash-plugins/logstash-filter-translate) [truncate](https://www.elastic.co/guide/en/logstash/current/plugins-filters-truncate.html) Truncates fields longer than a given length [logstash-filter-truncate](https://github.com/logstash-plugins/logstash-filter-truncate) [urldecode](https://www.elastic.co/guide/en/logstash/current/plugins-filters-urldecode.html) Decodes URL-encoded fields [logstash-filter-urldecode](https://github.com/logstash-plugins/logstash-filter-urldecode) [useragent](https://www.elastic.co/guide/en/logstash/current/plugins-filters-useragent.html) Parses user agent strings into fields [logstash-filter-useragent](https://github.com/logstash-plugins/logstash-filter-useragent) [uuid](https://www.elastic.co/guide/en/logstash/current/plugins-filters-uuid.html) Adds a UUID to events [logstash-filter-uuid](https://github.com/logstash-plugins/logstash-filter-uuid) [xml](https://www.elastic.co/guide/en/logstash/current/plugins-filters-xml.html) Parses XML into fields [logstash-filter-xml](https://github.com/logstash-plugins/logstash-filter-xml) 插件通用配置项 Setting Input type Required [`add_field`](https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html#plugins-filters-mutate-add_field) [hash](http://www.elastic.co/guide/en/logstash/7.4/configuration-file-structure.html#hash) No [`add_tag`](https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html#plugins-filters-mutate-add_tag) [array](http://www.elastic.co/guide/en/logstash/7.4/configuration-file-structure.html#array) No [`enable_metric`](https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html#plugins-filters-mutate-enable_metric) [boolean](http://www.elastic.co/guide/en/logstash/7.4/configuration-file-structure.html#boolean) No [`id`](https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html#plugins-filters-mutate-id) [string](http://www.elastic.co/guide/en/logstash/7.4/configuration-file-structure.html#string) No [`periodic_flush`](https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html#plugins-filters-mutate-periodic_flush) [boolean](http://www.elastic.co/guide/en/logstash/7.4/configuration-file-structure.html#boolean) No [`remove_field`](https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html#plugins-filters-mutate-remove_field) [array](http://www.elastic.co/guide/en/logstash/7.4/configuration-file-structure.html#array) No [`remove_tag`](https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html#plugins-filters-mutate-remove_tag) [array](http://www.elastic.co/guide/en/logstash/7.4/configuration-file-structure.html#array) No 八. Output插件插件一览表 Plugin Description Github repository [boundary](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-boundary.html) Sends annotations to Boundary based on Logstash events [logstash-output-boundary](https://github.com/logstash-plugins/logstash-output-boundary) [circonus](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-circonus.html) Sends annotations to Circonus based on Logstash events [logstash-output-circonus](https://github.com/logstash-plugins/logstash-output-circonus) [cloudwatch](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-cloudwatch.html) Aggregates and sends metric data to AWS CloudWatch [logstash-output-cloudwatch](https://github.com/logstash-plugins/logstash-output-cloudwatch) [csv](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-csv.html) Writes events to disk in a delimited format [logstash-output-csv](https://github.com/logstash-plugins/logstash-output-csv) [datadog](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-datadog.html) Sends events to DataDogHQ based on Logstash events [logstash-output-datadog](https://github.com/logstash-plugins/logstash-output-datadog) [datadog_metrics](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-datadog_metrics.html) Sends metrics to DataDogHQ based on Logstash events [logstash-output-datadog_metrics](https://github.com/logstash-plugins/logstash-output-datadog_metrics) [elastic_app_search](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-elastic_app_search.html) Sends events to the Elastic App Search solution [logstash-output-elastic_app_search](https://github.com/logstash-plugins/logstash-output-elastic_app_search) [elasticsearch](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-elasticsearch.html) Stores logs in Elasticsearch [logstash-output-elasticsearch](https://github.com/logstash-plugins/logstash-output-elasticsearch) [email](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-email.html) Sends email to a specified address when output is received [logstash-output-email](https://github.com/logstash-plugins/logstash-output-email) [exec](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-exec.html) Runs a command for a matching event [logstash-output-exec](https://github.com/logstash-plugins/logstash-output-exec) [file](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-file.html) Writes events to files on disk [logstash-output-file](https://github.com/logstash-plugins/logstash-output-file) [ganglia](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-ganglia.html) Writes metrics to Ganglia’s `gmond` [logstash-output-ganglia](https://github.com/logstash-plugins/logstash-output-ganglia) [gelf](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-gelf.html) Generates GELF formatted output for Graylog2 [logstash-output-gelf](https://github.com/logstash-plugins/logstash-output-gelf) [google_bigquery](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-google_bigquery.html) Writes events to Google BigQuery [logstash-output-google_bigquery](https://github.com/logstash-plugins/logstash-output-google_bigquery) [google_cloud_storage](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-google_cloud_storage.html) Uploads log events to Google Cloud Storage [logstash-output-google_cloud_storage](https://github.com/logstash-plugins/logstash-output-google_cloud_storage) [google_pubsub](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-google_pubsub.html) Uploads log events to Google Cloud Pubsub [logstash-output-google_pubsub](https://github.com/logstash-plugins/logstash-output-google_pubsub) [graphite](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-graphite.html) Writes metrics to Graphite [logstash-output-graphite](https://github.com/logstash-plugins/logstash-output-graphite) [graphtastic](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-graphtastic.html) Sends metric data on Windows [logstash-output-graphtastic](https://github.com/logstash-plugins/logstash-output-graphtastic) [http](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-http.html) Sends events to a generic HTTP or HTTPS endpoint [logstash-output-http](https://github.com/logstash-plugins/logstash-output-http) [influxdb](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-influxdb.html) Writes metrics to InfluxDB [logstash-output-influxdb](https://github.com/logstash-plugins/logstash-output-influxdb) [irc](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-irc.html) Writes events to IRC [logstash-output-irc](https://github.com/logstash-plugins/logstash-output-irc) [java_sink](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-java_sink.html) Discards any events received [core plugin](https://github.com/elastic/logstash/blob/7.4/logstash-core/src/main/java/org/logstash/plugins/outputs/Sink.java) [java_stdout](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-java_stdout.html) Prints events to the STDOUT of the shell [core plugin](https://github.com/elastic/logstash/blob/7.4/logstash-core/src/main/java/org/logstash/plugins/outputs/Stdout.java) [juggernaut](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-juggernaut.html) Pushes messages to the Juggernaut websockets server [logstash-output-juggernaut](https://github.com/logstash-plugins/logstash-output-juggernaut) [kafka](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-kafka.html) Writes events to a Kafka topic [logstash-output-kafka](https://github.com/logstash-plugins/logstash-output-kafka) [librato](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-librato.html) Sends metrics, annotations, and alerts to Librato based on Logstash events [logstash-output-librato](https://github.com/logstash-plugins/logstash-output-librato) [loggly](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-loggly.html) Ships logs to Loggly [logstash-output-loggly](https://github.com/logstash-plugins/logstash-output-loggly) [lumberjack](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-lumberjack.html) Sends events using the `lumberjack` protocol [logstash-output-lumberjack](https://github.com/logstash-plugins/logstash-output-lumberjack) [metriccatcher](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-metriccatcher.html) Writes metrics to MetricCatcher [logstash-output-metriccatcher](https://github.com/logstash-plugins/logstash-output-metriccatcher) [mongodb](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-mongodb.html) Writes events to MongoDB [logstash-output-mongodb](https://github.com/logstash-plugins/logstash-output-mongodb) [nagios](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-nagios.html) Sends passive check results to Nagios [logstash-output-nagios](https://github.com/logstash-plugins/logstash-output-nagios) [nagios_nsca](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-nagios_nsca.html) Sends passive check results to Nagios using the NSCA protocol [logstash-output-nagios_nsca](https://github.com/logstash-plugins/logstash-output-nagios_nsca) [opentsdb](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-opentsdb.html) Writes metrics to OpenTSDB [logstash-output-opentsdb](https://github.com/logstash-plugins/logstash-output-opentsdb) [pagerduty](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-pagerduty.html) Sends notifications based on preconfigured services and escalation policies [logstash-output-pagerduty](https://github.com/logstash-plugins/logstash-output-pagerduty) [pipe](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-pipe.html) Pipes events to another program’s standard input [logstash-output-pipe](https://github.com/logstash-plugins/logstash-output-pipe) [rabbitmq](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-rabbitmq.html) Pushes events to a RabbitMQ exchange [logstash-output-rabbitmq](https://github.com/logstash-plugins/logstash-output-rabbitmq) [redis](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-redis.html) Sends events to a Redis queue using the `RPUSH` command [logstash-output-redis](https://github.com/logstash-plugins/logstash-output-redis) [redmine](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-redmine.html) Creates tickets using the Redmine API [logstash-output-redmine](https://github.com/logstash-plugins/logstash-output-redmine) [riak](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-riak.html) Writes events to the Riak distributed key/value store [logstash-output-riak](https://github.com/logstash-plugins/logstash-output-riak) [riemann](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-riemann.html) Sends metrics to Riemann [logstash-output-riemann](https://github.com/logstash-plugins/logstash-output-riemann) [s3](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-s3.html) Sends Logstash events to the Amazon Simple Storage Service [logstash-output-s3](https://github.com/logstash-plugins/logstash-output-s3) [sns](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-sns.html) Sends events to Amazon’s Simple Notification Service [logstash-output-sns](https://github.com/logstash-plugins/logstash-output-sns) [solr_http](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-solr_http.html) Stores and indexes logs in Solr [logstash-output-solr_http](https://github.com/logstash-plugins/logstash-output-solr_http) [sqs](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-sqs.html) Pushes events to an Amazon Web Services Simple Queue Service queue [logstash-output-sqs](https://github.com/logstash-plugins/logstash-output-sqs) [statsd](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-statsd.html) Sends metrics using the `statsd` network daemon [logstash-output-statsd](https://github.com/logstash-plugins/logstash-output-statsd) [stdout](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-stdout.html) Prints events to the standard output [logstash-output-stdout](https://github.com/logstash-plugins/logstash-output-stdout) [stomp](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-stomp.html) Writes events using the STOMP protocol [logstash-output-stomp](https://github.com/logstash-plugins/logstash-output-stomp) [syslog](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-syslog.html) Sends events to a `syslog` server [logstash-output-syslog](https://github.com/logstash-plugins/logstash-output-syslog) [tcp](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-tcp.html) Writes events over a TCP socket [logstash-output-tcp](https://github.com/logstash-plugins/logstash-output-tcp) [timber](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-timber.html) Sends events to the Timber.io logging service [logstash-output-timber](https://github.com/logstash-plugins/logstash-output-timber) [udp](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-udp.html) Sends events over UDP [logstash-output-udp](https://github.com/logstash-plugins/logstash-output-udp) [webhdfs](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-webhdfs.html) Sends Logstash events to HDFS using the `webhdfs` REST API [logstash-output-webhdfs](https://github.com/logstash-plugins/logstash-output-webhdfs) [websocket](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-websocket.html) Publishes messages to a websocket [logstash-output-websocket](https://github.com/logstash-plugins/logstash-output-websocket) [xmpp](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-xmpp.html) Posts events over XMPP [logstash-output-xmpp](https://github.com/logstash-plugins/logstash-output-xmpp) [zabbix](https://www.elastic.co/guide/en/logstash/7.4/plugins-outputs-zabbix.html) Sends events to a Zabbix server [logstash-output-zabbix](https://github.com/logstash-plugins/logstash-output-zabbix) 插件通用配置项 Setting Input type Required [`codec`](https://www.elastic.co/guide/en/logstash/current/plugins-outputs-csv.html#plugins-outputs-csv-codec) [codec](http://www.elastic.co/guide/en/logstash/7.4/configuration-file-structure.html#codec) No [`enable_metric`](https://www.elastic.co/guide/en/logstash/current/plugins-outputs-csv.html#plugins-outputs-csv-enable_metric) [boolean](http://www.elastic.co/guide/en/logstash/7.4/configuration-file-structure.html#boolean) No [`id`](https://www.elastic.co/guide/en/logstash/current/plugins-outputs-csv.html#plugins-outputs-csv-id) [string](http://www.elastic.co/guide/en/logstash/7.4/configuration-file-structure.html#string) No 九. Codec插件插件一览表 Plugin Description Github repository [avro](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-avro.html) Reads serialized Avro records as Logstash events [logstash-codec-avro](https://github.com/logstash-plugins/logstash-codec-avro) [cef](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-cef.html) Reads the ArcSight Common Event Format (CEF). [logstash-codec-cef](https://github.com/logstash-plugins/logstash-codec-cef) [cloudfront](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-cloudfront.html) Reads AWS CloudFront reports [logstash-codec-cloudfront](https://github.com/logstash-plugins/logstash-codec-cloudfront) [cloudtrail](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-cloudtrail.html) Reads AWS CloudTrail log files [logstash-codec-cloudtrail](https://github.com/logstash-plugins/logstash-codec-cloudtrail) [collectd](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-collectd.html) Reads events from the `collectd` binary protocol using UDP. [logstash-codec-collectd](https://github.com/logstash-plugins/logstash-codec-collectd) [dots](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-dots.html) Sends 1 dot per event to `stdout` for performance tracking [logstash-codec-dots](https://github.com/logstash-plugins/logstash-codec-dots) [edn](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-edn.html) Reads EDN format data [logstash-codec-edn](https://github.com/logstash-plugins/logstash-codec-edn) [edn_lines](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-edn_lines.html) Reads newline-delimited EDN format data [logstash-codec-edn_lines](https://github.com/logstash-plugins/logstash-codec-edn_lines) [es_bulk](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-es_bulk.html) Reads the Elasticsearch bulk format into separate events, along with metadata [logstash-codec-es_bulk](https://github.com/logstash-plugins/logstash-codec-es_bulk) [fluent](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-fluent.html) Reads the `fluentd` `msgpack` schema [logstash-codec-fluent](https://github.com/logstash-plugins/logstash-codec-fluent) [graphite](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-graphite.html) Reads `graphite` formatted lines [logstash-codec-graphite](https://github.com/logstash-plugins/logstash-codec-graphite) [gzip_lines](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-gzip_lines.html) Reads `gzip` encoded content [logstash-codec-gzip_lines](https://github.com/logstash-plugins/logstash-codec-gzip_lines) [jdots](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-jdots.html) Renders each processed event as a dot [core plugin](https://github.com/elastic/logstash/blob/7.4/logstash-core/src/main/java/org/logstash/plugins/codecs/Dots.java) [java_line](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-java_line.html) Encodes and decodes line-oriented text data [core plugin](https://github.com/elastic/logstash/blob/7.4/logstash-core/src/main/java/org/logstash/plugins/codecs/Line.java) [java_plain](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-java_plain.html) Processes text data with no delimiters between events [core plugin](https://github.com/elastic/logstash/blob/7.4/logstash-core/src/main/java/org/logstash/plugins/codecs/Plain.java) [json](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-json.html) Reads JSON formatted content, creating one event per element in a JSON array [logstash-codec-json](https://github.com/logstash-plugins/logstash-codec-json) [json_lines](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-json_lines.html) Reads newline-delimited JSON [logstash-codec-json_lines](https://github.com/logstash-plugins/logstash-codec-json_lines) [line](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-line.html) Reads line-oriented text data [logstash-codec-line](https://github.com/logstash-plugins/logstash-codec-line) [msgpack](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-msgpack.html) Reads MessagePack encoded content [logstash-codec-msgpack](https://github.com/logstash-plugins/logstash-codec-msgpack) [multiline](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-multiline.html) Merges multiline messages into a single event [logstash-codec-multiline](https://github.com/logstash-plugins/logstash-codec-multiline) [netflow](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-netflow.html) Reads Netflow v5 and Netflow v9 data [logstash-codec-netflow](https://github.com/logstash-plugins/logstash-codec-netflow) [nmap](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-nmap.html) Reads Nmap data in XML format [logstash-codec-nmap](https://github.com/logstash-plugins/logstash-codec-nmap) [plain](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-plain.html) Reads plaintext with no delimiting between events [logstash-codec-plain](https://github.com/logstash-plugins/logstash-codec-plain) [protobuf](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-protobuf.html) Reads protobuf messages and converts to Logstash Events [logstash-codec-protobuf](https://github.com/logstash-plugins/logstash-codec-protobuf) [rubydebug](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-rubydebug.html) Applies the Ruby Awesome Print library to Logstash events [logstash-codec-rubydebug](https://github.com/logstash-plugins/logstash-codec-rubydebug) 十. 插件管理Logstash 插件是使用 Ruby开发的，Logstash 从很早的1.5.0+版开始，其插件模块和核心模块便分开维护，其插件使用的是 RubyGems包管理器来管理维护。所以 Logstash插件本质上就是自包含的RubyGems。RubyGems（简称 gems）是一个用于对 Ruby组件进行打包的 Ruby 打包系统。 它提供一个分发 Ruby 程序和库的标准格式，还提供一个管理程序包安装的工具。插件的名字格式：logstash-{input/output/filter}-插件名 示例：filter中的date插件：logstash-filter-date1. 安装插件#以安装dissect插件为例 /usr/share/logstash/bin/logstash-plugin install 插件名 #参数详解： --path.plugins 指定安装路径 2. 查看已安装的插件/usr/share/logstash/bin/logstash-plugin list #参数详解： --verbose 查看插件的版本 --verbose 查看组（input, filter, codec, output）下面的所有插件。例如查看filter下的所有插件 3. 更新插件#更新某个插件 /usr/share/logstash/bin/logstash-plugin update 插件名 #更新全部插件 /usr/share/logstash/bin/logstash-plugin update 4. 卸载插件/usr/share/logstash/bin/logstash-plugin remove 插件名 5. 给插件管理器设置代理export HTTP_PROXY=http://127.0.0.1:3128 6. 修改插件仓库地址Logstash插件默认仓库地址是：http://rubygems.org有一些开源的插件仓库：Geminabox：https://github.com/geminabox/geminabox Gemirro：https://github.com/PierreRambaud/gemirro Gemfury：https://gemfury.com/ Artifactory：http://www.jfrog.com/open-source/ 编辑/usr/share/logstash/Gemfile，将source \"https://rubygems.org\"改为source \"https://my.private.repository\"Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:06 "},"origin/logstash-采集MySQL慢查询日志到Elasticsearch.html":{"url":"origin/logstash-采集MySQL慢查询日志到Elasticsearch.html","title":"Pipeline示例--采集MySQL慢查询日志到Elasticsearch","keywords":"","body":"Logstash采集MySQL慢查询日志到Elasticsearch一、原始日志数据---------------------------------------标识分隔符（源文件中不存在）---------------------------------------------------- /opt/app/mysql/bin/mysqld, Version: 8.0.15 (MySQL Community Server - GPL). started with: Tcp port: 0 Unix socket: /opt/logs/mysql/mysql.sock Time Id Command Argument /opt/app/mysql/bin/mysqld, Version: 8.0.15 (MySQL Community Server - GPL). started with: Tcp port: 3306 Unix socket: /opt/logs/mysql/mysql.sock Time Id Command Argument ---------------------------------------标识分隔符（源文件中不存在）---------------------------------------------------- # Time: 2019-08-20T05:08:37.928071Z # User@Host: root[root] @ [172.17.89.18] Id: 1038 # Query_time: 2.100371 Lock_time: 1.263743 Rows_sent: 0 Rows_examined: 0 SET timestamp=1566277715; -- Dumping database structure for curiouser_alert_rule DROP DATABASE IF EXISTS `curiouser_alert_rule`; ---------------------------------------标识分隔符（源文件中不存在）---------------------------------------------------- # Time: 2019-09-02T02:56:05.166482Z # User@Host: root[root] @ [172.17.88.142] Id: 38433 # Query_time: 0.526962 Lock_time: 0.000101 Rows_sent: 1000 Rows_examined: 1000 SET timestamp=1567392964; /* ApplicationName=DataGrip 2019.2.1 */ select * from curiouser_notification.notif_send_records order by id desc limit 1000; ---------------------------------------标识分隔符（源文件中不存在）---------------------------------------------------- # Time: 2019-09-24T04:52:37.816164Z # User@Host: root[root] @ [172.17.0.115] Id: 88405 # Query_time: 0.622215 Lock_time: 0.000149 Rows_sent: 0 Rows_examined: 1 SET timestamp=1569300757; UPDATE xxl_job_registry SET `update_time` = NOW() WHERE `registry_group` = 'EXECUTOR' AND `registry_key` = 'metadata' AND `registry_value` = '192.168.215.94:9999'; ---------------------------------------标识分隔符（源文件中不存在）-------------------------------------------------- # Time: 2019-09-02T04:45:00.840024Z # User@Host: root[root] @ [172.17.0.113] Id: 38663 # Query_time: 0.557195 Lock_time: 0.000000 Rows_sent: 0 Rows_examined: 0 use curiouser_alert_rule; SET timestamp=1567399500; commit; Note：只摘录了几种典型格式的日志二、PipelineNote：Input插件将指定分割的多行数据变成一行放到一个Event的message中供filter插件处理input { file{ path => \"/root/logs/mysql-log/test.log\" start_position => \"beginning\" codec => multiline { # 以\"# Time:\"为分隔符，中间的所有多行内容归为一行并填充到Event时间中 pattern => \"^# Time:\" negate => true what => \"previous\" # 指定最多读取多少行，默认500行（以防执行初始数据库数据sql语句超过默认行） max_lines => 20000 } } } filter { grok { # 在使用codec/multiline搭配使用的时候，需要注意，grok和普通正则一样默认是不支持匹配回车换行的。就像你需要=～//m一样也需要单独指定，具体写法是在表达式开始位置加(?m)标记 match => { \"message\" => \"(?m)^# Time:.*\\s+#\\s+User@Host:\\s+%{USER:user}\\[[^\\]]+\\]\\s+@\\s+(?:(?\\S*) )?\\[(?:%{IPV4:clientip})?\\]\\s+Id:\\s+%{NUMBER:row_id:int}\\n#\\s+Query_time:\\s+%{NUMBER:Query_time:float}\\s+Lock_time:\\s+%{NUMBER:lock_time:float}\\s+Rows_sent:\\s+%{NUMBER:Row_sent:int}\\s+Rows_examined:\\s+%{NUMBER:Rows_examined:int}\\n\\s*(?:use %{DATA:database};\\s*\\n)?SET\\s+timestamp=%{NUMBER:timestamp};\\n\\s*(?(?\\w+)\\b.*)$\" } # 对于能匹配上面Grok正则的message就删除掉，不能匹配会原始保留 remove_field => [ \"message\" ] } #mutate { # gsub => [ \"sql\", \"\\n# Time: \\d+\\s+\\d+:\\d+:\\d+\", \"\" ] #} date { match => [ \"timestamp\", \"UNIX\" ] remove_field => [ \"timestamp\" ] } } output { #stdout { } elasticsearch { id => \"logstash_mysqlslowsql\" hosts => [\"localhost:9200\"] index=>\"mysql-test-log-%{+YYYY.MM.dd}\" document_type => \"_doc\" http_compression => true user => \"elastic\" password => \"elastic\" } #file{ # path => \"/root/logs/mysql-log/test-out.log\" #} } # ---------------------------------如果以\"^# User@Host:“为分隔符时的Grok正则表达式------------------------------ match => { \"message\" => \"(?m)^# User@Host: %{USER:User}\\[[^\\]]+\\] @ (?:(?\\S*) )?\\[(?:%{IP:Client_IP})?\\]\\s.*# Query_time: %{NUMBER:Query_Time:float}\\s+Lock_time: %{NUMBER:Lock_Time:float}\\s+Rows_sent: %{NUMBER:Rows_Sent:int}\\s+Rows_examined: %{NUMBER:Rows_Examined:int}\\s*(?:use %{DATA:Database};\\s*)?SET timestamp=%{NUMBER:timestamp};\\s*(?(?\\w+)\\s+.*)\\n# Time:.*$\" } match => { \"(?m)^#\\s+User@Host:\\s+%{USER:user}\\[[^\\]]+\\]\\s+@\\s+(?:(?\\S*) )?\\[(?:%{IPV4:clientip})?\\]\\s+Id:\\s+%{NUMBER:row_id:int}\\n#\\s+Query_time:\\s+%{NUMBER:Query_time:float}\\s+Lock_time:\\s+%{NUMBER:lock_time:float}\\s+Rows_sent:\\s+%{NUMBER:Row_sent:int}\\s+Rows_examined:\\s+%{NUMBER:Rows_examined:int}\\n\\s*(?:use %{DATA:database};\\s*\\n)?SET\\s+timestamp=%{NUMBER:timestamp};\\n-{0,2}\\s*(?(?\\w+)\\b.*;)\\s*(?:\\n#\\s+Time)?.*$\" } 三、日志数据经logstash处理后的数据格式{\"message\":\"/opt/app/mysql/bin/mysqld, Version: 8.0.15 (MySQL Community Server - GPL). started with:\\nTcp port: 0 Unix socket: /opt/logs/mysql/mysql.sock\\nTime Id Command Argument\\n/opt/app/mysql/bin/mysqld, Version: 8.0.15 (MySQL Community Server - GPL). started with:\\nTcp port: 3306 Unix socket: /opt/logs/mysql/mysql.sock\\nTime Id Command Argument\",\"host\":\"allinone.tools.curiouser.com\",\"path\":\"/root/logs/mysql-log/test.log\",\"tags\":[\"multiline\",\"_grokparsefailure\"],\"@timestamp\":\"2019-10-11T03:48:55.402Z\",\"@version\":\"1\"} {\"message\":\"# Time: 2019-08-20T05:08:37.928071Z\\n# User@Host: root[root] @ [172.17.89.18] Id: 1038\\n# Query_time: 2.100371 Lock_time: 1.263743 Rows_sent: 0 Rows_examined: 0\\nSET timestamp=1566277715;\\n-- Dumping database structure for curiouser_alert_rule\\nDROP DATABASE IF EXISTS `curiouser_alert_rule`;\",\"host\":\"allinone.tools.curiouser.com\",\"path\":\"/root/logs/mysql-log/test.log\",\"tags\":[\"multiline\",\"_grokparsefailure\"],\"@timestamp\":\"2019-10-11T03:48:55.444Z\",\"@version\":\"1\"} {\"message\":\"# Time: 2019-09-02T02:56:05.166482Z\\n# User@Host: root[root] @ [172.17.88.142] Id: 38433\\n# Query_time: 0.526962 Lock_time: 0.000101 Rows_sent: 1000 Rows_examined: 1000\\nSET timestamp=1567392964;\\n/* ApplicationName=DataGrip 2019.2.1 */ select * from curiouser_notification.notif_send_records order by id desc limit 1000;\",\"host\":\"allinone.tools.curiouser.com\",\"path\":\"/root/logs/mysql-log/test.log\",\"tags\":[\"multiline\",\"_grokparsefailure\"],\"@timestamp\":\"2019-10-11T03:48:55.447Z\",\"@version\":\"1\"} {\"host\":\"allinone.tools.curiouser.com\",\"row_id\":88405,\"tags\":[\"multiline\"],\"clientip\":\"172.17.0.115\",\"@timestamp\":\"2019-09-24T04:52:37.000Z\",\"@version\":\"1\",\"user\":\"root\",\"sql\":\"UPDATE xxl_job_registry\\n SET `update_time` = NOW()\\n WHERE `registry_group` = 'EXECUTOR'\\n AND `registry_key` = 'metadata'\\n AND `registry_value` = '192.168.215.94:9999';\",\"Query_time\":0.622215,\"path\":\"/root/logs/mysql-log/test.log\",\"action\":\"UPDATE\",\"Row_sent\":0,\"lock_time\":1.49E-4,\"Rows_examined\":1} {\"host\":\"allinone.tools.curiouser.com\",\"row_id\":38663,\"tags\":[\"multiline\"],\"clientip\":\"172.17.0.113\",\"@timestamp\":\"2019-09-02T04:45:00.000Z\",\"@version\":\"1\",\"user\":\"root\",\"sql\":\"commit;\",\"database\":\"curiouser_alert_rule\",\"Query_time\":0.557195,\"path\":\"/root/logs/mysql-log/test.log\",\"action\":\"commit\",\"Row_sent\":0,\"lock_time\":0.0,\"Rows_examined\":0} 四、日志数据在Elasticsearch中存储的结构{ \"_index\": \"mysql-test-log-2019.09.24\", \"_type\": \"_doc\", \"_id\": \"klPvuG0BV8kuVZccHbmj\", \"_version\": 1, \"_score\": null, \"_source\": { \"host\": \"allinone.tools.curiouser.com\", \"row_id\": 88405, \"tags\": [ \"multiline\" ], \"clientip\": \"172.17.0.115\", \"@timestamp\": \"2019-09-24T04:52:37.000Z\", \"@version\": \"1\", \"user\": \"root\", \"sql\": \"UPDATE xxl_job_registry\\n SET `update_time` = NOW()\\n WHERE `registry_group` = 'EXECUTOR'\\n AND `registry_key` = 'metadata'\\n AND `registry_value` = '192.168.215.94:9999';\", \"Query_time\": 0.622215, \"path\": \"/root/logs/mysql-log/test.log\", \"action\": \"UPDATE\", \"Row_sent\": 0, \"lock_time\": 0.000149, \"Rows_examined\": 1 }, \"fields\": { \"@timestamp\": [ \"2019-09-24T04:52:37.000Z\" ] }, \"sort\": [ 1569300757000 ] } 五、问题：有几种特殊格式的日志能采集到，但无法格式解析(时间戳会是以采集时间为准) 日志文件中最后一条最新的日志在logstash退出时才进行了收集 附录：1、filter中grok插件的正则^ : 匹配输入字符串的开始位置 \\s : 匹配任何不可见字符，包括空格、制表符、换页符等等。等价于[ \\f\\n\\r\\t\\v] \\S : 匹配任何可见字符。等价于 \\f\\n\\r\\t\\v \\n : 匹配一个换行符。等价于\\x0a和\\cJ \\b : 匹配一个单词边界，也就是指单词和空格间的位置（即正则表达式的“匹配”有两种概念，一种是匹配字符，一种是匹配位置，这里的\\b就是匹配位置的）。例如，“er\\b”可以匹配“never”中的“er”，但不能匹配“verb”中的“er” \\w : 匹配包括下划线的任何单词字符。类似但不等价于“[A-Za-z0-9_]”，这里的\"单词\"字符使用Unicode字符集 默认的正则匹配模式：%{NUMBER:row_id:int} 匹配模式:字段名:数值类型自定义的正则匹配模式： (?the pattern here)2、MySQL开启慢查询日志# slow_query_log 慢查询开启状态 # slow_query_log_file 慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录） # long_query_time 查询超过多少秒才记录 # 查看慢查询相关参数 mysql> show variables like 'slow_query%'; mysql> show variables like 'long_query_time'; # 全局变量设置 mysql> set global slow_query_log='ON'; mysql> set global slow_query_log_file='/usr/local/mysql/data/slow.log'; mysql> set global long_query_time=1; # 修改配置文件my.cnf [mysqld] slow_query_log = ON slow_query_log_file = /usr/local/mysql/data/slow.log long_query_time = 1 # 使用mysqldumpslow和mysqlsla分析mysql慢查询日志 # mysqldumpslow 慢日志分析工具 -s 按照那种方式排序 c：访问计数 l：锁定时间 r:返回记录 al：平均锁定时间 ar：平均访问记录数 at：平均查询时间 -t 是top n的意思，返回多少条数据。-g 可以跟上正则匹配模式，大小写不敏感。 # 得到返回记录最多的20个sql mysqldumpslow -s r -t 20 sqlslow.log # 得到平均访问次数最多的20条sql mysqldumpslow -s ar -t 20 sqlslow.log # 得到平均访问次数最多,并且里面含有ttt字符的20条sql mysqldumpslow -s ar -t 20 -g \"ttt\" sqldlow.log # 如果出现 -bash: mysqldumpslow: command not found 错误，请执行\"ln -s /usr/local/mysql/bin/mysqldumpslow /usr/bin\" # 如果出现如下错误，Died at /usr/bin/mysqldumpslow line 161, <> chunk 405659.说明你要分析的sql日志太大了，请拆分后再分析 5.5版本慢查询日志# Time: 180810 8:45:12 # User@Host: select[select] @ [10.63.253.59] # Query_time: 1.064555 Lock_time: 0.000054 Rows_sent: 1 Rows_examined: 319707 SET timestamp=1533861912; SELECT COUNT(*) FROM hs_forum_thread t WHERE t.`fid`='50' AND t.`displayorder`>='0'; 5.6版本慢查询日志# Time: 160928 18:36:08 # User@Host: root[root] @ localhost [] Id: 4922 # Query_time: 5.207662 Lock_time: 0.000085 Rows_sent: 1 Rows_examined: 526068 use db_name; SET timestamp=1475058968; select count(*) from redeem_item_consume where id 5.7版本慢查询日志# Time: 2018-07-09T10:04:14.666231Z # User@Host: bbs_code[bbs_code] @ [10.82.9.220] Id: 9304381 # Query_time: 5.274805 Lock_time: 0.000052 Rows_sent: 0 Rows_examined: 2 SET timestamp=1531130654; SELECT * FROM pre_common_session WHERE sid='Ba1cSC' OR lastactivity 慢查询日志异同点：每个版本的Time字段格式都不一样 相较于5.6、5.7版本，5.5版本少了Id字段 use db语句不是每条慢日志都有的 可能会出现像下边这样的情况，慢查询块# Time：下可能跟了多个慢查询语句# Time: 160918 2:00:03 # User@Host: dba_monitor[dba_monitor] @ [10.63.144.82] Id: 968 # Query_time: 0.007479 Lock_time: 0.000181 Rows_sent: 172 Rows_examined: 344 SET timestamp=1474135203; SELECT table_schema as 'DB',table_name as 'TABLE',CONCAT(ROUND(( data_length + index_length ) / ( 1024 * 1024 *1024 ), 2), '') as 'TOTAL',TABLE_COMMENT FROM information_schema.TABLES ORDER BY data_length + index_length DESC; # User@Host: dba_monitor[dba_monitor] @ [10.63.144.82] Id: 969 # Query_time: 0.003303 Lock_time: 0.000395 Rows_sent: 233 Rows_examined: 233 SET timestamp=1474135203; select TABLE_SCHEMA,TABLE_NAME,COLUMN_NAME,ORDINAL_POSITION,COLUMN_TYPE,ifnull(COLUMN_COMMENT,0) from COLUMNS where table_schema not in ('mysql','information_schema','performance_schema','test'); 参考链接https://my.oschina.net/lics/blog/916618 https://blog.51cto.com/fengwan/1758920 https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:06 "},"origin/elasticsearch-基础知识.html":{"url":"origin/elasticsearch-基础知识.html","title":"基础知识","keywords":"","body":"Elasticsearch基础知识Document 文档Elasticsearch 是面向Document文档的，文档是所有可搜索数据的最小单位 文档会被序列化JSON格式，保存在Elasticsearch中JSON对象由字段组成 每个字段都有对应的字段类型 (字符串/数值/布尔/日期/二进制/范围类型) 每个文档都有一个Unique ID手动指定ID 通过Elasticsearch自动生成 一个文档由Meta Data元数据与Source Data原始数据组成 { \"_index\": \"***\", # 文档所存在的索引名 \"_type\": \"_doc\", # 文档所属的类型名 \"_id\": \"***\", # 文档的唯一ID \"_version\": 1, # 文档的版本信息 \"_score\": null, # 文档相关性打分 \"_source\": { .... }, # 文档的原始JSON数据 \"fields\": { \"***\": [ \"***\" ] }, # 额外添加的字段 \"sort\": [ 1575256044058 ] # 排序 } Index 索引索引，即一系列documents的集合。Shard 分片分片是独立的，对于一个Search Request的行为，每个分片都会执行这个Request。分片分为两种类型：主分片（Primary Shard）和副本分片（Replica Shard）主分片：用以解决数据水平扩展的问题，通过主分片，可以将数据分布到集群内的所有节点上(主从复制)主分片在索引创建时指定，后续不允许修改，除非reindex 一个分片是一个运行的Lucene实例，Integer.MAX_VALUE - 128 = 2,147,483,519 个docs。 副本分片：用于解决数据高可用的问题，是主分片的拷贝（可以提高读吞吐量）副本分片数，可动态调整 主分片和备分片不会出现在同一个节点上（防止单点故障集群节点类型一个节点就是一个ElasticSearch的实例，本质上就是一个Java进程。每个节点都有名字，通过配置文件，或者启动时候 -E node.name = node1指定每一个节点在启动之后，会分配一个UID，保存在data目录下生产环境中一个节点应该设置单一的角色（意味着节点可以多角色） 节点类型 配置参数 默认值 作用 备注 master eligible node.master true 每个节点启动后，默认就是一个Master eligible节点（可以设置node.master:false 禁止）,Master-eligible节点可以参与选主流程，成为Master节点每个节点上都保存了集群的状态信息(所有节点信息，所有的索引和其相关的Mapping和Setting信息，分片路由信息)，只有Master节点可以修改集群状态信息 可以参加选主 data node.data true 当第一个节点启动，它会将自己选举成Master节点保存包含索引文档的分片数据，执行CRUD、搜索、聚合相关的操作。属于内存、CPU、IO密集型，对硬件资源要求高。 存储数据 ingest node.ingest True ingest节点可以运行一些pipeline的脚本 Coordinating 无 负责接收Client请求，将请求分发到合适的节点，最终把结果汇聚在一起返回给客户端。每个节点默认都起到了Coordinating Node的职责 每个节点默认都是coordinating节点，设置其他类型全部为false machine learning node.ml true(需要enable x-pack) 机器学习 集群状态ES集群状态有三种：Green：所有主分片和备份分片都准备就绪（分配成功），即使有一台机器挂了（假设一台机器一个实例），数据都不会丢失，但会变成Yellow状态 Yellow：所有主分片准备就绪，但存在至少一个主分片（假设是A）对应的备份分片没有就绪，此时集群属于警告状态，意味着集群高可用和容灾能力下降，如果刚好A所在的机器挂了，并且你只设置了一个备份（已处于未就绪状态），那么A的数据就会丢失（查询结果不完整），此时集群进入Red状态 Red：：至少有一个主分片没有就绪（直接原因是找不到对应的备份分片成为新的主分片）,此时查询的结果会出现数据丢失（不完整） Elasticsearch的写入请求Elasticsearch的写入请求主要包括：index、create、update、delete、bulk。bulk是实现对前四种的批量操作。 在6.x版本以后实际上走的都是bulk接口了。 create/index是直接新增doc，delete是直接根据_id删除doc。 ES的任意节点都可以作为协调节点(coordinating node)接受请求，当协调节点接受到请求后进行一系列处理，然后通过_routing字段找到对应的primary shard，并将请求转发给primary shard, primary shard完成写入后，将写入并发发送给各replica， raplica执行写入操作后返回给primary shard， primary shard再将请求返回给协调节点Elasticsearch写入过程Elasticsearch中每个index由多个shard组成，默认是5个，每个shard分布在不同的机器上。shard分为主分片和副本分片。 ​ 红色：Client Node（客户端节点）绿色：Primary Node（主分片节点）蓝色：Replica Node（副本分片节点）Elasticsearch索引过程Elasticsearch搜索过程Elasticsearch的准实时Elasticsearch的核心优势就是近乎实时，为什么说是近乎实时而非真实意义上的实时呢，因为Elasticsearch能够做到准实时，而并不是完全的实时。Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-12-03 21:50:57 "},"origin/elasticsearch--_cat-API.html":{"url":"origin/elasticsearch--_cat-API.html","title":"_cat","keywords":"","body":"Elasticsearch _cat APIs官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/cat.html查看_cat API支持的所有EndpointGET /_cat curl -XGET http://127.0.0.1:9200/_cat /_cat/allocation /_cat/shards /_cat/shards/{index} /_cat/master /_cat/nodes /_cat/tasks /_cat/indices /_cat/indices/{index} /_cat/segments /_cat/segments/{index} /_cat/count /_cat/count/{index} /_cat/recovery /_cat/recovery/{index} /_cat/health /_cat/pending_tasks /_cat/aliases /_cat/aliases/{alias} /_cat/thread_pool /_cat/thread_pool/{thread_pools} /_cat/plugins /_cat/fielddata /_cat/fielddata/{fields} /_cat/nodeattrs /_cat/repositories /_cat/snapshots/{repository} /_cat/templates 查询Endpoint参数GET /_cat/health?help curl -XGET \"http://127.0.0.1:9200/_cat/health?help\" # 参数全称 | 参数缩写 | 参数详解 ---------------------------------------------------------------------------------------------------- epoch | t,time | seconds since 1970-01-01 00:00:00 timestamp | ts,hms,hhmmss | time in HH:MM:SS cluster | cl | cluster name status | st | health status node.total | nt,nodeTotal | total number of nodes node.data | nd,nodeData | number of nodes that can store data shards | t,sh,shards.total,shardsTotal | total number of shards pri | p,shards.primary,shardsPrimary | number of primary shards relo | r,shards.relocating,shardsRelocating | number of relocating nodes init | i,shards.initializing,shardsInitializing | number of initializing nodes unassign | u,shards.unassigned,shardsUnassigned | number of unassigned shards pending_tasks | pt,pendingTasks | number of pending tasks max_task_wait_time | mtwt,maxTaskWaitTime | wait time of longest task pending active_shards_percent | asp,activeShardsPercent | active number of shards in percent 使用参数控制查询条件GET /_cat/health?h=st,t #带表头 GET /_cat/health?v&h=st,t 控制查询的输出排序GET _cat/indices?v&h=index,store.size,creation.date&s=store.size:desc,creation.date #查询出来的Index将会以store.size的大小降序输出。只输出Index名，store.size大小，创建时间戳 curl -XGET \"http://elasticsearch-service.logger.svc:9200/_cat/indices?v&h=index,store.size,creation.date&s=store.size:desc,creation.date\" 控制查询的输出格式GET _cat/indices?v&h=index,store.size,creation.date&s=store.size:desc,creation.date&format=yaml yaml - index: \"test-test-2019.05.21\" store.size: \"4.1gb\" creation.date: \"1558432572904\" - index: \".monitoring-es-7-2019.06.17\" store.size: \"1.2gb\" creation.date: \"1560729605158\" json[ { \"index\" : \"test-test-2019.05.21\", \"store.size\" : \"4.1gb\", \"creation.date\" : \"1558432572904\" }, { \"index\" : \".monitoring-es-7-2019.06.17\", \"store.size\" : \"1.2gb\", \"creation.date\" : \"1560729605158\" } ] text (default)index store.size creation.date test-test-2019.05.21 4.1gb 1558432572904 .monitoring-es-7-2019.06.17 1.2gb 1560729605158 cbor smile Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:51:58 "},"origin/elasticsearch-index-api.html":{"url":"origin/elasticsearch-index-api.html","title":"index","keywords":"","body":"Elasticsearch的Index API一、DDL 数据定义(索引的创建与删除)数据定义语言：Data Definition Language1. 创建索引PUT /index_name?pretty ======================================================== curl -sk -u username:userpassword -XPUT \"http://localhost:9200/index_name?pretty\" 2. 删除IndexDELET /index_name ======================================================== curl -sk -u username:userpassword -XDELETE \"http://127.0.0.1:9200/index_name\" 二、DCL 数据控制(索引的配置)数据控制语言：Data Control Language1. 查看索引的设置GET /index_name/_settings ======================================================== curl -sk -u username:userpassword \"http://127.0.0.1:9200/index_name/_settings\" 2. 查看索引的MappingGET /index_name/_mapping ======================================================== curl -sk -u username:userpassword \"http://127.0.0.1:9200/index_name/_mapping\" 3. 设置索引MappingPUT /index_name { \"mappings\": { \"index_name\": { \"dynamic\":\"false\", \"properties\": { \"id\": { \"type\": \"long\" }, \"prd_id\": { \"type\": \"long\" }, \"mer_id\": { \"type\": \"long\" }, \"data_status\": { \"type\": \"text\" }, \"datachange_createtime\": { \"type\": \"date\", \"format\": \"strict_date_optional_time||epoch_millis\" }, \"datachange_lasttime\": { \"type\": \"date\", \"format\": \"strict_date_optional_time||epoch_millis\" } } } } } ======================================================== curl -sk -u username:userpassword -XPUT \"http://127.0.0.1:9200/index_name\" -H 'Content-Type: application/json' -d' { \"mappings\": { \"index_name\": { \"dynamic\":\"false\", \"properties\": { \"id\": { \"type\": \"long\" }, \"prd_id\": { \"type\": \"long\" }, \"mer_id\": { \"type\": \"long\" }, \"data_status\": { \"type\": \"text\" }, \"datachange_createtime\": { \"type\": \"date\", \"format\": \"strict_date_optional_time||epoch_millis\" }, \"datachange_lasttime\": { \"type\": \"date\", \"format\": \"strict_date_optional_time||epoch_millis\" } } } } }' 三、DML 数据操作(文档的增、删、改)数据操作语言：Data Manipulation Language1. 向索引中插入一个文档向索引中插入一个ID为1的文档PUT /index_name/_doc/1 { \"name\": \"test\" } ======================================================== curl -sk -u username:userpassword \\ -XPUT \"http://localhost:9200/index_name/_doc/1\" \\ -H 'Content-Type: application/json' \\ -d'{ \"name\": \"test\" }' 2. 向索引中批量插入文档详见Elasticsearch索引文档批量操作3. 更新指定文档PUT /index_name/_doc/1?pretty { \"doc\": {\"name\": \"test1\"} } ======================================================== curl -sk -u username:userpassword \\ -XPUT \"http://localhost:9200/index_name/_doc/1\" \\ -H 'Content-Type: application/json' \\ -d ' { \"doc\": {\"name\": \"test1\"} } ' 4. 指定文档新增字段PUT /index_name/_doc/1?pretty { \"doc\": {\"name\": \"test1\"，\"new_field\": \"testN\"} } ======================================================== curl -sk -u username:userpassword \\ -XPUT \"http://localhost:9200/index_name/_doc/1\" \\ -H 'Content-Type: application/json' \\ -d ' { \"doc\": {\"name\": \"test1\"，\"new_field\": \"testN\"} } ' 三、DQL 数据查询 (文档的查询)数据查询语言：Data Query Language1. 查询索引中的所有文档只显示前十条GET /index_name/_search?pretty ======================================================== curl -sk -u username:userpassword \"http://localhost:9200/index_name/_search?pretty\" 2. 查询_id为1的文档GET /index_name/_doc/1?pretty ======================================================== curl -sk -u username:userpassword \"http://localhost:9200/index_name/_doc/1?pretty\" 3. 查询_id为1的文档的元数据GET index_name/_doc/1/_source ======================================================== curl -sk -u username:userpassword \"http://localhost:9200/index_name/_doc/1/_source?pretty\" 4. 查询符合指定条件的文档GET /index_name/_search?q=name:test1 ======================================================== curl -sk -u username:userpassword \"http://localhost:9200/index_name/_search?q=name:test1\" 5. 复杂查询GET /employee/_doc/_search { \"query\" : { \"bool\": { \"must\": { \"match\" : { \"last_name\" : \"smith\" } }, \"filter\": { \"range\" : { \"age\" : { \"gt\" : 30 } } } } } } #这条语句翻译成sql：select * from employee where last_name='smith' and age > 40 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-16 17:20:54 "},"origin/elasticsearch-bulk-api.html":{"url":"origin/elasticsearch-bulk-api.html","title":"bulk","keywords":"","body":"Elasticsearch索引文档的批量操作API：_bulk一、简介官方文档1. API请求URL格式POST /_bulk { \"index动作\": { \"_index\" : \"索引名\", \"_id\" : \"文档ID\" } }{ \"字段名\" : \"字段值\" } { \"delete动作\": { \"_index\" : \"索引名\", \"_id\" : \"文档ID\" } } { \"create动作\": { \"_index\" : \"索引名\", \"_id\" : \"文档ID\" } }{ \"字段名\" : \"字段值\" } { \"update动作\": { \"_index\" : \"索引名\", \"_id\" : \"文档ID\" } }{ \"doc\" : { \"字段名\" : \"字段值\" } } POST /索引名/_bulk {\"index\":{\"_id\":\"文档ID\"}}{ \"字段名\": \"字段值\" } {\"index\":{\"_id\":\"文档ID\"}}{ \"字段名\": \"字段值\" } 2. 支持的文档操作动作index如果索引中已经存在具有相同名称的文档，则创建失败，索引将根据需要添加或替换文档 create如果索引中已经存在具有相同名称的文档，则创建失败，索引将根据需要添加或替换文档 delete不期望下一行有文档数据。具有与标准delete API相同的语义 update期望在下一行中指定部分文档、upsert和脚本及其选项 3. 将文档操作数据存储在文本文本格式动作及元数据\\n 数据\\n 动作及元数据\\n 数据\\n .... 动作及元数据\\n 数据\\n 例如操作数据文本test.json数据如下：{\"index\": {\"_index\": \"test\", \"_type\": \"_doc\", \"_id\": 1}} {\"doc\": {\"name\": \"test1\"}} {\"index\": {\"_index\": \"test\", \"_type\": \"_doc\", \"_id\": 2}} {\"doc\": {\"name\": \"test2\"}} ======================================================================== {\"index\":{\"_id\":\"1\"}} { \"name\": \"test1\" } {\"index\":{\"_id\":\"2\"}} { \"name\": \"test2\" } {\"index\":{\"_id\":\"3\"}} { \"name\": \"test3\" } 操作API的Curl命令curl -X POST \"localhost:9200/_bulk\" -H 'Content-Type: application/json' --data-binary @test.json ======================================================================== curl -X POST \"localhost:9200/test/_bulk\" -H 'Content-Type: application/json' --data-binary @test.json 4. 注意事项批量操作的响应可能是很大的JSON数据，其中包含执行的每个操作的结果，显示的顺序与请求中出现的操作顺序相同。单个操作的失败不会影响其余操作。 批量操作的响应中没有标识操作成功的计数字段 二、API请求的参数三、Update动作的参数doc (partial document) upsert doc_as_upsert script params (for script) lang (for script) _source POST _bulk { \"update\" : {\"_id\" : \"1\", \"_index\" : \"index1\", \"retry_on_conflict\" : 3} } { \"doc\" : {\"field\" : \"value\"} } { \"update\" : { \"_id\" : \"0\", \"_index\" : \"index1\", \"retry_on_conflict\" : 3} } { \"script\" : { \"source\": \"ctx._source.counter += params.param1\", \"lang\" : \"painless\", \"params\" : {\"param1\" : 1}}, \"upsert\" : {\"counter\" : 1}} { \"update\" : {\"_id\" : \"2\", \"_index\" : \"index1\", \"retry_on_conflict\" : 3} } { \"doc\" : {\"field\" : \"value\"}, \"doc_as_upsert\" : true } { \"update\" : {\"_id\" : \"3\", \"_index\" : \"index1\", \"_source\" : true} } { \"doc\" : {\"field\" : \"value\"} } { \"update\" : {\"_id\" : \"4\", \"_index\" : \"index1\"} } { \"doc\" : {\"field\" : \"value\"}, \"_source\": true} 四、操作示例1. 向指定索引批量插入文档Kibana Dev Tools ConsolePOST _bulk { \"index\" : { \"_index\" : \"test\", \"_id\" : \"1\" } }{ \"name\" : \"test1\" } { \"index\" : { \"_index\" : \"test\", \"_id\" : \"2\" } }{ \"name\" : \"test2\" } { \"index\" : { \"_index\" : \"test\", \"_id\" : \"3\" } }{ \"name\" : \"test3\" } ======================================================================== POST /test/_bulk {\"index\":{\"_id\":\"1\"}}{ \"name\": \"test1\" } {\"index\":{\"_id\":\"2\"}}{ \"name\": \"test2\" } {\"index\":{\"_id\":\"3\"}}{ \"name\": \"test3\" } Curl命令 curl -XPOST \"http://localhost:9200/_bulk\" \\ -H 'Content-Type: application/json' \\ -d ' { \"index\" : { \"_index\" : \"test\", \"_id\" : \"1\" } }{ \"name\" : \"test1\" } { \"index\" : { \"_index\" : \"test\", \"_id\" : \"2\" } }{ \"name\" : \"test2\" } { \"index\" : { \"_index\" : \"test\", \"_id\" : \"3\" } }{ \"name\" : \"test3\" } ' ======================================================================== curl -XPOST \"http://localhost:9200/test/_bulk\" \\ -H 'Content-Type: application/json' \\ -d ' {\"index\":{\"_id\":\"1\"}}{ \"name\": \"test1\" } {\"index\":{\"_id\":\"2\"}}{ \"name\": \"test2\" } {\"index\":{\"_id\":\"3\"}}{ \"name\": \"test3\" } ' 2. 针对索引文档进行批量操作Kibana Dev Tools ConsolePOST _bulk { \"index\" : { \"_index\" : \"test\", \"_id\" : \"1\" } }{ \"field1\" : \"value1\" } { \"delete\" : { \"_index\" : \"test\", \"_id\" : \"2\" } } { \"create\" : { \"_index\" : \"test\", \"_id\" : \"3\" } }{ \"field1\" : \"value3\" } { \"update\" : { \"_index\" : \"test\", \"_id\" : \"1\" } }{ \"doc\" : { \"field2\" : \"value2\"} } Curl命令 curl -X POST \"localhost:9200/_bulk?pretty\" \\ -H 'Content-Type: application/json' \\ -d ' { \"index\" : { \"_index\" : \"test\", \"_id\" : \"1\" } } { \"field1\" : \"value1\" } { \"delete\" : { \"_index\" : \"test\", \"_id\" : \"2\" } } { \"create\" : { \"_index\" : \"test\", \"_id\" : \"3\" } } { \"field1\" : \"value3\" } { \"update\" : { \"_index\" : \"test\", \"_id\" : \"1\" } } { \"doc\" : {\"field2\" : \"value2\"} } ' Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-16 17:20:54 "},"origin/elasticsearch-ingest节点.html":{"url":"origin/elasticsearch-ingest节点.html","title":"Ingest节点","keywords":"","body":"ElasticSearch的Ingest角色节点一、简介Elasticsearch集群中的每一个节点有着各自的角色，不同的功能，共同保证集群存储分片、分词索引、聚合搜索等功能。Master节点：负责集群相关的操作，例如创建或删除索引，跟踪哪些节点是集群的一部分，以及决定将哪些分片分配给哪些节点。 拥有稳定的主节点是衡量集群健康的重要标志。 Data节点：保存包含索引文档的分片数据，执行CRUD、搜索、聚合相关的操作。属于内存、CPU、IO密集型，对硬件资源要求高。 Coordinating节点：每一个节点都默认设置为了协调节点。 搜索请求或大容量索引请求可能涉及不同数据节点上的数据。例如，搜索请求是分两个阶段执行的，由接收客户端请求的节点(即协调节点)进行协调。在分散阶段，协调节点将请求转发给持有数据的数据节点。每个数据节点在本地执行请求并将结果返回给协调节点。在收集阶段，协调节点将每个数据节点的结果简化为单个全局结果集。 Ingest节点：可以看作是数据前置处理转换的节点。在实际的文档索引发生之前，Ingest节点会拦截批量和索引请求，然后使用ingest Pipeline对文档进行过滤、转换等数据转换预处理操作，然后将文档传递回索引或批量API。类似于 logstash 中 filter 的作用。Ingest是5.X版本就有的特性 Ingest节点是通过包含多个processor的pipeline对文档进行预处理操作，processor是实际处理数据的插件。 默认情况下，所有节点都启用Ingest角色，因此任何节点都可以处理Ingest任务 可以创建专用的Ingest节点 要禁用节点的Ingest功能，需要在elasticsearch.yml 设置\"node.ingest：false\" 二、Ingest Pipeline与Logstash FilterLogstash处理数据的流程：logstash在pipeline filter中设置不同的插件对从Input传过来的数据进行加工处理，再输出带output中。Easticsearch Ingest Pipeline节点处理数据的流程：Ingest Pipeline是Ingest节点上用于 Logstash Filter Ingest Pipeline 支持的数据源 大量的输入和输出插件（比如：kafka，redis等）可供使用 不能从外部来源（例如消息队列或数据库）提取数据，必须批量bulk或索引index请求将数据推送到 Elasticsearch 应对数据激增的能力不同 Logstash 可在本地对数据进行缓冲以应对采集骤升情况。Logstash 支持与大量不同的消息队列类型进行集成。 极限情况下会出现：在长时间无法联系上 Elasticsearch 或者 Elasticsearch 无法接受数据的情况下，均有可能会丢失数据。 处理能力不同 支持的插件和功能点较Ingest节点多很多。 支持为数不多处理器操作。Ingest节点管道只能在单一事件的上下文中运行。Ingest通常不能调用其他系统或者从磁盘中读取数据。 排他式功能支持不同 支持采集附件处理器插件，此插件可用来处理和索引常见格式（例如 PPT、XLS 和 PDF）的附件。 不支持如上文件附件类型。 三、Ingest Pipeline1. Ingest Pipeline的定义及使用Ingest Pipeline中每个processor实现了对文档的某种转换，如移除某个字段，重命名某个字段等操作。pipeline定义语法格式如下：PUT _ingest/pipeline/my-pipeline-id { \"description\" : \"...\", # Pipeline功能描述(必须，string类型) \"version\" : 123, # 用于管理ingest pipeline的版本号(可选，Integer类型) \"processors\" : [ ... ] # 指定1个或多个processor(必须，数组类型) } 要使用某个pipeline，只需要在请求中简单的指定pipeline的id就可以了：PUT my-index/_doc/doc_id?pipeline=my_pipeline_id { \"a\": \"b\", \"foo\": \"bar\" } 2. Ingest Pipeline的管理API① Put添加或更新PipelinePUT /_ingest/pipeline/my-pipeline-id { \"description\" : \"describe pipeline\", \"version\" : 123, \"processors\" : [ { \"set\" : { \"field\": \"foo\", \"value\": \"bar\" } } ] } ② Get查看指定的PipelineGET _ingest/pipeline/my-pipeline-id 查看Pipeline的指定参数，例如查看Pipeline的版本号字段GET /_ingest/pipeline/my-pipeline-id?filter_path=*.version ③ Delete删除指定PipelineDELETE /_ingest/pipeline/my-pipeline-id 删除模糊匹配的PipelineDELETE /_ingest/pipeline/pipeline-* 删除所有PipelineDELETE /_ingest/pipeline/* ④ Simulate调用Ingest pipeline对指定的文档进行模拟测试。可以指定一个现有的Ingest pipeline来对提供的文档进行模拟测试，也可以在请求体中提供Ingest pipeline定义。POST _ingest/pipeline/_simulate { \"pipeline\": { \"description\": \"template\", \"processors\": [ { \"set\": { \"field\": \"{{name}}\", \"value\": \"{{name}} {{age}} {{birth}}\" } }, { \"set\": { \"field\": \"time\", \"value\": \"{{_ingest.timestamp}}\" } } ] }, \"docs\": [ { \"_index\": \"simulate_test\", \"_source\": { \"name\": \"kyle\", \"age\": 18, \"birth\": \"1993-09-01\" } }, { \"_index\": \"simulate_test\", \"_source\": { \"name\": \"reason\", \"age\": 20, \"birth\": \"1990-02-03\" } } ] } 模拟测试调用已经创建的Ingest PipelinePOST /_ingest/pipeline/my-pipeline-id/_simulate { \"docs\": [ { \"_index\": \"index\", \"_id\": \"id\", \"_source\": { \"foo\": \"bar\" } }, { \"_index\": \"index\", \"_id\": \"id\", \"_source\": { \"foo\": \"rab\" } } ] } docs(必须, 数组)字段支持的参数:_index：(可选, string类型) 包含文档的索引名 _id：(可选, string类型) 文档的唯一标识 _source：(必须, json对象) 文档的JSON数据 3. Index Setting设置索引默认Ingest Pipeline可在Index Setting中设置“index.default_pipeline”参数指定默认Ingest Pipeline。如果Index Setting设置了默认Ingest Pipeline,但Ingest Pipeline不存在，索引请求将会失败。参数值设为_none则表示不使用Ingest Pipeline进行文档预处理PUT test/_settings { \"number_of_replicas\": 0, \"index\":{ \"analysis.analyzer.default.type\":\"ik_max_word\", \"analysis.search_analyzer.default.type\":\"ik_smart\", \"default_pipeline\": \"my-pipeline-id\" } } 四、Ingest Pipeline中的ProcessorsProcessor的配置格式如下{ \"PROCESSOR_NAME\" : { ... processor configuration options ... } } 所有Processors支持以下通用参数 tag：只是Pipeline中特定Processors实例化的字符串标识符。tag字段不影响处理Processors的行为，但是对于特定Processors的记录和跟踪错误非常有用。 on_failure：用于设置Pipeline或Processor中的异常处理。详情见Ingest Pipeline的异常处理 if：设置判断条件来决定Processors是否处理符合条件的文档。详情见Processor中的条件判断 1. Processor获取、处理文档中的字段数据获取文档_source 原始数据中的字段{ \"set\": { \"field\": \"my_field\", \"value\": 582.1 } } # 或者 { \"set\": { \"field\": \"_source.my_field\", \"value\": 582.1 } } 获取文档Metadata元数据中的字段 Processor可直接处理文档Metadata元数据中_index,_type,_id,_routingElasticsearch不允许原始数据_source中的字段与Metadata元数据中的字段相同{ \"set\": { \"field\": \"_id\", \"value\": \"1\" } } 获取Ingest的元数据字段 除了文档Metadata元数据和_source原始数据中的字段外，Processor可以在文档处理过程中添加与Ingest相关的元数据。Ingest元数据是暂时的，在文档被管道处理之后就会丢失，因此不会被索引。例如ingest会在在_ingest下添加了ingest时间戳，用于标识对文档进行预处理的时间，获取方式如下：# 该示例添加了一个名称为received的字段。该值是es收到index 或 bulk 请求预处理文档的时间。 { \"set\": { \"field\": \"received\", \"value\": \"{{_ingest.timestamp}}\" } } 2. Processor中的条件判断Ingest pipeline的processor支持if判断来决定是否处理指定条件的文档。if字段必须包含返回布尔值的脚本。如果脚本的计算结果为true，那么将为给定的文档执行Processor，否则将跳过它。 Ingest pipeline processor中的if判断语句会被解释为elasticsearch官方支持的“Painless script”格式脚本 if字段使用脚本选项中定义的脚本字段作为对象，并通过脚本处理程序中脚本使用的相同的ctx变量访问文档的只读版本。 ① 在判断条件中获取文档中的嵌套字段在文档中原始数据_source下有大量的嵌套JSON数据，那如何在Processor中的条件获取中嵌套较深的字段数据呢？可使用“a.b.c”这种形式获取。 如果原始数据中没有a.b存在，条件语句会抛出“NullPointerExceptions”的异常，可在Processor的条件判断引用字段时使用“?.” PUT _ingest/pipeline/drop_guests_network { \"processors\": [ { \"drop\": { \"if\": \"ctx.network?.name == 'Guest'\" } } ] } ② 复杂的条件判断例如可以在drop processor中，判断原始数据某个数组类型的字段中是否包含\"prod\"特殊字符PUT _ingest/pipeline/not_prod_dropper { \"processors\": [ { \"drop\": { \"if\": \"\"\" Collection tags = ctx.tags; if(tags != null){ for (String tag : tags) { if (tag.toLowerCase().contains('prod')) { return false; } } } return true; \"\"\" } } ] } # 以下文档会被丢弃 POST test/_doc/1?pipeline=not_prod_dropper { \"tags\": [\"application:myapp\", \"env:Stage\"] } # 以下文档不会被丢弃 POST test/_doc/2?pipeline=not_prod_dropper { \"tags\": [\"application:myapp\", \"env:Production\"] } ③ 判断条件的正则表达式如果要在if条件中使用正则表达式，需要在elasticsearch.yml中设置script.painless.regex.enabled: truePUT _ingest/pipeline/check_url { \"processors\": [ { \"set\": { \"if\": \"ctx.href?.url =~ /^http[^s]/\", \"field\": \"href.insecure\", \"value\": true } } ] } PUT _ingest/pipeline/check_url { \"processors\": [ { \"set\": { \"if\": \"ctx.href?.url != null && ctx.href.url.startsWith('http://')\", \"field\": \"href.insecure\", \"value\": true } } ] } ④ Pipeline Processor中的条件判断可在Pipeline Processor中设置判断条件来决定是否调用其他PipelinePUT _ingest/pipeline/logs_pipeline { \"description\": \"A pipeline of pipelines for log files\", \"version\": 1, \"processors\": [ { \"pipeline\": { \"if\": \"ctx.service?.name == 'apache_httpd'\", \"name\": \"httpd_pipeline\" } }, { \"pipeline\": { \"if\": \"ctx.service?.name == 'syslog'\", \"name\": \"syslog_pipeline\" } }, { \"fail\": { \"message\": \"This pipeline requires service.name to be either `syslog` or `apache_httpd`\" } } ] } 3. 内置的ProcessorsAppend Processor Bytes Processor Circle Processor Convert Processor Date Processor Date Index Name Processor Dissect Processor Dot Expander Processor Drop Processor Fail Processor Foreach Processor GeoIP Processor Grok Processor Gsub Processor HTML Strip Processor Join Processor JSON Processor KV Processor Lowercase Processor Pipeline Processor Remove Processor Rename Processor Script Processor Set Processor Set Security User Processor Split Processor Sort Processor Trim Processor Uppercase Processor URL Decode Processor User Agent processor 4. 自定义processors自定义的processors必须让所有elasticsearch节点都要安装，在elasticsearch.yml中添加“plugin.mandatory：ingest-attachment”五、Ingest Pipeline的异常处理针对一些比较复杂的Pipeline，其中可能定义了多个Processor进行文档处理，而这些Processor是按照顺序执行，如果在执行过程中一个遇到了异常，后续processor将不会执行，这是不可取的。可以在pipeline或processor语法块中使用on_failure参数进行异常捕获。 如果在processor语法块指定了on_failure配置，不管它是否为空，processor抛出的任何异常都会被捕获，而Pipeline将继续执行其他的processor。 因为可以在on_failure语句的范围内定义更多的处理器，所以可以嵌套失败处理。 同时也可以设置\"on_failure\": true进行忽略异常，而不做任何处理{ \"description\" : \"my first pipeline with handled exceptions\", \"processors\" : [ { \"rename\" : { \"field\" : \"foo\", \"target_field\" : \"bar\", \"ignore_failure\" : true } } ] } 以下Ingest Pipeline在rename processor中设置了当文档中没有指定字段\"foo\"时，会在异常处理参数中使用set processor添加\"error\"字段{ \"description\" : \"my first pipeline with handled exceptions\", \"processors\" : [ { \"rename\" : { \"field\" : \"foo\", \"target_field\" : \"bar\", \"on_failure\" : [ { \"set\" : { \"field\" : \"error\", \"value\" : \"field \\\"foo\\\" does not exist, cannot rename to \\\"bar\\\"\" } } ] } } ] } 以下Ingest Pipeline在全局定义块中设置了当匹pipeline其中processor处理抛出异常，整个pipeline出错时，会在异常处理参数中使用set processor添加\"_index\"字段{ \"description\" : \"my first pipeline with handled exceptions\", \"processors\" : [ ... ], \"on_failure\" : [ { \"set\" : { \"field\" : \"_index\", \"value\" : \"failed-{{ _index }}\" } } ] } 六、Filebeat Modules模块的Ingest Pipeline以Filebeat Nginx模块处理访问日志的Ingest PIpeline为例，文件路径：/usr/share/filebeat/module/nginx/access/ingest/default.json{ \"description\": \"Pipeline for parsing Nginx access logs. Requires the geoip and user_agent plugins.\", \"processors\": [ { \"grok\": { \"field\": \"message\", \"patterns\": [ \"\\\"?(?:%{IP_LIST:nginx.access.remote_ip_list}|%{DATA:source.address}) - %{DATA:user.name} \\\\[%{HTTPDATE:nginx.access.time}\\\\] \\\"%{DATA:nginx.access.info}\\\" %{NUMBER:http.response.status_code:long} %{NUMBER:http.response.body.bytes:long} \\\"%{DATA:http.request.referrer}\\\" \\\"%{DATA:user_agent.original}\\\"\" ], \"pattern_definitions\": { \"IP_LIST\": \"%{IP}(\\\"?,?\\\\s*%{IP})*\" }, \"ignore_missing\": true } }, { \"grok\": { \"field\": \"nginx.access.info\", \"patterns\": [ \"%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}\", \"\" ], \"ignore_missing\": true } }, { \"remove\": { \"field\": \"nginx.access.info\" } }, { \"split\": { \"field\": \"nginx.access.remote_ip_list\", \"separator\": \"\\\"?,?\\\\s+\", \"ignore_missing\": true } }, { \"split\": { \"field\": \"nginx.access.origin\", \"separator\": \"\\\"?,?\\\\s+\", \"ignore_missing\": true } }, { \"set\": { \"field\": \"source.ip\", \"value\": \"\" } }, { \"script\": { \"lang\": \"painless\", \"source\": \"boolean isPrivate(def dot, def ip) { try { StringTokenizer tok = new StringTokenizer(ip, dot); int firstByte = Integer.parseInt(tok.nextToken()); int secondByte = Integer.parseInt(tok.nextToken()); if (firstByte == 10) { return true; } if (firstByte == 192 && secondByte == 168) { return true; } if (firstByte == 172 && secondByte >= 16 && secondByte 参考文档https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html https://blog.csdn.net/laoyang360/article/details/93376355 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-12-02 22:47:36 "},"origin/elasticsearch-7.1的xpack权限控制.html":{"url":"origin/elasticsearch-7.1的xpack权限控制.html","title":"Xpack","keywords":"","body":"一、Context之前ELK套装安装X-Pack的安全功能时，只有安装30天的试用许可证时间，以允许访问所有功能。 当许可证到期时，X-Pack将以降级模式运行。可以购买订阅以继续使用X-Pack组件的全部功能（https://www.elastic.co/subscriptions）。但是,最近官方从6.8.0和7.1.0开始免费提供安全功能. 本次实验,所有ELK组件版本均为7.1.0,以容器单节点运行二. Elasticsearch开启Xpackelasticsearch的容器化部署参考笔记: ElasticSearch的容器化部署.md 配置参数可以通过环境变量的方式注入,主要的几个环境变量参数xpack.monitoring.collection.enabled(开启自我监控): true path.repo(设置snapshot存储仓库的路径): /usr/share/elasticsearch/snapshots-repository discovery.type(设置当前节点为单节点模式): single-node cluster.name(设置elasticsearch的集群名): curiouser bootstrap.memory_lock: 'true' TZ(设置时区): Asia/Shanghai ES_JAVA_OPTS(设置elasticsearch的JVM堆栈大小): '-Xms1g -Xmx2g' ELASTIC_USERNAME: \"kibana\" ELASTIC_PASSWORD: \"kibana\" xpack.security.enabled: 'true' xpack.security.transport.ssl.enabled: \"true\" xpack.security.transport.ssl.verification_mode: \"certificate\" xpack.security.transport.ssl.keystore.path: \"/usr/share/elasticsearch/config/certs/elastic-certificates.p12\" xpack.security.transport.ssl.truststore.path: \"/usr/share/elasticsearch/config/certs/elastic-certificates.p12\" xpack.security.http.ssl.enabled: \"false\" 查看elasticsearch是否开启xpack的安全验证 curl -XGET 'localhost:9200/_cat/health?v&pretty' # curl -XGET \"http://127.0.0.1:9200/_cat/health?v&pretty\" # 使用上述命令会返回401,提示未授权验证,使用以下命令进行安全验证地访问 curl --user kibana:****kibana用户的密码**** -XGET 'localhost:9200/_cat/health?v&pretty' 三、Kibana开启Xpackkibana的容器化部署详见笔记: Kibana的容器化部署.md 配置参数可以通过环境变量的方式注入,主要的几个环境变量参数:ELASTICSEARCH_USERNAME: kibana用户 ELASTICSEARCH_PASSWORD: kibana用户的随机密码 TZ(设置时区): Asia/Shanghai 镜像中默认指定的elasticsearch地址为:http://elasticsearch:9200,刚好在open shift中部署的elasticsearch的svc名为\"elasticsearch\",它的访问方式为:http://elasticsearch:9200或者http://elasticsearch.命名空间.svc:9200 登录Kibana进行验证使用elastic 超级用户进行登录，密码来自 setup-passwords 命令输出的结果 四、Logstash开启Xpack配置logstash发送监控数据到elasticsearch xpack.monitoring.elasticsearch.hosts: \"http://elasticsearch:9200\" xpack.monitoring.enabled: \"true\" xpack.monitoring.elasticsearch.username: \"logstash_system\" xpack.monitoring.elasticsearch.password: \"***logstash_system用户的密码****\" 在kibana中查看logstash的监控数据 在kibana中创建logstash-pipeline角色,授予\"manage_index_template\",\"monitor\"的集群权限和\"write\",\"delete\",\"create_index\",\"manage_ilm\",\"manage\"的Index权限,然后绑定到logstash-pipeline用户上,用以创建Index并向其中写入数据 在pipeline的elasticsearch output插件中设置用户和密码 output{ elasticsearch{ hosts => \"elasticsearch:9200\" index => \"%{AppID}-%{+YYYY.MM.dd}\" user => \"logstash-pipeline\" password => \"****logstash-pipeline用户密码****\" } } 查看logstash的pipeline是否将数据写入的elasticsearch 附录：Kibana上的角色权限Cluster相关的角色权限 角色权限 权限描述 all Privileges to create snapshots for existing repositories. Can also list and view details on existing repositories and snapshots. create_snapshot Privileges to create snapshots for existing repositories. Can also list and view details on existing repositories and snapshots. manage Builds on `monitor` and adds cluster operations that change values in the cluster. This includes snapshotting, updating settings, and rerouting. It also includes obtaining snapshot and restore status. This privilege does not include the ability to manage security. manage_ccr All cross-cluster replication operations related to managing follower indices and auto-follow patterns. It also includes the authority to grant the privileges necessary to manage follower indices and auto-follow patterns. This privilege is necessary only on clusters that contain follower indices. manage_data_frame_transforms All operations on index templates. manage_ilm All operations on index templates. manage_index_templates All operations on index templates. manage_ingest_pipelines All operations on ingest node pipelines. manage_ml All machine learning operations, such as creating and deleting datafeeds, jobs, and model snapshots.Note：Datafeeds that were created prior to version 6.2 or created when security features were disabled run as a system user with elevated privileges, including permission to read all indices. Newer datafeeds run with the security roles of the user who created or updated them. manage_pipeline All operations on ingest pipelines. manage_rollup All rollup operations, including creating, starting, stopping and deleting rollup jobs. manage_saml Enables the use of internal Elasticsearch APIs to initiate and manage SAML authentication on behalf of other users. manage_security All security-related operations such as CRUD operations on users and roles and cache clearing. manage_token All security-related operations on tokens that are generated by the Elasticsearch Token Service. manage_watcher All watcher operations, such as putting watches, executing, activate or acknowledging.Note：Watches that were created prior to version 6.1 or created when the security features were disabled run as a system user with elevated privileges, including permission to read and write all indices. Newer watches run with the security roles of the user who created or updated them. monitor All cluster read-only operations, like cluster health and state, hot threads, node info, node and cluster stats, and pending cluster tasks. monitor_data_frame_transforms All read-only operations related to data frames. monitor_ml All read-only machine learning operations, such as getting information about datafeeds, jobs, model snapshots, or results. monitor_rollup All read-only rollup operations, such as viewing the list of historical and currently running rollup jobs and their capabilities. monitor_watcher All read-only watcher operations, such as getting a watch and watcher stats. read_ccr All read-only cross-cluster replication operations, such as getting information about indices and metadata for leader indices in the cluster. It also includes the authority to check whether users have the appropriate privileges to follow leader indices. This privilege is necessary only on clusters that contain leader indices. read_ilm All read-only index lifecycle management operations, such as getting policies and checking the status of index lifecycle management transport_client All privileges necessary for a transport client to connect. Required by the remote cluster to enable Cross Cluster Search. Index相关的角色权限 角色权限 权限描述 all Any action on an index create Privilege to index documents. Also grants access to the update mapping action.NoteThis privilege does not restrict the index operation to the creation of documents but instead restricts API use to the index API. The index API allows a user to overwrite a previously indexed document. create_index Privilege to create an index. A create index request may contain aliases to be added to the index once created. In that case the request requires the manage privilege as well, on both the index and the aliases names. delete Privilege to delete documents. delete_index Privilege to delete an index. index Privilege to index and update documents. Also grants access to the update mapping action. manage All monitor privileges plus index administration (aliases, analyze, cache clear, close, delete, exists, flush, mapping, open, force merge, refresh, settings, search shards, templates, validate). manage_follow_index All actions that are required to manage the lifecycle of a follower index, which includes creating a follower index, closing it, and converting it to a regular index. This privilege is necessary only on clusters that contain follower indices. manage_ilm All index lifecycle management operations relating to managing the execution of policies of an index This includes operations like retrying policies, and removing a policy from an index. manage_leader_index All actions that are required to manage the lifecycle of a leader index, which includes forgetting a follower. This privilege is necessary only on clusters that contain leader indices. monitor All actions that are required for monitoring (recovery, segments info, index stats and status). read Read-only access to actions (count, explain, get, mget, get indexed scripts, more like this, multi percolate/search/termvector, percolate, scroll, clear_scroll, search, suggest, tv). read_cross_cluster Read-only access to the search action from a remote cluster. view_index_metadata Read-only access to index metadata (aliases, aliases exists, get index, exists, field mappings, mappings, search shards, type exists, validate, warmers, settings, ilm). This privilege is primarily available for use by Kibana users. write Privilege to perform all write operations to documents, which includes the permission to index, update, and delete documents as well as performing bulk operations. Also grants access to the update mapping action. 参考链接https://www.elastic.co/cn/blog/getting-started-with-elasticsearch-security https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html https://www.elastic.co/guide/en/elastic-stack-overview/7.1/get-started-logstash-user.html https://www.elastic.co/guide/en/logstash/current/ls-security.html https://www.elastic.co/guide/en/logstash/current/docker-config.html#docker-env-configCopyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-12-02 21:50:07 "},"origin/elasticSearch-索引的快照备份与恢复.html":{"url":"origin/elasticSearch-索引的快照备份与恢复.html","title":"Snapshots","keywords":"","body":"一、Contextshared file system：NFS S3 HDFS 二、使用NFS作为快照仓库后端存储1. 在es集群中的某一个节点创建NFS文件系统，ES集群节点进行挂载yum install -y nfs-utils rpcbind ;\\ systemctl enable nfs ;\\ systemctl enable rpcbind ;\\ systemctl start nfs ;\\ systemctl start rpcbind ;\\ mkdir -p /data/es/Elastic-SnapShots ;\\ echo \"/data/es/Elastic-SnapShots 172.16.3.0/24(rw,sync,no_root_squash,no_subtree_check) \" >> /etc/exports ;\\ export -r ;\\ showmount -e 127.0.0.1 2. 集群其他节点挂载NFS共享目录yum install nfs-utils -y ;\\ mkdir -p /data/es/Elastic-SnapShots ;\\ echo \"172.16.3.5:/data/es/Elastic-SnapShots /data/es/Elastic-SnapShots nfs defaults 0 0\" >> /etc/fstab ;\\ mount -a ;\\ df -mh 3. 给elasticsearch授予共享目录/data/es/Elastic-SnapShots权限chown -R elasticsearch:elasticsearch /data/es/Elastic-SnapShots 4. ES集群所有节点配置文件设置echo 'path.repo: [\"/data/es/Elastic-SnapShots\"]' >> /etc/elasticsearch/elasticsearch.yml ;\\ systemctl restart elasticsearch;\\ systemctl status elasticsearch 三、使用HDFS作为快照仓库后端存储ES版本：5.6.8 HDFS版本：2.6.01、所有ES节点安装repository-hdfs插件在线安装插件/usr/share/elasticsearch/bin/elasticsearch-plugin install repository-hdfs 离线安装插件，插件下载地址：https://artifacts.elastic.co/downloads/elasticsearch-plugins/repository-hdfs/repository-hdfs-5.6.8.zipwget https://artifacts.elastic.co/downloads/elasticsearch-plugins/repository-hdfs/repository-hdfs-5.6.8.zip ;\\ /usr/share/elasticsearch/bin/elasticsearch-plugin install file:///root/repository-hdfs-5.6.8.zip 2、重启ES集群所有节点systemctl restart elasticsearch ;\\ systemctl status elasticsearch 3、后续创建HDFS类型仓库时遇到的问题ES会以elasticsearch用户(即启动elasticsearch后台进程的用户)在HDFS的/user下创建文件时提示权限不足。所以修改HDFS上/user的权限 hdfs dfs -chmod -R 777 /user 如果HDFS集群在ES集群外面，ES中的Hadoop客户端向通过Hadoop NameNode节点返回的DataNode节点写数据时会找不到DataNode节点。因为创建仓库时只是指定NameNode节点的外网地址，而返回的DataNode节点IP地址是DataNode向NameNode节点注册的内网IP地址，ES集群根本无法访问到。所以打通两者之间的网络。 四、在 kibana 的 Dev Tools 上管理快照仓库1、注册NFS类型的快照仓库PUT /_snapshot/快照仓库名 { \"type\": \"fs\", \"settings\": { \"compress\": true, \"location\": \"/data/es/Elastic-SnapShots\" } } ## settings的其他参数： ​ # chunk_size Big files can be broken down into chunks during snapshotting if needed. The chunk size can be specified in bytes or by using size value notation, i.e. 1g, 10m, 5k. Defaults to null (unlimited chunk size). #max_restore_bytes_per_sec Throttles per node restore rate. Defaults to 40mb per second. #max_snapshot_bytes_per_sec Throttles per node snapshot rate. Defaults to 40mb per second. #readonly Makes repository read-only. Defaults to false. 2、注册HDFS类型的快照仓库PUT _snapshot/快照仓库名 { \"type\": \"hdfs\", \"settings\": { \"uri\": \"hdfs://172.16.3.10:9000\", \"compress\": true, \"path\": \"elasticsearch/respositories\" } } ##settings的其他参数： ​ #uri The uri address for hdfs. ex: \"hdfs://:/\". (Required) #path The file path within the filesystem where data is stored/loaded. ex: \"path/to/file\". (Required) #load_defaults Whether to load the default Hadoop configuration or not. (Enabled by default) #conf. Inlined configuration parameter to be added to Hadoop configuration. (Optional) Only client oriented properties from the hadoop core and hdfs configuration files will be recognized by the plugin. #compress Whether to compress the metadata or not. (Disabled by default) #chunk_size Override the chunk size. (Disabled by default) #security.principal Kerberos principal to use when connecting to a secured HDFS cluster. If you are using a service principal for your elasticsearch node, you may use the _HOST pattern in the principal name and the plugin will replace the pattern with the hostname of the node at runtime (see Creating the Secure Repository). 3、删除快照仓库DELETE /_snapshot/快照仓库名 4、查看所有的快照仓库GET _snapshot/_all 五、快照管理1、创建包含所有Index的全量快照PUT /_snapshot/my_backup/snapshot_1?wait_for_completion=true 2、创建中包含指定索引的快照PUT /_snapshot/快照仓库名/快照名?wait_for_completion=true { \"indices\": \"index-A,index-B\", \"ignore_unavailable\": true, \"include_global_state\": false } 3、查看仓库中所有的快照GET _snapshot/快照仓库名/_all GET _cat/snapshots/快照仓库名 4、删除快照DELETE _snapshot/快照仓库名/快照名 5、查看多个快照的状态GET /_snapshot/快照仓库名/快照1,快照2/_status 6、查看某一个快照状态GET _snapshot/快照仓库名/快照名/_status 7、恢复一个快照POST _snapshot/快照仓库名/快照名/_restore 六、使用 _cat API格式化查询快照仓库中的的快照使用Snapshot API查出来的信息是JSON格式的，后续处理比较麻烦。可使用\"_cat\" API Endpoint格式化查询输出Snapshot仓库中的快照信息。关于\"_cat\" API的详细使用信息详见Elasticsearch的\"_cat\"API1、查看_cat的snapshots API的所有参数GET _cat/snapshots?help 或 curl -XGET \"http://localhost:9200/_cat/snapshots?help\" 名字 简称 描述 id id,snapshot unique snapshot status s,status snapshot name start_epoch ste,startEpoch start time in seconds since 1970-01-01 00:00:00 start_time sti,startTime start time in HH:MM:SS end_epoch ete,endEpoch end time in seconds since 1970-01-01 00:00:00 end_time eti,endTime end time in HH:MM:SS duration dur,duration duration indices i,indices number of indices successful_shards ss,successful_shards number of successful shards failed_shards fs,failed_shards number of failed shards total_shards ts,total_shards number of total shards reason r,reason reason for failures 2、示例例如只查看快照仓库中的快照名并排序GET _cat/snapshots/pvc-snap-repo?h=id&s=id 或 curl -XGET \"http://elasticsearch:9200/_cat/snapshots/pvc-snap-repo?h=id&s=id\" # 返回的结果格式是纯文本的 # apm-7.1.1-metric-2019.07.16-snapshot-2019.07.22 # apm-7.1.1-onboarding-2019.07.16-snapshot-2019.07.22 # apm-7.1.1-span-2019.07.16-snapshot-2019.07.22 # apm-7.1.1-transaction-2019.07.16-snapshot-2019.07.22 # ansi-kpo-tek1269219-h136-2019.07.16-snapshot-2019.07.22 # curiouser-ocp-allinone-audit-2019.08.15-snapshot-2019.08.22 # kibana_sample_data_logs-snapshot-2019.07.22 # springboot2-demo-dev-2019.07.12-snapshot-2019.07.15 # springboot2-demo-dev-2019.07.13-snapshot-2019.07.17 七、更新Elasticsearch 7.2.0新版本有了管理Snapshot Repository的新功能 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:51:58 "},"origin/elasticsearch-插件管理.html":{"url":"origin/elasticsearch-插件管理.html","title":"插件管理","keywords":"","body":"ES自带的有插件管理脚本命令以RPM方式安装的ES，插件管理脚本在/usr/share/elasticsearch/bin/elasticsearch-plugin。该脚本能安装，列出，移除插件$> cd /usr/share/elasticsearch/bin/ $> ./elasticsearch-plugin list #列出所有插件 $> ./elasticsearch-plugin install plugin_name #安装插件 $> ./elasticsearch-plugin remove plugin_name #卸载插件 #该脚本的参数 #-v 输出详细信息 #-s 输出最简信息 The script may return the following exit codes:0 : everything was OK 64 : unknown command or incorrect option parameter 74 : IO error 70 : any other error 设置代理来安装插件$ sudo ES_JAVA_OPTS=\"-Dhttp.proxyHost=代理服务器IP地址 -Dhttp.proxyPort=代理服务器端口 -Dhttps.proxyHost=代理服务器IP地址 -Dhttps.proxyPort=代理服务器端口\" bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.0/elasticsearch-analysis-ik-6.3.0.zip Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:51:59 "},"origin/elasticsearch-optimizing.html":{"url":"origin/elasticsearch-optimizing.html","title":"性能优化","keywords":"","body":"Elasticsearch性能优化一、写入性能优化在文档写入时，会根据_routing来计算（OperationRouting类）得出文档要写入哪个分片。这里的写入请求只会写主分片，当主分片写入成功后，会同时把写入请求发送给所有的副本分片，当副本分片写入成功后，会传回返回信息给主分片，主分片得到所有副本分片的返回信息后，再返回给客户端。在写入时，我们可以在Request自己指定_routing，也可以在Mapping指定文档中的Field值作为_routing。如果没有指定_routing，则会把_id作为_routing进行计算。由于写入时，具有相同_routing的文档一定会分配在同一个分片上，所以如果是自定义的_routing，在查询时，一定要指定_routing进行查询，否则是查询不到文档的。这并不是局限性，恰恰相反，指定_routing的查询，性能上会好很多，因为指定_routing意味着直接去存储数据的shard上搜索，而不会搜索所有shard。二、索引性能优化三、搜索性能优化优化索引速度1. 使用bulk批量操作批量请求将比单文档索引请求产生更好的性能。为了知道批量请求的最佳大小，您应该在具有单个shard的单个节点上运行基准测试。首先尝试一次索引100个文档，然后索引200个，然后索引400个，等等，在每次基准测试运行时将批量请求中的文档数量增加一倍。当索引速度开始趋于稳定时，您就知道已经达到了数据批量请求的最佳大小。2. 查询返回大小尽量使用 Scroll滚动查询API。3. 按照日期规划索引4. 索引分片个数设置5. 索引分片副本数设置6. 禁止大文档禁止单个Document的大小超过默认设置http.max_content_length(默认值100MB)（如果单个doc大小超过了设置值，elasticsearch会直接拒绝索引）。 虽然可修改http.max_content_length参数提高默认doc大小，但 Lucene引擎依旧会有2GB大小的限制 单个大doc会加重网络、内存和磁盘的消耗 7. 禁止节点开启Swapping8. 节点给系统缓存预留内存文件系统缓存将用于缓冲I / O操作9. 文档ID尽量自动生成10. 节点硬件尽量选性能好的11. 提高索引缓存区大小12. 使用多线程分散写入操作使用单个线程发送批处理写入请求13. 调整索引刷新间：refresh_interval默认情况下索引的refresh_interval为1秒,这意味着数据写1秒后就可以被搜索到,每次索引的 refresh 会产生一个新的 lucene 段,这会导致频繁的 segment merge 行为,如果你不需要这么高的搜索实时性,应该降低索引refresh 周期,如:index.refresh_interval: 120s二、优化查询速度三、优化硬盘使用Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-12-03 21:50:57 "},"origin/elasticsearch-问题总结.html":{"url":"origin/elasticsearch-问题总结.html","title":"问题总结","keywords":"","body":"一、elasticsearch集群开启“xpack的monitoring功能“导致”failed to flush export bulks和 there are no ingest nodes in this cluster”报错原因：xpack的monitoring功能需要定义exporter用于导出监控数据， 默认的exporter是local exporter，也就是直接写入本地的集群，并且要求节点开启了ingest选项。解决方案:将集群的结点配置里的ingest角色打开 或者在集群设置elasticsearch.yml里，将local exporter的use ingest关掉 xpack.monitoring.exporters.my_local: type: local use_ingest: false 但一般的，使用local cluster监控自己存在很大的问题，故障发生时，监控也没法看到了。 生产上最好是设置一个单独的监控集群，然后可以配置一个HTTP exporter，将监控数据送往这个监控集群 参考：https://www.elastic.co/guide/en/x-pack/5.5/monitoring-cluster.html#http-exporter-reference https://elasticsearch.cn/question/1915 二、Elasticsearch的监控日志索引Index的保存期限为7天Elasticsearch的监控日志索引Index为\".monitoring-*\"开头的，保存期限为7天，7天之后会自动删除。参考https://discuss.elastic.co/t/how-system-index-like-monitoring-es-6-2018-02-06-are-being-deleted-automatically/119578 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:51:59 "},"origin/prometheus-Kubernetes或Openshift的Prometheus监控体系.html":{"url":"origin/prometheus-Kubernetes或Openshift的Prometheus监控体系.html","title":"Prometheus","keywords":"","body":"Kubernetes或Openshift的Prometheus监控体系一、Overviewcluster-monitoring-operator ：负责在 OpenShfit 环境中部署基于 Prometheus 的监控系统 GIthub：https://github.com/openshift/cluster-monitoring-operator 部署基于 Prometheus 监控系统中的组件 Prometheus Operator Prometheus Alertmanager cluster for cluster and application level alerting kube-state-metrics node_exporter prometheus operator：负责配置、管理Prometheus和AlertmanagerGIthub： https://github.com/coreos/prometheus-operator 相关博客：https://blog.csdn.net/ygqygq2/article/details/83655552 功能：Create/Destroy: 在Kubernetes namespace中更容易启动一个Prometheus实例，一个特定的应用程序或团队更容易使用Operator。 Simple Configuration: 配置Prometheus的基础东西，比如在Kubernetes的本地资源versions, persistence, retention policies, 和replicas。 Target Services via Labels: 基于常见的Kubernetes label查询，自动生成监控target 配置；不需要学习普罗米修斯特定的配置语言。 架构 kube-state-metrics：监听 Kubernetes API server 并自动生成相关对象的metrics信息(并不修改相关对象的配置)，在80端口(默认)暴露出HTTP的endpoint /metricGIthub：https://github.com/kubernetes/kube-state-metrics node-exporter：以Daemonset的形式部署在Openshift集群的各个节点上，采集OS级别的metrics信息 监控的Target Prometheus itself Prometheus-Operator cluster-monitoring-operator Alertmanager cluster instances Kubernetes apiserver kubelets (the kubelet embeds cAdvisor for per container metrics) kube-controllers kube-state-metrics node-exporter etcd (if etcd monitoring is enabled) Note: 1. Prometheus Pod 中，除了 Prometheus 容器外，还有一个 prometheus-config-reloader 容器。它负责导入在需要的时候让Prometheus 重新加载配置文件。 2. 配置文件被以 Secret 形式创建并挂载给 prometheus-config-reloader Pod。一旦配置有变化，它会调用 Prometheus 的接口，使其重新加载配置文件。 相关链接https://docs.okd.io/3.11/install_config/prometheus_cluster_monitoring.html#prometheus-cluster-monitoring http://www.cnblogs.com/sammyliu/p/10155442.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:11 "},"origin/prometheus-Ceph-Exporter对接Prometheus以监控ceph集群.html":{"url":"origin/prometheus-Ceph-Exporter对接Prometheus以监控ceph集群.html","title":"Ceph Exporter","keywords":"","body":"一、Overview由于在Openshift集群外使用了Ceph RBD和Ceph Filesystem作为PV的后端动态存储文件系统，所以ceph的集群监控也可使用Prometheus体系中的Ceph Exporter，接入到Openshift集群中的Prometheus。 二、以DaemonSet的形式部署Ceph Exporter --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: labels: app: ceph-exporter name: ceph-exporter spec: selector: matchLabels: app: ceph-exporter template: metadata: labels: app: ceph-exporter spec: containers: - image: digitalocean/ceph_exporter imagePullPolicy: IfNotPresent name: ceph-exporter ports: - containerPort: 9128 hostPort: 9128 name: http protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/ceph name: ceph-confdir resources: limits: cpu: 200m memory: 400Mi requests: cpu: 100m memory: 200Mi dnsPolicy: ClusterFirst hostNetwork: true hostPID: true nodeSelector: beta.kubernetes.io/os: linux restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: node-exporter　#使用Node-Exporter创建的ServiceAccount serviceAccountName: node-exporter terminationGracePeriodSeconds: 30 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master volumes: - hostPath: path: /etc/ceph #将ceph节点的配置文件路径暴露给exporter type: \"\" name: ceph-confdir templateGeneration: 1 updateStrategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate --- apiVersion: v1 kind: Endpoints metadata: labels: k8s-app: ceph-exporter name: ceph-exporter subsets: - addresses: - ip: 192.168.1.96 nodeName: allinone.okd311.curiouser.com targetRef: kind: Pod ports: - name: http port: 9128 protocol: TCP --- apiVersion: v1 kind: Service metadata: annotations: prometheus.io/port: '9128' prometheus.io/scrape: 'true' labels: k8s-app: ceph-exporter name: ceph-exporter spec: clusterIP: None ports: - name: http port: 9128 protocol: TCP targetPort: http selector: app: ceph-exporter sessionAffinity: None type: ClusterIP --- apiVersion: route.openshift.io/v1 kind: Route metadata: annotations: openshift.io/host.generated: 'true' labels: k8s-app: ceph-exporter name: ceph-exporter spec: port: targetPort: http to: kind: Service name: ceph-exporter weight: 100 wildcardPolicy: None 三、Ceph Exporter对接Prometheus备份Prometheus原配置文件secret Prometheus原始配置secret文件 创建新的Prometheus配置secret在原Prometheus配置文件中添加consul 服务发现和ceph-exporter相关的配置...省略... - job_name: consul-prometheus metrics_path: /monitor/prometheus scrape_interval: 20s scheme: http scrape_timeout: 5s consul_sd_configs: - server: consul-server.consul.svc:8500 services: [] scheme: http allow_stale: true refresh_interval: 20s - job_name: openshift-monitoring/ceph-exporter/0 honor_labels: false kubernetes_sd_configs: - role: endpoints namespaces: names: - openshift-monitoring scrape_interval: 30s scheme: http relabel_configs: - action: keep source_labels: - __meta_kubernetes_service_label_k8s_app regex: ceph-exporter - action: keep source_labels: - __meta_kubernetes_endpoint_port_name regex: http - source_labels: - __meta_kubernetes_namespace target_label: namespace - source_labels: - __meta_kubernetes_pod_name target_label: pod - source_labels: - __meta_kubernetes_service_name target_label: service - source_labels: - __meta_kubernetes_service_name target_label: job replacement: ${1} - source_labels: - __meta_kubernetes_service_label_k8s_app target_label: job regex: (.+) replacement: ${1} - target_label: endpoint replacement: http ...省略... 替换Prometheus的POD secret 四、验证 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:11 "},"origin/sentry.html":{"url":"origin/sentry.html","title":"Sentry日志聚合告警平台","keywords":"","body":"一、简介虽然我们有很多工具可以让开发工作变得更容易，但是发现和排查线上问题的过程仍然在很多时候让我们觉得很痛苦。当生产系统中产生了一个bug时，我们如何快速地得到报警？如何评估它的影响和紧迫性？如何快速地找到问题的根源？当hotfix完修复程序后，又如何知道它是否解决了问题？Sentry在帮助我们与现有流程集成时回答了这些问题。例如，线上有一个bug，代码的某处逻辑的NullPointerException造成了这个问题，Sentry会立即发现错误，并通过邮件或其他基于通知规则的集成通知到相关责任人员，这个通知可以把我们引入到一个指示板，这个指示板为我们提供了快速分类问题所需的上下文，如：频率、用户影响、代码那一部分受到影响以及那个团队可能是问题的所有者。Sentry是一个实时事件的日志聚合平台。Sentry支持的客户端SDK：那么Sentry是如何实现实时日志监控报警的呢？首先，Sentry是一个C/S架构，分为服务端和客户端 。SDK我们需要在自己应用中集成Sentry的SDK才能在应用发生错误是将错误信息发送给Sentry服务端。根据语言和框架的不同，我们可以选择自动或自定义设置特殊的错误类型报告给Sentry服务端。而Sentry的服务端分为web、cron、worker这几个部分，应用（客户端）发生错误后将错误信息上报给web，web处理后放入消息队列或Redis内存队列，worker从队列中消费数据进行处理。官方文档：https://docs.sentry.io/二、部署Sentry 本身是基于 Django 开发的，需要Postgresql、 RedisKubernetes Helm ChartsRedis Charts: https://github.com/helm/charts/tree/master/stable/redis Postgresql Charts: https://github.com/helm/charts/tree/master/stable/postgresql Sentry Charts: https://github.com/helm/charts/tree/master/stable/sentry OpenShift 资源声明文件部署资源声明文件三、配置在Sentry完成一个项目的设置后，您将获得一个我们称之为DSN或数据源名称的值.它看起来很像一个标准的URL，但它实际上只是Sentry SDK所需的配置的标识.它由几个部分组成，包括协议，公共密钥和密钥，服务器地址和项目标识符。'{PROTOCOL}://{PUBLIC_KEY}:{SECRET_KEY}@{HOST}/{PATH}{PROJECT_ID}' 四、Sentry集成LDAP认证登陆由于Sentry没有集成LDAP认证的官方插件，所以推荐了第三方插件sentry-ldap-auth来实现。官方说明文档：https://docs.sentry.io/server/plugins/#rd-party-pluginsSentry-ldap-auth插件地址：https://github.com/Banno/getsentry-ldap-auth第三方制作集成Sentry-ldap-auth插件Docker镜像的GitHub：https://github.com/locaweb/docker-sentry-ldap1、修改默认镜像，添加插件DockerfileFROM docker.io/sentry:9.1.2 RUN apt-get -qq update && DEBIAN_FRONTEND=noninteractive apt-get install -y -q libxslt1-dev libxml2-dev libpq-dev libldap2-dev libsasl2-dev libssl-dev RUN echo \"sentry-ldap-auth\\npython-decouple==3.0\\ndjango-auth-ldap /tmp/req.txt && \\ pip install -r /tmp/req.txt && \\ apt-get remove -y -q libxslt1-dev libxml2-dev libpq-dev libldap2-dev libsasl2-dev libssl-dev && \\ rm -rf /var/lib/apt/lists/* /tmp/req.txt MakefileIMAGE_BASE = harbor.curiouser.cn IMAGE_NAME = tools/sentry-9.1.2-ldap IMAGE_VERSION = v1 all: build build: docker build --rm -f Dockerfile -t ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} . push: docker push ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} 2、修改部署charts中的configmap，添加插件认证代码Sentry Charts目录下templates/configmap.yamlapiVersion: v1 kind: ConfigMap metadata: # .....上文省略...... data: # .....上文省略...... sentry.conf.py: |- # .....上文省略...... #################### # LDAP settings ## #################### SENTRY_USE_LDAP = env('SENTRY_USE_LDAP', False) if SENTRY_USE_LDAP: import ldap from django_auth_ldap.config import LDAPSearch, GroupOfUniqueNamesType AUTH_LDAP_SERVER_URI = env('LDAP_SERVER', 'ldap://localhost：389') AUTH_LDAP_BIND_DN = env('LDAP_BIND_DN', '') AUTH_LDAP_BIND_PASSWORD = env('LDAP_BIND_PASSWORD', '') # 设置搜索用户配置 AUTH_LDAP_USER_SEARCH = LDAPSearch( env('LDAP_USER_DN'), ldap.SCOPE_SUBTREE, env('LDAP_USER_FILTER', '(&(objectClass=inetOrgPerson)(cn=%(user)s))') ) # 设置搜索用户组配置 #AUTH_LDAP_GROUP_SEARCH = LDAPSearch( # env('LDAP_GROUP_DN', ''), # ldap.SCOPE_SUBTREE, # env('LDAP_GROUP_FILTER', '(objectClass=groupOfUniqueNames)') #) if env('LDAP_GROUP_TYPE', '') == 'groupOfUniqueNames': AUTH_LDAP_GROUP_TYPE = GroupOfUniqueNamesType() elif env('LDAP_GROUP_TYPE', '') == 'groupOfNames': AUTH_LDAP_GROUP_TYPE = GroupOfNamesType() elif env('LDAP_GROUP_TYPE', '') == 'posixGroup': AUTH_LDAP_GROUP_TYPE = PosixGroupType() elif env('LDAP_GROUP_TYPE', '') == 'nestedGroupOfNames': AUTH_LDAP_GROUP_TYPE = NestedGroupOfNamesType() AUTH_LDAP_REQUIRE_GROUP = None AUTH_LDAP_DENY_GROUP = None # 用户属性Mapping AUTH_LDAP_USER_ATTR_MAP = { 'username': env('LDAP_SENTRY_USER_FIELD', 'cn'), 'name': env('LDAP_MAP_FULL_NAME', 'sn'), 'email': env('LDAP_MAP_MAIL', 'mail') } # 用户搜索缓存 AUTH_LDAP_FIND_GROUP_PERMS = env('LDAP_FIND_GROUP_PERMS', False) AUTH_LDAP_CACHE_GROUPS = env('LDAP_CACHE_GROUPS', True) AUTH_LDAP_GROUP_CACHE_TIMEOUT = env('LDAP_GROUP_CACHE_TIMEOUT', 3600) # 用户在Sentry中的对应关系 AUTH_LDAP_DEFAULT_SENTRY_ORGANIZATION = env('LDAP_DEFAULT_SENTRY_ORGANIZATION','sentry') AUTH_LDAP_SENTRY_ORGANIZATION_ROLE_TYPE = 'manager' AUTH_LDAP_SENTRY_SUBSCRIBE_BY_DEFAULT = False AUTH_LDAP_SENTRY_ORGANIZATION_GLOBAL_ACCESS = True AUTH_LDAP_SENTRY_USERNAME_FIELD = env('LDAP_SENTRY_USERNAME_FIELD', 'cn') AUTHENTICATION_BACKENDS = AUTHENTICATION_BACKENDS + ('sentry_ldap_auth.backend.SentryLdapBackend',) ldap_is_active = env('LDAP_GROUP_ACTIVE', '') ldap_is_superuser = env('LDAP_GROUP_SUPERUSER', '') ldap_is_staff = env('LDAP_GROUP_STAFF', '') if ldap_is_active or ldap_is_superuser or ldap_is_staff: AUTH_LDAP_USER_FLAGS_BY_GROUP = { 'is_active': ldap_is_active, 'is_superuser': ldap_is_superuser, 'is_staff': ldap_is_staff, } # django_auth_ldap日志输出 import logging logger = logging.getLogger('django_auth_ldap') logger.addHandler(logging.StreamHandler()) ldap_loglevel = getattr(logging, env('LDAP_LOGLEVEL', 'DEBUG')) logger.setLevel(ldap_loglevel) LOGGING['overridable'] = ['sentry', 'django_auth_ldap'] LOGGING['loggers']['django_auth_ldap'] = {'handlers': ['console'],'level': env('LDAP_LOGLEVEL','DEBUG')} # .....下文省略...... 3、在Values文件中添加Web组件部署时的环境变量values-dev.yaml... web: ... env: ... - name: TZ value: Asia/Shanghai - name: SENTRY_USE_LDAP value: 'True' - name: LDAP_SERVER value: 'ldap://openldap.openldap.svc:389' - name: LDAP_BIND_DN value: 'cn=readonly,dc=curiouser,dc=com' - name: LDAP_BIND_PASSWORD value: readonly - name: LDAP_USER_DN value: 'ou=employee,dc=curiouser,dc=com' - name: LDAP_USER_FILTER value: >- (&(memberOf=cn=sentry,ou=apps,dc=curiouser,dc=com)(cn=%(user)s)) - name: LDAP_SENTRY_USER_FIELD value: 'cn' - name: LDAP_MAP_FULL_NAME value: 'sn' - name: LDAP_MAP_MAIL value: 'mail' - name: LDAP_GROUP_TYPE value: groupOfUniqueNames - name: LDAP_GROUP_CACHE_TIMEOUT value: '3600' - name: LDAP_CACHE_GROUPS value: 'True' - name: LDAP_DEFAULT_SENTRY_ORGANIZATION value: sentry - name: LDAP_FIND_GROUP_PERMS value: 'False' - name: LDAP_SENTRY_USERNAME_FIELD value: 'cn' - name: LDAP_LOGLEVEL value: INFO 五、Kubernetes event 客户端镜像GitHub：https://github.com/getsentry/sentry-docs/issues/1330六、Sentry-cli客户端Sentry命令行客户端，通常用于发送一些基本事件到服务端官方文档：https://docs.sentry.io/cli/GitHub地址：https://github.com/getsentry/sentry-cli1. 下载安装手动下载安装github下载地址：https://github.com/getsentry/sentry-cli/releases/ 脚本下载安装curl -sL https://sentry.io/get-cli/ | bash NPM下载安装 # 全局安装 npm install -g @sentry/cli --unsafe-perm Docker Image $ docker pull getsentry/sentry-cli $ docker run --rm -v $(pwd):/work getsentry/sentry-cli --help 2. 配置全局配置文件：~/.sentryclircINI语法格式 [auth] token=your-auth-token 环境变量 默认会读取当前.env 文件加载环境变量。可设置SENTRY_LOAD_DOTENV=0禁止 export SENTRY_AUTH_TOKEN=your-auth-token 命令行参数 sentry-cli --auth-token your-auth-token 项目配置文件 支持加载.properties，也可通过环境变量SENTRY_PROPERTIES指定项目配置文件路径 环境变量的形式 配置文件中的形式 描述 **SENTRY_AUTH_TOKEN** **auth.token** 与Sentry服务端通信用的认证Token **SENTRY_API_KEY** **auth.api_key** The legacy API key for authentication if you have one. **SENTRY_URL** **defaults.url** The URL to use to connect to sentry. This defaults to `https://sentry.io/`. **SENTRY_ORG** **defaults.org** The slug of the organization to use for a command. **SENTRY_PROJECT** **defaults.project** The slug of the project to use for a command. **http.keepalive** This ini only setting is used to control the behavior of the SDK with regards to HTTP keepalives. The default is *true* but it can be set to *false* to disable keepalive support. **http_proxy** **http.proxy_url** The URL that should be used for the HTTP proxy. The standard `http_proxy` environment variable is also honored. Note that it is lowercase. **http.proxy_username** This ini only setting sets the proxy username in case proxy authentication is required. **http.proxy_password*** This ini only setting sets the proxy password in case proxy authentication is required. **http.verify_ssl** This can be used to disable SSL verification when set to false. You should never do that unless you are working with a known self signed server locally. **http.check_ssl_revoke** If this is set to false then SSL revocation checks are disabled on Windows. This can be useful when working with a corporate SSL MITM proxy that does not properly implement revocation checks. Do not use this unless absolutely necessary. **SENTRY_HTTP_MAX_RETRIES** **http.max_retries** Sets the maximum number of retry attempts for upload operations (e.g., uploads of release files and debug symbols). The default is `5`. **ui.show_notifications** If this is set to false some operating system notifications are disabled. This currently primarily affects xcode builds which will not show notifications for background builds. **SENTRY_LOG_LEVEL** **log.level** Configures the log level for the SDK. The default is `warning`. If you want to see what the library is doing you can set it to `info` which will spit out more information which might help to debug some issues with permissions. **dsym.max_upload_size** Sets the maximum upload size in bytes (before compression) of debug symbols into one batch. The default is 35MB or 100MB (depending on the version of sentry-cli) which is suitable for sentry.io but if you are using a different sentry server you might want to change this limit if necessary. **SENTRY_NO_PROGRESS_BAR** If set to `1`, then `sentry-cli` will not display progress bars for any operations. **SENTRY_DISABLE_UPDATE_CHECK** **update.disable_check** If set to `true`, then the automatic update check in sentry-cli is disabled. This was introduced in 1.17. Versions before that did not include an update check. The update check is also not enabled for npm based installations of sentry-cli at the moment. **DEVICE_FAMILY** **device.family** Device family value reported to Sentry. **DEVICE_MODEL** **device.model** Device model value reported to Sentry. 3. 获取Auth Token4. 获取并设置项目DSN创建项目，获取DSN设置DSN环境变量export SENTRY_DSN=https://:@sentry.io/ 5. 验证配置文件$ sentry-cli info Sentry Server: http://sentry-web-sentry.apps.okd311.curiouser.com Default Organization: Sentry Default Project: sentry-cli Authentication Info: Method: Auth Token User: *** Scopes: - event:admin - event:read - member:read - org:read - project:read - project:releases - team:read 6. Sentry-cli命令行参数sentry-cli 1.49.0 Command line utility for Sentry. This tool helps you manage remote resources on a Sentry server like sourcemaps, debug symbols or releases. Use `--help` on the subcommands to learn more about them. USAGE: sentry-cli OPTIONS: --api-key 指定Sentry API key. --auth-token 指定Sentry auth token. -h, --help 打印帮助信息 --log-level 设置日志输出级别(日志级别:TRACE、DEBUG、INFO、WARN、ERROR) --url 指定Sentry服务端地址.[默认：https://sentry.io/] -V, --version 打印版本信息 子命令： bash-hook Prints out a bash script that does error handling. difutil Locate or analyze debug information files. help 显示帮助信息 info 打印Sentry服务端信息 issues Manage issues in Sentry. login Authenticate with the Sentry server. projects 管理sentry项目Manage projects on Sentry. react-native Upload build artifacts for react-native projects. releases Manage releases on Sentry. repos Manage repositories on Sentry. send-event Send a manual event to Sentry. uninstall Uninstall the sentry-cli executable. update Update the sentry-cli executable. upload-dif Upload debugging information files. upload-proguard Upload ProGuard mapping files to a project. 7. 手动发送事件命令行参数$ sentry-cli send-event [选项] NOTE: This command will validate input parameters and attempt to send an event to Sentry. Due to network errors, rate limits or sampling the event is not guaranteed to actually arrive. Check debug output for transmission errors by passing --log-level=debug or setting `SENTRY_LOG_LEVEL=debug`. 选项: -d, --dist Set the distribution. -E, --env 发送事件时一起发送指定的环境变量 -e, --extra ... 给事件添加额外信息 -f, --fingerprint ... 修改事件指纹. -h, --help 打印帮助信息 -l, --level 设置事件的严重程度或日志级别(默认error) --log-level 设置日志数据级别(TRACE, DEBUG, INFO, WARN, ERROR] --logfile Send a logfile as breadcrumbs with the event (last 100 records) -m, --message ... 设置事件消息 -a, --message-arg ... 设置事件参数 --no-environ 设置发送事件时不一起发送系统环境变量 -p, --platform 设置事件所处平台。默认是'other' -r, --release Optional identifier of the release. -t, --tag ... 给事件添加标签 -u, --user ... 给事件添加用户信息 --with-categories Parses off a leading category for breadcrumbs from the logfile 示例 sentry-cli send-event \\ -m \"this is sentry-cli test event\" \\ -t sentry-cli:test \\ --no-environ \\ -l info \\ -p centos \\ -e a:b \\ -e c:d \\ -u root:curiouser \\ --log-level DEBUG\\ -a hahha \\ -E HOSTNAME:$HOSTNAME 事件发送成功后，命令行会返回事件编号8. 事件信息显示Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-12-03 23:24:05 "},"origin/docker-Dockerfile中CMD与ENTRYPOINT命令的区别.html":{"url":"origin/docker-Dockerfile中CMD与ENTRYPOINT命令的区别.html","title":"Dockerfile中CMD与ENTRYPOINT命令的区别","keywords":"","body":"CMD与ENTRYPOINT区别CMD命令设置容器启动后默认执行的命令及其参数，但CMD设置的命令能够被docker run命令后面的命令行参数替换 ENTRYPOINT配置容器启动时的执行命令（不会被忽略，一定会被执行，即使运行 docker run时指定了其他命令） ENTRYPOINT 的 Exec 格式用于设置容器启动时要执行的命令及其参数，同时可通过CMD命令或者命令行参数提供额外的参数 ENTRYPOINT 中的参数始终会被使用，这是与CMD命令不同的一点 1. Shell格式和Exec格式命令Shell格式：指令 CMD java -jar test.jar Exec格式：指令 [\"executable\", \"param1\", \"param2\", ...] ENTRYPOINT [\"java\", \"-jar\", \"test.jar\"] 2. Shell格式和Exec格式命令的区别Shell格式中的命令会直接被Shell解析 Exec格式不会直接解析，需要加参数 3. CMD和ENTRYPOINT指令支持的命令格式CMD 指令的命令支持以下三种格式:Exec格式: CMD [\"executable\",\"param1\",\"param2\"] Exec参数: CMD [\"param1\",\"param2\"] 用来为ENTRYPOINT 提供参数 Shell格式: CMD command param1 param2 ENTRYPOINT 指令的命令支持以下了两种格式:Exec格式：可用使用CMD的参数和可使用docker run [image] 参数后面追加的参数 Shell格式 ：不会使用 CMD参数，可使用docker run [image] 参数后面追加的参数 4. 示例ENTRYPOINT的Exec格式# Dockerfile FROM centos ENTRYPOINT [\"/bin/echo\", \"Hello\"] # 启动容器的命令: docker run -it [image] # 输出: Hello # 启动容器的命令: docker run -it [image] Test # 输出: Hello Test ENTRYPOINT的Exec格式 + CMD的Exec格式# Dockerfile FROM centos ENTRYPOINT [\"/bin/echo\", \"Hello\"] CMD [\"Word\"] # 启动容器的命令: docker run -it [image] # 输出: Hello Word # 启动容器的命令: docker run -it [image] Test # 输出: Hello Test ENTRYPOINT的Exec格式 + CMD的shell格式# Dockerfile FROM centos ENTRYPOINT [\"/bin/echo\", \"Hello\"] CMD Word # 启动容器的命令: docker run -it [image] # 输出: Hello /bin/sh -c Word # 启动容器的命令: docker run -it [image] Test # 输出: Hello Test ENTRYPOINT的shell格式# Dockerfile FROM centos ENTRYPOINT /bin/echo \"Hello\" # 启动容器的命令: docker run -it [image] # 输出: Hello # 启动容器的命令: docker run -it [image] Test # 输出: Hello ENTRYPOINT的shell格式 + CMD的Shell格式# Dockerfile FROM centos ENTRYPOINT /bin/echo \"Hello\" CMD Word # 启动容器的命令: docker run -it [image] # 输出: Hello # 启动容器的命令: docker run -it [image] Test # 输出: Hello ENTRYPOINT的shell格式 +CMD的Exec格式# Dockerfile FROM centos ENTRYPOINT /bin/echo \"Hello\" CMD [\"Word\"] # 启动容器的命令: docker run -it [image] # 输出: Hello # 启动容器的命令: docker run -it [image] Test # 输出: Hello 参考链接https://blog.csdn.net/weixin_42971363/article/details/91506844Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:51:58 "},"origin/docker-使用Makefile操作Dockerfile.html":{"url":"origin/docker-使用Makefile操作Dockerfile.html","title":"使用Makefile操作Dockerfile.md","keywords":"","body":" IMAGE_BASE = docker-registry-default.apps.okd311.curiouser.com/openshift IMAGE_NAME = demo-springboot2 IMAGE_VERSION = latest IMAGE_TAGVERSION = $(GIT_COMMIT) all: build tag push build: docker build --rm -f Dockerfile -t ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} . tag: docker tag ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_TAGVERSION} push: docker push ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} docker push ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_TAGVERSION} makefile中的命令必须以tab作为开头(分隔符),不能用扩展的tab即用空格代替的tab。(如果是vim编辑的话,执行 set noexpandtab)。否则会报如下错误：`Makefile:10: * multiple target patterns. Stop.`**Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:51:58 "},"origin/shell-变量.html":{"url":"origin/shell-变量.html","title":"变量","keywords":"","body":"一. 变量的定义1. 将命令的输出赋予变量var=`shell命令` # `是反引号 2. 变量的参数替换和扩展 表达式 含义 ${var_DEFAULT} 如果var没有被声明, 那么就以$DEFAULT作为其值 * ${var=DEFAULT} 如果var没有被声明, 那么就以$DEFAULT作为其值 * ${var:-DEFAULT} 如果var没有被声明, 或者其值为空, 那么就以$DEFAULT作为其值 * ${var:=DEFAULT} 如果var没有被声明, 或者其值为空, 那么就以$DEFAULT作为其值 * ${var+OTHER} 如果var声明了, 那么其值就是$OTHER, 否则就为null字符串 ${var:+OTHER} 如 果var被设置了, 那么其值就是$OTHER, 否则就为null字符串 ${var?ERR_MSG} 如果var没 被声明, 那么就打印$ERR_MSG* ${var:?ERR_MSG} 如果var没 被设置, 那么就打印$ERR_MSG* ${!varprefix*} 匹配之前所有以varprefix开头进行声明的变量 ${!varprefix@} 匹配之前所有以varprefix开头进行声明的变量 3. 读取标准输入输出赋值给变量read -p \"请输入一个字符： \" key echo $key 二. 变量的引用$var ${var} ${var:defaultvalue} 三. 内置变量 内置变量 描述 $? 上一条命令执行状态 0 代表执行成功，1代表执行失败 \\$0~\\$9 位置参数1-9 \\${10} 位置参数10 \\$# 位置参数个数 \\$$ 脚本进程的PID \\$- 传递到脚本中的标识 \\$! 运行在后台的最后一个作业的进程ID(PID) \\$_ 之前命令的最后一个参数 \\$@ 传递给脚本或函数的所有参数。被双引号(\" \")包含时，与 \\$* 稍有不同 \\$* 传递给脚本或函数的所有参数 \\$0 脚本的文件名 $* 和 $@ 的区别$ 和 $@ 都表示传递给函数或脚本的所有参数，不被双引号(\" \")包含时，都以\"$1\" \"$2\" … \"$n\" 的形式输出所有参数。但是当它们被双引号(\" \")包含时，\"$\" 会将所有的参数作为一个整体，以\"$1 $2 … $n\"的形式输出所有参数；\"$@\" 会将各个参数分开，以\"$1\" \"$2\" … \"$n\" 的形式输出所有参数。四. 数值型变量的运算1. 数值型变量的加减乘除#样本数据 a=120 b=110 ((c=$a+$b)) #结果：230 ((d=$a-$b)) #结果：10 ((e=$a*$b)) #结果：13200 ((f=$a/$b)) #结果：1 c=$((a+b)) #结果：220 d=$((a-b)) #结果：20 e=$((a*b)) #结果：12000 f=$((a/b)) #结果：1 c=`expr a+b` #结果：220 d=`expr $a - $b` #结果：20 e=`expr $a \\* $b` #结果：12000 f=`expr $a / $b` #结果：1 2. 数值型变量的自增a=1 #第一种整型变量自增方式 a=$(($a+1)) echo $a #第二种整型变量自增方式 a=$[$a+1] echo $a #第三种整型变量自增方式 a=`expr $a + 1` echo $a #第四种整型变量自增方式 let a++ echo $a #第五种整型变量自增方式 let a+=1 echo $a #第六种整型变量自增方式 ((a++)) echo $a 3. 数值类型变量的位数截取a=1560418197875 # 截去后三位,要求只取\"1560418197\" # 方式1: 数值运算 b=$((a/1000)) # 方式2：字符截取（将数值变量当成字符串来处理） c=${a:0:-3} Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:12 "},"origin/shell-文件目录的判断.html":{"url":"origin/shell-文件目录的判断.html","title":"文件目录的判断","keywords":"","body":"文件目录的判断 [ -a FILE ] 如果 FILE 存在则为真。 [ -b FILE ] 如果 FILE 存在且是一个块文件则返回为真。 [ -c FILE ] 如果 FILE 存在且是一个字符文件则返回为真。 [ -d FILE ] 如果 FILE 存在且是一个目录则返回为真。 [ -e FILE ] 如果 指定的文件或目录存在时返回为真。 [ -f FILE ] 如果 FILE 存在且是一个普通文件则返回为真。 [ -g FILE ] 如果 FILE 存在且设置了SGID则返回为真。 [ -h FILE ] 如果 FILE 存在且是一个符号符号链接文件则返回为真。（该选项在一些老系统上无效） [ -k FILE ] 如果 FILE 存在且已经设置了冒险位则返回为真。 [ -p FILE ] 如果 FILE 存并且是命令管道时返回为真。 [ -r FILE ] 如果 FILE 存在且是可读的则返回为真。 [ -s FILE ] 如果 FILE 存在且大小非0时为真则返回为真。 [ -u FILE ] 如果 FILE 存在且设置了SUID位时返回为真。 [ -w FILE ] 如果 FILE 存在且是可写的则返回为真。（一个目录为了它的内容被访问必然是可执行的） [ -x FILE ] 如果 FILE 存在且是可执行的则返回为真。 [ -O FILE ] 如果 FILE 存在且属有效用户ID则返回为真。 [ -G FILE ] 如果 FILE 存在且默认组为当前组则返回为真。（只检查系统默认组） [ -L FILE ] 如果 FILE 存在且是一个符号连接则返回为真。 [ -N FILE ] 如果 FILE 存在 and has been mod如果ied since it was last read则返回为真。 [ -S FILE ] 如果 FILE 存在且是一个套接字则返回为真。 [ FILE1 -nt FILE2 ] 如果 FILE1 比 FILE2 新, 或者 FILE1 存在但是 FILE2 不存在则返回为真。 [ FILE1 -ot FILE2 ] 如果 FILE1 比 FILE2 老, 或者 FILE2 存在但是 FILE1 不存在则返回为真。 [ FILE1 -ef FILE2 ] 如果 FILE1 和 FILE2 指向相同的设备和节点号则返回为真。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:12 "},"origin/shell-数值型的判断.html":{"url":"origin/shell-数值型的判断.html","title":"数值型的判断","keywords":"","body":"数值型的判断# -gt 大于，如[ $a -gt $b ] # -lt 小于，如[ $a -lt $b ] # -eq 等于，如[ $a -eq $b ] # -ne 不等于，如[ $a -ne $b ] # -ge 大于等于，如[ $a -ge $b ] # le 小于等于 ，如 [ $a -le $b ] # 大于(需要双括号),如:(($a > $b)) # >= 大于等于(需要双括号),如:(($a >= $b)) Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:12 "},"origin/shell-if判断.html":{"url":"origin/shell-if判断.html","title":"if判断","keywords":"","body":"if [ command ]; then 符合该条件执行的语句 fi if [ command ]; then command执行返回状态为0要执行的语句 else command执行返回状态为1要执行的语句 fi if [ command1 ]; then command1执行返回状态为0要执行的语句 elif [ command2 ]; then command2执行返回状态为0要执行的语句 else command1和command2执行返回状态都为1要执行的语句 fi PS: [ command ]，command前后要有空格Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:12 "},"origin/shell-for循环语句.html":{"url":"origin/shell-for循环语句.html","title":"for循环语句","keywords":"","body":"第一类：数字性循环#!/bin/bash for((i=1;i #!/bin/bash for i in $(seq 1 10) do echo $(expr $i \\* 3 + 1); done #!/bin/bash for i in {1..10} do echo $(expr $i \\* 3 + 1); done #!/bin/bash awk 'BEGIN{for(i=1; i 第二类：字符性循环#!/bin/bash for i in `ls`; do echo $i is file name\\! ; done #!/bin/bash for i in $* ; do echo $i is input chart\\! ; done #!/bin/bash for i in f1 f2 f3 ; do echo $i is appoint ; done #!/bin/bash list=\"rootfs usr data data2\" for i in $list; do echo $i is appoint ; done 第三类：路径查找#!/bin/bash for file in /proc/*; do echo $file is file path \\! ; done #!/bin/bash for file in $(ls *.sh) do echo $file is file path \\! ; done Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:11 "},"origin/shell-while循环语句.html":{"url":"origin/shell-while循环语句.html","title":"while循环语句","keywords":"","body":"while condition ; do statements ... done Note: 和if一样，condition可以有一系列的statements组成，值是最后的statment的exit statusCopyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:12 "},"origin/shell-until循环语句.html":{"url":"origin/shell-until循环语句.html","title":"until循环语句","keywords":"","body":"until [condition-is-true] ; do statements ... done Note: 执行statements，直至command正确运行。在循环的顶部判断条件,并且如果条件一直为false那就一直循环下去Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:12 "},"origin/shell-字符串的截取拼接.html":{"url":"origin/shell-字符串的截取拼接.html","title":"字符串的截取拼接","keywords":"","body":"一. 字符串的截取 表达式 含义 `${#string}` $string的 长度 `${string:position}` 在\\$string中, 从位置$position开始提取子串 `${string:position:length}` 在\\$string中, 从位置$position开始提取长度为$length的子串 `${string#substring}` 从 变量\\$string的开头, 删除最短匹配$substring的子串 `${string##substring}` 从 变量\\$string的开头, 删除最长匹配$substring的子串 `${string%substring}` 从 变量\\$string的结尾, 删除最短匹配$substring的子串 `${string%%substring}` 从 变量\\$string的结尾, 删除最长匹配$substring的子串 `${string/substring/replacement}` 使用\\$replacement, 来代替第一个匹配的$substring `${string//substring/replacement}` 使用\\$replacement, 代替所有匹配的$substring `${string/#substring/replacement}` 如果\\$string的前缀匹配$substring, 那么就用$replacement来代替匹配到的$substring `${string/%substring/replacement}` 如果\\$string的后缀匹配$substring, 那么就用$replacement来代替匹配到的$substring `expr match \"$string\" '$substring'` 匹配\\$string开头的$substring* 的长度 `expr \"$string\" : '$substring'` 匹 配\\$string开头的$substring* 的长度 `expr index \"$string\" $substring` 在\\$string中匹配到的$substring的第一个字符出现的位置 `expr substr $string $position $length` 在\\$string中 从位置$position开始提取长度为$length的子串 `expr match \"$string\" '($substring)'` 从\\$string的 开头位置提取$substring* `expr \"$string\" : '($substring)'` 从\\$string的 开头位置提取$substring* `expr match \"$string\" '.*($substring)'` 从\\$string的 结尾提取$substring* `expr \"$string\" : '.*($substring)'` 从\\$string的 结尾提取\\$substring* 1. # 号从左边开始，删除第一次匹配到条件的左边字符，保留右边字符# 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a#*/};echo $b # 结果：openshift/origin-metrics-cassandra:v3.9 2. ## 号从左边开始，删除最后一次匹配到条件的左边字符，保留右边字符# 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a##*/};echo $b # 结果：origin-metrics-cassandra:v3.9 3. %号从右边开始，删除第一次匹配到条件的右边内容，保留左边字符（不保留匹配条件）# 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a%/*};echo $b # 结果：docker.io/openshift 4. %% 号从右边开始，删除最后一次匹配到条件的右边内容，保留左边字符（不保留匹配条件）# 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a%%/*};echo $b # 结果：docker.io 5. 从左边第几个字符开始，及字符的个数# 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a:0:5};echo $b # 结果：docke 6. 从左边第几个字符开始，一直到结束# 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a:7};echo $b # 结果：io/openshift/origin-metrics-cassandra:v3.9 7. 从右边第几个字符开始，及字符的个数# 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a:0-8:5};echo $b # 结果：dra:v 8. 从右边第几个字符开始，一直到结束# 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a:0-8};echo $b # 结果：dra:v3.9 9. 截取字符串中的ip# 样本: a=\"当前 IP：123.456.789.172 来自于：中国 上海 上海 联通\" b=${a//[!0-9.]/};echo $b 或者 echo $a | grep -o -E \"[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]*\" # 结果：123.456.789.172 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:12 "},"origin/shell-字符串的包含判断关系.html":{"url":"origin/shell-字符串的包含判断关系.html","title":"字符串的包含判断关系","keywords":"","body":"样本数据a=\"test\" b=\"curiouser\" c=\"test hahah devops\" 1. 通过grep来判断if `echo $c |grep -q $a` ;then echo \"$c\" \" ----包含--- \" \"$a\" else echo \"$c\" \" ----不包含--- \" \"$a\" fi 2. 字符串运算符if [[ $c =~ $a ]] ;then echo \"$c\" \" ----包含--- \" \"$a\" else echo \"$c\" \" ----不包含--- \" \"$a\" fi 3. 用通配符*号代替str1中非str2的部分，如果结果相等说明包含，反之不包含if [[ $c == *$a* ]] ;then echo \"$c\" \" ----包含--- \" \"$a\" else echo \"$c\" \" ----不包含--- \" \"$a\" fi 4. 利用替换if [[ ${c/$a//} == $c ]] ;then echo \"$c\" \" ----不包含--- \" \"$a\" else echo \"$c\" \" ----包含--- \" \"$a\" fi Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:12 "},"origin/shell-自定义函数.html":{"url":"origin/shell-自定义函数.html","title":"自定义函数","keywords":"","body":"自定义函数的格式[ function ] 函数名 [()] { action; [return int;] } # 1.函数在被调用前先声明好 # 2.function关键字可有无 自定义函数的调用# 函数名() # 函数名(参数1,参数2) # 函数名 参数1 参数2 # $(函数名 参数1 参数2) Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:12 "},"origin/maven-Settings配置文件详解.html":{"url":"origin/maven-Settings配置文件详解.html","title":"Mave Settings文件详解","keywords":"","body":"一、settings.xml文件作用从settings.xml的文件名就可以看出，它是用来设置maven参数的配置文件。并且settings.xml是maven的全局配置文件。而pom.xml文件是所在项目的局部配置。Settings.xml中包含类似本地仓储位置、修改远程仓储服务器、认证信息等配置。二、settings.xml文件位置settings.xml文件一般存在于两个位置：全局配置: ${M2_HOME}/conf/settings.xml 用户配置: user.home/.m2/settings.xml Note： 1. 局部配置优先于全局配置。配置优先级从高到低：pom.xml> user settings > global settings 如果这些文件同时存在，在应用配置时，会合并它们的内容，如果有重复的配置，优先级高的配置会覆盖优先级低的。三、settings.xml元素详解settings.xml中的顶级元素 LocalRepository该值表示构建系统本地仓库的路径。其默认值：~/.m2/repository。${user.home}/.m2/repository InteractiveMode表示maven是否需要和用户交互以获得输入。如果maven需要和用户交互以获得输入，则设置成true，反之则应为false。默认为true。true UsePluginRegistrymaven是否需要使用plugin-registry.xml文件来管理插件版本。如果需要让maven使用文件~/.m2/plugin-registry.xml来管理插件版本，则设为true。默认为false。false Offline表示maven是否需要在离线模式下运行。如果构建系统需要在离线模式下运行，则为true，默认为false。 当由于网络设置原因或者安全因素，构建服务器不能连接远程仓库的时候，该配置就十分有用。false PluginGroups当插件的组织id（groupId）没有显式提供时，供搜寻插件组织Id（groupId）的列表。 该元素包含一个pluginGroup元素列表，每个子元素包含了一个组织Id（groupId）。 当我们使用某个插件，并且没有在命令行为其提供组织Id（groupId）的时候，Maven就会使用该列表。默认情况下该列表包含了org.apache.maven.plugins和org.codehaus.mojo。 org.codehaus.mojo Servers仓库的下载和部署是在pom.xml文件中的repositories和distributionManagement元素中定义的。然而，一般类似用户名、密码（有些仓库访问是需要安全认证的）等信息不应该在pom.xml文件中配置，这些信息可以配置在settings.xml中。 server001 my_login my_password ${usr.home}/.ssh/id_dsa some_passphrase 664 775 Mirrors为仓库列表配置的下载镜像列表。 planetmirror.com PlanetMirror Australia http://downloads.planetmirror.com/pub/maven2 central Proxies用来配置不同的代理 myproxy true http proxy.somewhere.com 8080 proxyuser somepassword *.google.com|ibiblio.org Profiles根据环境参数来调整构建配置的列表。settings.xml中的profile元素是pom.xml中profile元素的裁剪版本。 它包含了id、activation、repositories、pluginRepositories和 properties元素。这里的profile元素只包含这五个子元素是因为这里只关心构建系统这个整体（这正是settings.xml文件的角色定位），而非单独的项目对象模型设置。如果一个settings.xml中的profile被激活，它的值会覆盖任何其它定义在pom.xml中带有相同id的profile。 test Activation自动触发profile的条件逻辑。 如pom.xml中的profile一样，profile的作用在于它能够在某些特定的环境中自动使用某些特定的值；这些环境通过activation元素指定。 activation元素并不是激活profile的唯一方式。settings.xml文件中的activeProfile元素可以包含profile的id。profile也可以通过在命令行，使用-P标记和逗号分隔的列表来显式的激活（如，-P test） false 1.5 Windows XP Windows x86 5.1.2600 mavenVersion 2.0.3 ${basedir}/file2.properties ${basedir}/file1.properties 注：在maven工程的pom.xml所在目录下执行mvn help:active-profiles命令可以查看中央仓储的profile是否在工程中生效。 properties对应profile的扩展属性列表。 maven属性和ant中的属性一样，可以用来存放一些值。这些值可以在pom.xml中的任何地方使用标记${X}来使用，这里X是指属性的名称。属性有五种不同的形式，并且都能在settings.xml文件中访问1.0通过${project.version}获得version的值。 3. settings.x: 指代了settings.xml中对应元素的值。例如：false通过 ${settings.offline}获得offline的值。 4. Java System Properties: 所有可通过java.lang.System.getProperties()访问的属性都能在POM中使用该形式访问，例如 ${java.home}。 5. x: 在元素中，或者外部文件中设置，以$的形式使用。{someVar}的形式使用。 --> ${user.home}/our-project 注：如果该profile被激活，则可以在pom.xml中使用${user.install}。 Repositories远程仓库列表，它是maven用来填充构建系统本地仓库所使用的一组远程仓库 codehausSnapshots Codehaus Snapshots false always warn http://snapshots.maven.codehaus.org/maven2 default pluginRepositories发现插件的远程仓库列表。 和repository类似，只是repository是管理jar包依赖的仓库，pluginRepositories则是管理插件的仓库。 maven插件是一种特殊类型的构件。由于这个原因，插件仓库独立于其它仓库。pluginRepositories元素的结构和repositories元素的结构类似。每个pluginRepository元素指定一个Maven可以用来寻找新插件的远程地址 ActiveProfiles手动激活profiles的列表，按照profile被应用的顺序定义activeProfile。 该元素包含了一组activeProfile元素，每个activeProfile都含有一个profile id。任何在activeProfile中定义的profile id，不论环境设置如何，其对应的 profile都会被激活。如果没有匹配的profile，则什么都不会发生。 例如，env-test是一个activeProfile，则在pom.xml（或者profile.xml）中对应id的profile会被激活。如果运行过程中找不到这样一个profile，Maven则会像往常一样运行。 env-test Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:06 "},"origin/maven-生命周期阶段.html":{"url":"origin/maven-生命周期阶段.html","title":"Maven 生命周期阶段","keywords":"","body":"Maven的生命周期以及阶段插件Maven拥有三个生命周期，每个生命周期包含一些阶段，这些阶段是有顺序的，并且后面的阶段依赖于前面的阶段。 运行任何一个阶段的时候，它前面的所有阶段都会被运行 Maven三个生命周期只是定义了各个阶段要做的事情、但是不做任何实际工作、实际工作都是由插件的目标来完成的。插件以独立的形式存在、Maven会在需要的时候下载并使用插件 一个插件有可能有多个功能、每个功能就是一个目标。比如maven-dependency-plugin有十多个目标、每个目标对应了一个功能。插件的目标为dependency:analyze、dependency:tree和dependency:list。通用写法：插件前缀:插件目标。比如compiler:compile 一、Maven的生命周期阶段及插件绑定上面三个生命周期中有很多原来的生命周期阶段没有默认绑定插件、也就意味着默认情况下他们没有任何意义。当然如果我们有自己特殊的处理、可以为他们绑定特殊的插件、比如下面会有提到的在打包的时候生成jar包的源码、可以在default生命周期的verify阶段绑定生成源码插件的生成源码的目标。二、自定义插件绑定自定义绑定允许我们自己掌控插件目标与生命周期的结合、下面以生成项目主代码的源码jar为例。使用到的插件和他的目标为：maven-source-plugin:jar-no-fork、将其绑定到default生命周期阶段verify上（可以任意指定三套生命周期的任意阶段）、在项目的POM配置中（也可以在父POM中配置、后面聚合与继承会有提到） org.apache.maven.plugins maven-source-plugin 2.1.1 attach-sources verify jar-no-fork # build元素下的plugins子元素中声明插件的使用。使用的maven-source-plugin插件，其groupId为org.apache.maven.plugins（官方插件的groupId），version版本为2.1.1.对于自定义绑定的插件，应应指定一个非快照的版本，避免插件版本变化造成构件不稳定。 execution元素用来配置执行的任务。上面配置了一个id为attach-source的任务，通过phrase将其绑定到了verify生命周期阶段上，再通过goals配置指定要执行的插件目标（及插件功能）。 需要注意的是： 即使不通过phrase来配置生命周期阶段，有的插件也定义了默认的生命周期阶段。可使用maven-help-plugin来查看插件的详细信息。例如： mvn help：describe-Dplugin=org.apache.maven.plugins:maven-source-plugin:2.1.1 上述配置有插件的坐标声明、还有excutions下面每个excution子元素配置的执行的一个个任务、通过phase指定与生命周期的那个阶段绑定、在通过goals指定执行绑定插件的哪些目标。当插件的目标绑定到不同的生命周期阶段的时候、插件目标的执行顺序是有生命周期阶段的顺序决定的、当多个插件目标绑定到同一生命周期阶段的时候、顺序是按照插件声明的顺序来决定目标的执行顺序。三、插件配置有三种方式可配置插件功能目标执行任务时的参数1、命令行例如maven-surefire-Plugin提供了maven.test.skip参数，当值为true时就跳过单元测试。在命令行时，加上-D参数配置该插件的参数就能跳过单元测试maven install -Dmaven.test.skip=true 参数-D是Java自带的，其功能就是通过命令行设置Java系统环境变量。 2、POM中设置插件的全局任务配置例如在下面POM文件中配置了maven-source-plugin插件要编译什么java版本的源代码，生成什么java版本的字节码文件 org.apache.maven.plugins maven-source-plugin 2.1.1 1.5 1.5 这样，不管绑定到compile阶段的maven-source-plugin：compile任务，还是绑定到test-compile阶段的maven-source-plugin：testCompiler任务，就都能使用到该配置来基于Java 1.5版本来执行任务3、POM中插件任务配置用户还可以在POM文件中定义某插件的任务参数。例如以maven-antrun-plugin插件为例。他有一个run目标，可以用来在Maven中调用Ant任务。用户可以将maven-antrun-plugin的目标run绑定到多个生命周期阶段上，再加以不同的配置，就可以让Maven在不同的生命阶周期段执行不同的任务 org.apache.maven.plugins maven-antrun-plugin 1.3 ant-validate validate run HAHAHAHHAHAHAHHAHAHAHHAHAHA ant-verify verify run lalallalal 参考链接https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html#Lifecycle_Reference https://maven.apache.org/plugins/index.html https://blog.csdn.net/zhaojianting/article/details/80321488 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:06 "},"origin/regular-expression详解.html":{"url":"origin/regular-expression详解.html","title":"正则表达式","keywords":"","body":"正则表达式详解一、什么是正则表达式？正则表达式（regular expression）就是用一个“字符串”来描述一个特征，然后去验证另一个“字符串”是否符合这个特征。比如 表达式“ab+” 描述的特征是“一个 'a' 和 任意个 'b' ”，那么 'ab', 'abb', 'abbbbbbbbbb' 都符合这个特征。二、正则表达式能干什么？验证字符串是否符合指定特征，比如验证是否是合法的邮件地址 用来查找字符串，从一个长的文本中查找符合指定特征的字符串，比查找固定字符串更加灵活方便 用来替换，比普通的替换更强大 三、正则表达式规则参考链接http://www.regexlab.com/zh/regref.htm Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:11 "},"origin/ceph-rbd单节点安装.html":{"url":"origin/ceph-rbd单节点安装.html","title":"Ceph RBD单节点安装","keywords":"","body":"Prerequisite Hostname OS Ceph版本 allinone.curiouser.com CentOS 7.4.1708 10.2.10（Jewel） 关闭防火墙和SeLinuxsystemctl disable firewalld ; systemctl stop firewalld ; sed -i \"s/SELINUX=enforcing/SELINUX=disabled/\" /etc/selinux/config ; setenforce 0 ; sestatus -v SSH免密码登录打通ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa ; ssh-copy-id root@allinone.curiouser.com ; ssh allinone.curiouser.com Hosts绑定IP地址域名解析echo \"192.168.1.21 allinone.curiouser.com\" >> /etc/hosts 创建ceph用户并设置用户密码和为其添加root权限useradd ceph && echo ceph:ceph | chpasswd ; echo \"ceph ALL=(root) NOPASSWD:ALL\" > /etc/sudoers.d/ceph ; chmod 0440 /etc/sudoers.d/ceph 配置Ceph和Epel的yum源仓库 vim /etc/yum.repos.d/ceph.repo [Ceph] name=Ceph packages for $basearch baseurl=[http://download.ceph.com/rpm-jewel/el7/$basearch](http://download.ceph.com/rpm-jewel/el7/$basearch) enabled=1 gpgcheck=1 type=rpm-md gpgkey=[https://download.ceph.com/keys/release.asc](https://download.ceph.com/keys/release.asc) priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=[http://download.ceph.com/rpm-jewel/el7/noarch](http://download.ceph.com/rpm-jewel/el7/noarch) enabled=1 gpgcheck=1 type=rpm-md gpgkey=[https://download.ceph.com/keys/release.asc](https://download.ceph.com/keys/release.asc) priority=1 # =========================================================================================== vim /etc/yum.repos.d/epel.repo [epel] name=Extra Packages for Enterprise Linux 7 - $basearch # baseurl=[http://download.fedoraproject.org/pub/epel/7/$basearch](http://download.fedoraproject.org/pub/epel/7/$basearch) metalink=[https://mirrors.fedoraproject.org/metalink?repo=epel-7&arch=$basearch](https://mirrors.fedoraproject.org/metalink?repo=epel-7&arch=$basearch) failovermethod=priority enabled=1 gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 # =========================================================================================== 可以修改ceph源（外国的源总是timeout） export CEPH_DEPLOY_REPO_URL=[http://mirrors.163.com/ceph/rpm-jewel/el7](http://mirrors.163.com/ceph/rpm-jewel/el7) ; export CEPH_DEPLOY_GPG_URL=[http://mirrors.163.com/ceph/keys/release.asc](http://mirrors.163.com/ceph/keys/release.asc) (可选)手动安装下载ceph的rpm包（使用ceph-deploy install 安装ceph包网速太慢。）官方下载: http://download.ceph.com/rpm-jewel/el7/x86_64/ 。需要下载的包如下： ceph-10.2.10-0.el7.x86_64.rpm ceph-base-10.2.10-0.el7.x86_64.rpm ceph-common-10.2.10-0.el7.x86_64.rpm ceph-mds-10.2.10-0.el7.x86_64.rpm ceph-mon-10.2.10-0.el7.x86_64.rpm ceph-osd-10.2.10-0.el7.x86_64.rpm ceph-radosgw-10.2.10-0.el7.x86_64.rpm ceph-selinux-10.2.10-0.el7.x86_64.rpm rbd-mirror-10.2.10-0.el7.x86_64.rpm yum localinstall -y ./*.rpm 一、安装Ceph-Deploy安装Ceph-deployyum install ceph-deploy -y 安装ceph相关的软件 ceph-deploy install $HOSTNAME 二、创建集群配置文件创建ceph-deploy的集群配置文件夹，路径并切换过去mkdir my-cluster ;cd my-cluster 用 ceph-deploy 创建集群，用 new 命令、并指定主机作为初始监视器。 ceph-deploy new $HOSTNAME # 该操作会在~/my-cluster下会生成三个文件 $&gt; ls -rw-rw-r-- 1 ceph ceph 251 Jan 12 16:34 ceph.conf -rw-rw-r-- 1 ceph ceph 15886 Jan 12 16:30 ceph.log -rw------- 1 ceph ceph 73 Jan 12 16:30 ceph.mon.keyring # ceph.conf中默认的osd pool为3，对应了三个node节点。如果只有两个node节点，则需要修改ceph.conf中的默认值 [global] fsid = 25c13add-967e-4912-bb33-ebbc2cb9376d mon_initial_members = allinone.curiouser.com mon_host = 172.16.2.3 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx filestore_xattr_use_omap = true osd pool default size=1 三、创建Monitorceph-deploy mon create $HOSTNAME ; ceph-deploy gatherkeys $HOSTNAME ; ceph mds stat #查看mds节点状态 四、创建OSD方式一：(可选)手动节点上挂载lvm存储到某个目录下，作为node节点上OSD的数据存储目录yum install -y lvm2 ; disk=/dev/vdc ; pvcreate ${disk} ; vgcreate ${disk} ; vgcreate -s 16m ceph-osd ${disk} ; PE_Number=`vgdisplay|grep \"Free PE\"|awk '{print $5}'` ; lvcreate -l ${PE_Number} -n ceph-osd ceph-osd ; mkfs.xfs /dev/ceph-osd/ceph-osd ; mkdir -p /data/ceph/osd ; chown -R ceph:ceph /data/ceph/osd ; echo \"/dev/ceph-osd/ceph-osd /data/ceph/osd xfs defaults 0 0\" &gt;&gt;/etc/fstab ; mount -a ; df -mh #LV的文件系统格式注意要xfs,CentOS推荐使用xfs的文件系统.如果是ext4，需要在/etc/ceph/ceph.conf 中添加参数用来限制文件名的长度 osd max object name len = 256 osd max object namespace len = 64 # 之后重启osd服务 systemctl restart ceph-osd.target 准备并激活node节点上的OSD #准备Node节点上的OSD ceph-deploy osd prepare $HOSTNAME:/data/ceph/osd #激活Node节点上的OSD ceph-deploy osd activate $HOSTNAME:/data/ceph/osd #查看OSD状态 ceph osd tree 方式二：(不是以目录为OSD数据存储设备，而是直接以硬盘。其实就是省去手动在硬盘上创建分区的操作) #准备Node节点上的OSD ceph-deploy osd prepare $HOSTNAME:/dev/vdc #激活Node节点上的OSD ceph-deploy osd activate $HOSTNAME:/dev/vdc1 #查看OSD状态 ceph osd tree 五、安装验证 #集群健康状态检查 $> ceph health HEALTH_OK $> ceph -s $> systemctl is-enabled ceph-mds.target ceph-mon.target ceph-osd.target ceph-radosgw.target ceph.target 六、其他信息Ceph相关的SystemD Unitsceph-mds.target ceph-mon.target ceph-osd.target ceph-radosgw.target ceph.target Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:51:58 "},"origin/ceph-filesystem单节点安装.html":{"url":"origin/ceph-filesystem单节点安装.html","title":"Ceph FileSystem单节点安装","keywords":"","body":"Context Hostname OS Ceph版本 allinone.curiouser.com CentOS 7.4.1708 10.2.10（Jewel） 一个cephfs至少要求两个librados存储池，一个为data，一个为metadata。当配置这两个存储池时，注意：为metadata pool设置较高级别的副本级别，因为metadata的损坏可能导致整个文件系统不用 建议metadata pool使用低延时存储，比如SSD，因为metadata会直接影响客户端的响应速度 Preflight一个 clean+active 的cluster（Ceph RBD单节点安装） cluster fb506b4e-43b8-4634-acb9-ea3ee5a97b91 health HEALTH_OK monmap e1: 1 mons at {allinone=192.168.1.96:6789/0} election epoch 29, quorum 0 allinone fsmap e4: 1/1/1 up {0=allinone.okd311.curiouser.com=up:active} osdmap e113: 1 osds: 1 up, 1 in flags sortbitwise,require_jewel_osds pgmap v61453: 192 pgs, 3 pools, 2639 MB data, 985 objects 2730 MB used, 94500 MB / 97231 MB avail 192 active+clean 一、操作部署元数据服务器MDSceph-deploy mds create $HOSTNAME 创建cephfs需要的两个存储池：一个pool用来存储数据，一个pool用来存储元数据ceph osd pool create cephfs_data 64 ceph osd pool create cephfs_metadata 64 创建CephFS ceph fs new cephfs cephfs_metadata cephfs_data ceph fs ls 二、验证$ ceph mds stat e4: 1/1/1 up {0=allinone.okd311.curiouser.com=up:active} 三、客户端挂载Kernel方式#加载rbd内核模块 modprobe rbd lsmod | grep rbd # 获取client.admin用户的秘钥 ceph auth get client.admin # [client.admin] # key = AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== # caps mds = \"allow *\" # caps mon = \"allow *\" # caps osd = \"allow *\" mkdir /mnt/mycephfs mount -t ceph allinone.okd311.curiouser.com:/ /mnt/mycephfs -o name=admin,secret=AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== FUSE方式yum -y install ceph-fuse ceph-fuse -k /etc/ceph/ceph.client.admin.keyring -m 192.168.197.154:6789 ~/mycephfs/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:51:58 "},"origin/pxe-kickstart无人值守部署OS.html":{"url":"origin/pxe-kickstart无人值守部署OS.html","title":"PXE-Kickstart无人值守部署OS","keywords":"","body":"一、PXE Kickstart网络引导无人值守部署主机OS1. PXE简介PXE(Pre-boot Execution Environment，预启动执行环境)是由Intel公司开发的最新技术，工作于Client/Server的网络模式，支持工作站通过网络从远端服务器下载映像，并由此支持通过网络启动作系统，在启动过程中，终端要求服务器分配IP地址，再用TFTP（trivial file transfer protocol）或MTFTP(multicast trivial file transfer protocol)协议下载一个启动软件包到本机内存中执行，由这个启动软件包完成终端基本软件设置，从而引导预先安装在服务器中的终端操作系统。严格来说，PXE 并不是一种安装方式，而是一种引导方式。进行 PXE 安装的必要条件是在要安装的计算机中必须包含一个 PXE 支持的网卡（NIC），即网卡中必须要有 PXE Client。PXE 协议可以使算机通过网络启动。此协议分为 Client端和 Server 端，而PXE Client则在网卡的 ROM 中。当计算机引导时，BIOS 把 PXE Client 调入内存中执行，然后由 PXE Client 将放置在远端的文件通过网络下载到本地运行。运行 PXE 协议需要设置 DHCP 服务器和 TFTP 服务器。DHCP 服务器会给 PXE Client（将要安装系统的主机）分配一个 IP 地址，由于是给 PXE Client 分配 IP 地址，所以在配置 DHCP 服务器时需要增加相应的 PXE 设置。此外，在 PXE Client 的 ROM 中，已经存在了 TFTP Client，那么它就可以通过 TFTP 协议到 TFTP Server 上下载所需的文件了。2. PXE工作流程① PXE Client 从自己的PXE网卡启动，向本网络中的DHCP服务器索取IP② DHCP 服务器返回分配给客户机的IP 以及PXE文件的放置位置(该文件一般是放在一台TFTP服务器上)③ PXE Client 向本网络中的TFTP服务器索取pxelinux.0 文件④ PXE Client 取得pxelinux.0 文件后之执行该文件⑤ 根据pxelinux.0 的执行结果，通过TFTP服务器加载内核和文件系统⑥ 进入安装画面, 此时可以通过选择HTTP、FTP、NFS 方式之一进行安装详细工作流程，请参考下面这幅图：3. Kickstart简介Kickstart是一种无人值守的安装方式。它的工作原理是在安装过程中记录典型的需要人工干预填写的各种参数，并生成一个名为ks.cfg的文件。如果在安装过程中（不只局限于生成Kickstart安装文件的机器）出现要填写参数的情况，安装程序首先会去查找Kickstart生成的文件，如果找到合适的参数，就采用所找到的参数；如果没有找到合适的参数，便需要安装者手工干预了。所以，如果Kickstart文件涵盖了安装过程中可能出现的所有需要填写的参数，那么安装者完全可以只告诉安装程序从何处取ks.cfg文件，然后就去忙自己的事情。等安装完毕，安装程序会根据ks.cfg中的设置重启系统，并结束安装。二、PXE+Kickstart无人值守安装OS的工作流程三、PXE服务端配置Prerequisite PXE主机： 主机名 IP地址 OS 路由器 pk.tools.curiouser.com 192.168.1.80 CentOS 7.5.1804 192.168.1.1 1、基础准备上传ISO文件并挂载，关闭Firewall和SELinuxyum install -y wget && \\ mkdir /mnt/{cdrom,iso} && \\ wget http://vault.centos.org/7.5.1804/isos/x86_64/CentOS-7-x86_64-Minimal-1804.iso /mnt/iso && \\ echo \"/mnt/iso/CentOS-7-x86_64-Minimal-1804.iso /mnt/cdrom iso9660 defaults,loop 0 0\" >> /etc/fstab && \\ mount -a && \\ df -mh && \\ setenforce 0 && \\ sed -i \"s/SELINUX=enforcing/SELINUX=disabled/\" /etc/selinux/config && \\ systemctl stop firewalld && \\ systemctl disable firewalld && \\ systemctl stop firewalld 2、配置HTTD服务安装服务yum install -y httpd 配置服务ln -s /mnt/cdrom/ /var/www/html/CentOS7 启动验证服务，服务端口tcp:80systemctl start httpd && \\ systemctl enable httpd && \\ systemctl status httpd && \\ ss -tnl | grep 80 3、配置DHCP服务安装服务yum install -y dhcp 配置服务/etc/dhcp/dhcpd.confdefault-lease-time 600; max-lease-time 7200; log-facility local7; subnet 192.168.1.0 netmask 255.255.255.0 { option routers 192.168.1.1; # 给 client 的默认网关 option subnet-mask 255.255.255.0; # 给 client 的子网掩码 option domain-name \"curiouser.com\"; # 给 client 的搜索域 option domain-name-servers 192.168.1.1; # 给 client 的域名服务器 range dynamic-bootp 192.168.1.100 192.168.1.120; # 可供分配的IP范围 default-lease-time 21600; max-lease-time 43200; next-server 192.168.1.80; # TFTP Server 的IP地址 filename \"pxelinux.0\"; # pxelinux启动文件位置; } 启动验证服务,服务端口为67systemctl start dhcpd && \\ systemctl enable dhcpd && \\ systemctl status dhcpd && \\ ss -nulp | grep dhcpd 4、配置TFTP服务安装服务yum install -y tftp-server tftp xinetd net-tools 配置服务/etc/xinetd.d/tftpservice tftp { socket_type = dgram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd server_args = -s /var/lib/tftpboot #默认disable是yes的，把它改为no即可 disable = no per_source = 11 cps = 100 2 flags = IPv4 } 启动验证服务,服务端口为UDP:69systemctl start xinetd && \\ systemctl enable xinetd && \\ systemctl status xinetd && \\ ss -unlp | grep 69 && \\ netstat -a | grep tftp && \\ netstat -tunap | grep :69 5、准备相关文件安装服务yum install -y syslinux tree 拷贝文件cp /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/ && \\ cp /mnt/cdrom/images/pxeboot/{vmlinuz,initrd.img} /var/lib/tftpboot/ && \\ cp /mnt/cdrom/isolinux/{vesamenu.c32,boot.msg,splash.png} /var/lib/tftpboot/ && \\ cp /usr/share/syslinux/{chain.c32,mboot.c32,menu.c32,memdisk} /var/lib/tftpboot/ && \\ mkdir /var/lib/tftpboot/pxelinux.cfg /var/lib/tftpboot/目录结构tree -phL 2 /var/lib/tftpboot/ ├── [-rw-r--r-- 84] boot.msg # 窗口提示信息文件,提示信息在菜单出现前出现，显示时间较短，可以添加些艺术字之类的信息。 ├── [-rw-r--r-- 20K] chain.c32 ├── [-rw-r--r-- 50M] initrd.img # 这是一个初始化文件，一个最小的系统镜像 ├── [-rw-r--r-- 33K] mboot.c32 ├── [-rw-r--r-- 26K] memdisk ├── [-rw-r--r-- 54K] menu.c32 ├── [-rw-r--r-- 26K] pxelinux.0 ├── [drwxr-xr-x 21] pxelinux.cfg # 启动菜单目录 ├── [-rw-r--r-- 186] splash.png # 窗口背景图片 ├── [-rw-r--r-- 149K] vesamenu.c32 # 系统自带的两种窗口模块之一 └── [-rwxr-xr-x 5.9M] vmlinuz # 内核文件 创建/var/lib/tftpboot/pxelinux.cfg/default （default文件参数详见：PXE引导配置文件参数详解）bash -c 'cat >/var/lib/tftpboot/pxelinux.cfg/default 6、创建KS文件 /var/www/html/CentOS7.cfg方式一：手动编写(KS文件具体参数详情见笔记：Kickstart文件参数详解)install text lang en_US.UTF-8 keyboard us auth --useshadow --passalgo=sha512 url --url=\"http://192.168.1.80/CentOS7\" rootpw --iscrypted $1$6/87AF3n$eczKeiNRBv7H.GXnur1Ld/ selinux --disabled firewall --disabled network --bootproto=dhcp --device=ens192 --ipv6=auto --activate network --hostname=test reboot timezone Asia/Shanghai --isUtc --nontp bootloader --location=mbr --boot-drive=sda clearpart --all --drives=sda services --enabled=NetworkManager,sshd firstboot --enable ignoredisk --only-use=sda #(可选)autopart --type=lvm --fstype=xfs part /boot --fstype=\"xfs\" --ondisk=sda --size=200 part / --fstype=\"xfs\" --ondisk=sda --size=30720 part /opt --fstype=\"xfs\" --ondisk=sda --size=10240 part /var --fstype=\"xfs\" --grow --ondisk=sda --size=1 %packages @^minimal @core %end %post --interpreter=/bin/bash --log=/root/post-install.log mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak cat >> /etc/yum.repos.d/ustc.repo /dev/null yum makecache > /dev/null yum install -y tree vim telnet nc unzip git net-tools wget bind-utils > /dev/null ipaddr=$(ip addr | awk '/^[0-9]+: / {}; /inet.*global/ {print gensub(/(.*)\\/(.*)/, \"\\\\1\", \"g\", $2)}'| sed -n '1p') echo $ipaddr $HOSTNAME >> /etc/hosts echo \"Set HOSTNAME test\" echo \"Disabled SELinux and Firewall\" echo \"/dev/sda /boot xfs 200MB\" echo \"/dev/sda / xfs 30G\" echo \"/dev/sda /opt xfs 10G\" echo \"/dev/sda /var xfs RemainingCapacity\" echo \"Make Yum Repository To USE USTC Yum Repository \" echo \"Installed Tools : tree vim telnet nc unzip git net-tools wget bind-utils\" echo \" #######################\" >> /etc/motd echo \" # Keep Your Curiosity #\" >> /etc/motd echo \" #######################\" >> /etc/motd %end 方式二、使用system-config-kickstart图形化界面配置安装：system-config-kickstartyum install -y system-config-kickstart 7、验证KS文件的语法正确性yum install -y pykickstart ksvalidator /var/www/html/CentOS7.cfg 8、(可选)自动安装配置脚本前提：CentOS-7-x86_64-Everything-1804.iso已经放置在/mnt/iso文件夹下 pxe-kickstart-CentOS7.cfg mkdir /mnt/cdrom && \\ echo \"/mnt/iso/CentOS-7-x86_64-Everything-1804.iso /mnt/cdrom iso9660 defaults,loop 0 0\" >> /etc/fstab && \\ mount -a && \\ df -mh && \\ setenforce 0 && \\ sed -i \"s/SELINUX=enforcing/SELINUX=disabled/\" /etc/selinux/config && \\ systemctl stop firewalld && \\ systemctl disable firewalld && \\ systemctl stop firewalld && \\ yum install -y httpd dhcp tftp-server tftp xinetd net-tools syslinux tree && \\ ln -s /mnt/cdrom/ /var/www/html/CentOS7 && \\ systemctl start httpd && \\ systemctl enable httpd && \\ systemctl status httpd && \\ ss -tnl | grep 80 && \\ bash -c 'cat >> /etc/dhcp/dhcpd.conf /etc/xinetd.d/tftp /var/lib/tftpboot/pxelinux.cfg/default 参考链接★★★★☆：https://blog.csdn.net/yanghua1012/article/details/80426659★★★★☆：http://www.178linux.com/99307★★★★☆：https://blog.51cto.com/lzhnb/2117618★★★☆☆：https://marclop.svbtle.com/creating-an-automated-centos-7-install-via-kickstart-file★★★☆☆：https://docs.centos.org/en-US/centos/install-guide/Kickstart2/#sect-kickstart-file-create★★★☆☆：https://www.cnblogs.com/cloudos/p/8143929.html★★★☆☆：http://bbs.51cto.com/thread-621450-1.htmlCopyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:11 "},"origin/pxe-kickstart文件参数详解.html":{"url":"origin/pxe-kickstart文件参数详解.html","title":"Kickstart文件参数详解","keywords":"","body":"一、简介kickstart文件中各部分(section)要遵循一定的顺序。每个部分中的项(Item)并不需要按照一定的顺序排列，除非有其他要求。各部分的顺序如下：命令部分 %packages部分 %pre, %post, 以及%traceback部分 -- 这些部分的顺序可以任意排列 %packages, %pre, %post以及%traceback部分需要以%end结束。 不要求的项(Item)可以被省略。 省略任何一个被要求的项将会导致安装程序向用户询问相关的问题，就像典型安装过程向用户询问那样。一旦用户给出了答案，安装过程将会继续自动进行，除非又遇到缺失的项。 以(#)开头的行作为注释行被忽略。 如果在kickstart安装中使用了不推荐的命令、选项或者语法，警告日志将会被记录到anaconda日志中。因为在一个或者两个发行版之间这些不推荐的项经常会被删掉，所以检查安装日志以确保没有使用这些项非常必要。当使用ksvalidator的时候，这些不推荐的项会导致错误。 如果选项后接等号（=），则必须指定一个值 引用磁盘的特殊说明：Kickstart一直通过设备节点名(例如 sda)来引用磁盘。Linux内核采用了更加动态的方法，设备名并不会在重启时保持不变。因此，这会使得在Kickstart脚本中引用磁盘变得复杂。为了满足稳定的设备命名，你可以在项(Item)中使用/dev/disk代替设备名。例如，你可以使用： part / --fstype=ext4 --onpart=/dev/disk/by-path/pci-0000:00:05.0-scsi-0:0:0:0-part1 part / --fstype=ext4 --onpart=/dev/disk/by-id/ata-ST3160815AS_6RA0C882-part1 来代替： part / --fstype=ext4 --onpart=sda1 这种方式提供了对磁盘的持久引用，因而比仅仅使用sda更加有意义。 这在大的存储环境中特别有意义。你也可以使用类似于shell的入口来应用磁盘。这种方式主要用来简化大的存储环境中clearpart以及ignoredisk命令的使用。例如，为了替代： ignoredisk --drives=sdaa,sdab,sdac 你可以使用如下的入口： ignoredisk --drives=/dev/disk/by-path/pci-0000:00:05.0-scsi-* 最后，如果想要在任何地方引用已经存在的分区或者文件系统（例如，在part --ondisk=中），你可以通过文件系统标签(label)或者UUID来进行。例如： part /data --ondisk=LABEL=data part /misc --ondisk=UUID=819ff6de-0bd6-4bf4-8b72-dbe41033a85b 二、必需选项bootloader ：指明引导程序(bootloader)如何被安装。引申Bootloader相关概念：CentOS系统启动流程# 建议给Bootloader设置密码以防止黑客修改系统启动项或者不授权登录系统 --append= 指定内核参数，要指定多个参数，使用空格分隔。引导程序默认的参数是\"rhgb quiet\"。举例: \"bootloader --location=mbr --append=\"hdd=ide-scsi ide=nodma\" --boot-drive= 指定安装bootloader到哪个磁盘上 --leavebootorder 防止安装程序更改 EFI 或者 ISeries/PSeries 系统中的现有可引导映像。 --driveorder= 指定在 BIOS 引导顺序中的首选驱动器。例如: bootloader --driveorder=sda,hda --location= 指定引导记录的写入位置（在大多数情况下不需要指定这个选项），有效值如下 1.mbr 默认选项。具体要看该驱动器是使用主引导记录（MBR）还是 GUID 分区表（GPT）方案： a.在使用 GPT 格式化的磁盘中，这个选项会在 BIOS 引导分区中安装 stage 1.5 引导装载程序。 b.在使用 MBR 格式化的磁盘中，会在 MBR 与第一个分区之间的空白空间中安装 stage 1.5。 2.partition 在包含内核的分区的第一个扇区中安装引导装载程序 3.none -不安装引导装载程序。 --password= 如果使用GRUB2,则会将使用这个选项指定的密码设定为引导装载程序密码.这应用来限制对GRUB2 shell的访问,并可以跳过任意内核选项.如果指定密码,GRUB2还会询问用户名。该 用户名总是root --iscrypted 通常当使用 --password= 选项指定引导装载程序密码时，会将其以明文方式保存在 Kickstart 文件中。如果要加密此密码，可使用这个选项和一个加密的密码。 请使用 grub2-mkpasswd-pbkdf2 命令生成加密的密码，输入要使用的密码，并将该命令的输出结果（以 grub.pbkdf2 开头的哈希符号）复制到 Kickstart 文件中 --timeout= 指定引导装载程序引导默认选项前等待的时间（以秒为单位）。 --default= 设定引导装载程序配置中的默认引导映像。 --extlinux 使用 extlinux 引导装载程序而不是 GRUB2。这个选项只能用于支持 extlinux 的系统。 --disabled 这个选项是 --location=none 的加强版。--location=none 只是简单地禁用 bootloader 安装，而 --disabled 则不仅禁用 bootloader 安装，也会禁用 bootloader 软件 包的安装，从而节省了空间。 keyboard：设置系统键盘类型--vckeymap= # 指定VConsole应该使用的字符映射表。是字符映射表的文件名，和/usr/lib/kbd/keymaps目录下的文件名除去\".map.gz\"后相同。 --xlayouts=,,..., #指定 X 布局列表，该列表可使用逗号分开，无空格。接受与 setxkbmap(1) 相同格式的值，可以是 layout 格式（比如 cz），也可 #以是 layout (variant) 格式（比如 cz (qwerty)）。 --switch=,,..., #指定布局切换选项（在多个键盘布局间切换的快捷方式）列表。必须使用逗号分开多个选项，无空格。接受值与 setxkbmap(1) 格式相同。 ​ #示例使用 --xlayouts= 选项设置两个键盘布局（English (US) 和 Czech (qwerty)），并允许使用 Alt+Shift 在二者之间进行切换： # keyboard --xlayouts=us,'cz (qwerty)' --switch=grp:alt_shift_toggle lang：设置在安装过程中使用的语言以及系统的缺省语言示例：lang en_US 文本模式的安装过程不支持某些语言(主要是中文,日语,韩文和印度的语言).如果用lang命令指定这些语言中的一种,安装过程仍然会使用英语,但是系统会缺省使用指定的语言. part 或者 partition：在系统上创建一个分区。（install模式必须）part |swap|pv.id|rdid.id options 1.mntpoint:挂载点，是在创建普通分区时指定新分区挂载位置的项；挂载点需要格式正. 例如 /, /usr, /home 2.swap 分区将被用作交换分区。为了自动决定交换分区的大小，可以使用--recommended选项。 3.raid. 表示创建的分区类型为raid型；必须用id号进行唯一区别； 4.pv. 表示所创建的分区类型为LVM型；必须用唯一id号进行区别； --size= 最小分区大小(MB)。这里可以指定一个整数值如500.不要在后面加MB。 --grow 告诉分区增长以填满可用空间(如果有的话)，或者填满设置的最大值。注意，--grow并不支持RAID卷在上的分区。 --maxsize= 分区被设置为grow时的最大分区大小(MB)。指定一个整数值，不要在后面加上MB。 --noformat 告诉安装程序不格式化分区，和--onpart一起使用。 --onpart= or --usepart= 把分区放在已经存在的设备上。使用\"--onpart=LABEL=name\"或者\"--onpart=UUID=name\" 来通过各自的标签(label)或uuid来指定一个分区。Stop (medium size).png Anaconda也许会以特殊的顺序创建分区，所以使用标签比只用分区名要安全些 --ondisk= or --ondrive= 强制在特定的磁盘上创建。 --asprimary 强制分区作为主分区，否则会导致分区失败。 --fsprofile= 为在该分区上创建文件系统的程序指定使用类型。使用类型定义了创建文件系统时各种各样的调整参数。为了让该选项能起作用，文件系统必须支持使用类型的概念，而且必须有 一个配置文件列出所有可用的类型。对于ext2/3/4，配置文件位于/etc/mke2fs.conf。 --fstype= 为分区设置文件系统类型。有效值包括ext4,ext3,ext2,btrfs,swap以及vfat。其它文件系统是否有效取决于传递给anaconda使能其它文件系统的命令行参数。 --fsoptions= 为挂载文件系统指定自由格式的字符串选项。该字符串将会被拷贝到安装系统的/etc/fstab文件中并且应该被引号括起来。 --label= 指定分区上创建的文件系统标签。如果所指定的标签已经被其它文件系统使用，新的标签将会被创建。 --recommended 自动决定分区大小。 --onbiosdisk= 强制在BIOS发现的特定磁盘上创建分区。 --encrypted 说明该分区应该被加密 --passphrase= 指定加密分区时指定的密码短语。如果没有上述--encrypted选项，该选项不起任何作用。如果没有密码短语被指定，将会使用系统范围内的默认密码短语，如果没有默认的， 安装器会停下来提醒。 --escrowcert= 从加载X.509认证。存储用证书加密过的数据加密密钥。只在--encrypted指定时有效。 --backuppassphrase 只在--escrowcert指定时相关。除了存储数据加密密钥之外，产生一个随机密码短语并将其添加到该分区。然后使用--escrowcert指定的证书加密并存储于/root。如果 不止一个LUKS卷使用--backuppassphrase，它们将共享该密码短语。 例： part /boot --fstype=“ext3” --size=100 part swap --fstype=“swap” –size=512 part / --bytes-pre-inode=4096 --fstype=“ext4”--size=10000 part /data --onpart=/dev/sdb1 --noformat part raid.100 --size=2000 part pv.100 --size=1000 auth/authconfig：设置系统的授权验证选项。它只是authconfig程序的封装，因而所有被authconfig程序识别的选项都可以应用于auth命令。想要获取完整的列表，请参考authconfig手册。默认情况下，密码一般会被加密但并不会放在shadow文件中。--enablemd5,每个用户口令都使用md5加密. --enablenis,启用NIS支持.在缺省情况下,--enablenis使用在网络上找到的域.域应该总是用--nisdomain=选项手工设置. --nisdomain=,用在NIS服务的NIS域名. --nisserver=,用来提供NIS服务的服务器(默认通过广播). --useshadow或--enableshadow,使用屏蔽口令. --enableldap,在/etc/nsswitch.conf启用LDAP支持,允许系统从LDAP目录获取用户的信息(UIDs,主目录,shell 等等).要使用这个选项,必须安装nss_ldap软件包.也必须用--ldapserver=和--ldapbasedn=指定服务器和base DN(distinguished name). --enableldapauth,把LDAP作为一个验证方法使用.这启用了用于验证和更改密码的使用LDAP目录的pam_ldap模块.要使用这个选项,必须安装nss_ldap软件包.也必须用--ldapserver=和--ldapbasedn=指定服务器和base DN. --ldapserver=,如果指定了--enableldap或--enableldapauth,使用这个选项来指定所使用的LDAP服务器的名字.这个选项在/etc/ldap.conf文件里设定. --ldapbasedn=,如果指定了--enableldap或--enableldapauth,使用这个选项来指定用户信息存放的LDAP目录树里的DN.这个选项在/etc/ldap.conf文件里设置. --enableldaptls,使用TLS(传输层安全)查寻.该选项允许LDAP在验证前向LDAP服务器发送加密的用户名和口令. --enablekrb5,使用Kerberos 5验证用户.Kerberos自己不知道主目录,UID或shell.如果启用了Kerberos,必须启用LDAP,NIS,Hesiod或者使用/usr/sbin/useradd命令来使这个工作站获知用户的帐号.如果使用这个选项,必须安装pam_krb5软件包. --krb5realm=,工作站所属的Kerberos 5领域. --krb5kdc=,为领域请求提供服务的KDC.如果的领域内有多个KDC,使用逗号(,)来分隔它们. --krb5adminserver=,领域内还运行kadmind的KDC.该服务器处理改变口令以及其它管理请求.如果有不止一个KDC,该服务器必须是主KDC. --enablehesiod,启用Hesiod支持来查找用户主目录,UID 和 shell.在网络中设置和使用 Hesiod 的更多信息,可以在 glibc 软件包里包括的/usr/share/doc/glibc-2.x.x/README.hesiod里找到.Hesiod是使用DNS记录来存储用户,组和其他信息的 DNS 的扩展. --hesiodlhs,Hesiod LHS(\"left-hand side\")选项在/etc/hesiod.conf里设置.Hesiod 库使用这个选项来决定查找信息时搜索DNS的名字,类似于LDAP对 base DN的使用. --hesiodrhs,Hesiod RHS(\"right-hand side\")选项在/etc/hesiod.conf里设置.Hesiod 库使用这个选项来决定查找信息时搜索DNS的名字,类似于LDAP对base DN的使用. --enablesmbauth,启用对SMB服务器(典型的是Samba或Windows服务器)的用户验证.SMB验证支持不知道主目录,UID 或 shell.如果启用SMB,必须通过启用LDAP,NIS,Hesiod或者用/usr/sbin/useradd命令来使用户帐号为工作站所知.要使用这个选项,必须安装pam_smb软件包. --smbservers=,用来做SMB验证的服务器名称.要指定不止一个服务器,用逗号(,)来分隔它们. --smbworkgroup=,SMB服务器的工作组名称. --enablecache,启用nscd服务.nscd服务缓存用户,组和其他类型的信息.如果选择在网络上用NIS,LDAP或hesiod分发用户和组的信息,缓存就尤其有用. rootpw：设置系统root账号的密码rootpw [--iscrypted|--plaintext] [--lock] password --iscrypted #如果该选项存在,口令就会假定已被加密.可使用以下命令生成用随机盐值进行sha512加密后的密码 # python -c 'import crypt,getpass;pw=getpass.getpass();print(crypt.crypt(pw) if (pw==getpass.getpass(\"Confirm: \")) else exit())' --plaintext # 使用不加密的密码 --lock #锁定root用户，root用户将无法登陆Console 三、可选选项install/upgradeinstall：默认安装方法。必须从 cdrom、harddrive、nfs、liveimg 或者 url（用于 FTP、HTTP、或者 HTTPS 安装）中指定安装类型 upgrade: 升级现有系统. text/graphical：kickstart安装模式text：在文本模式下执行kickstart安装. graphical： 在图形模式下执行kickstart安装.kickstart安装默认在图形模式下安装. cdrom/harddrive/url/nfs/liveimg：指定安装类型cdrom: 从系统上的第一个光盘驱动器CD-ROM/DVD中安装. harddrive [--biospart= | --partition=] [--dir=]: # 从本地驱动器上包含ISO镜像的目录安装，该驱动器必须是vfat或者ext2文件系统。除了改目录之外，还需要以后面的方式提供install.img。 # 一种方式是由boot.iso启动，另一种是在ISO镜像相同的目录中创建一个images/目录，然后将install.img放在那里。 --biospart= 安装用到的BIOS分区（例如82p2）。 --partition= 安装用到的硬盘分区 --dir= 包含ISO镜像和images/install.img的目录 # 例如:harddrive --partition=hdb2 --dir=/tmp/install-tree nfs --server= --dir= [--opts=] # 从指定的NFS服务器安装. --server=,指定NFS服务器（主机名或者IP）。 --dir=,包含安装树的variant目录的目录. --opts=,用于挂载NFS输出的Mount选项(可选). # 例如:nfs --server=nfsserver.example.com --dir=/tmp/install-tree url --url=|--mirrorlist= [--proxy=] [--noverifyssl] # 通过FTP或HTTP从远程服务器上的安装树中安装. --url= 安装用到的URL。支持的协议有HTTP, HTTPS, FTP, file等。 --mirrorlist= 安装用到的镜像URL。在该URL中完成$releasever和$basearch的变量替换 --proxy= 指定安装时用到的HTTP/HTTPS/FTP代理。参数的各个部分用实际值来代替 --noverifyssl 对于HTTPS服务器上的目录树，不用检查服务器的证书以及服务器的主机名匹配证书的域名 # 例如:url --url http:///或url --url ftp://:@/ liveimg --url= [--proxy=] [--checksum=] [--noverifyssl] #使用磁盘映像而不是软件包安装。映像文件可以是取自实时 ISO 映像的 squashfs.img 文件，压缩 tar 文件（.tar、.tbz、.tgz、.txz、.tar.bz2、.tar.gz 或者 .tar.xz） #或者安装介质可以挂载的任意文件系统。支持的文件系统为 ext2、ext3、ext4、vfat 和 xfs。 # Anaconda预期该镜像包含完成系统安装所需的实用程序。因此，创建磁盘镜像最好的方法是使用livemedia-creator。 # 如果该镜像包含/LiveOS/*.img（这是squashfs.img的构成），LiveOS中的第一个*.img将会被挂载，并用来安装目标系统。 --url= 执行安装的位置。支持的协议为 HTTP、HTTPS、FTP 和 file。 --proxy=[protocol://][username[:password]@]host[:port] 指定在安装时用到的HTTP/HTTPS/FTP代理。参数的各个部分用实际值来代替。 --checksum= 可选，镜像文件的sha256校验和。 --noverifyssl 对于HTTPS服务器上的目录树，不用检查服务器的证书以及服务器的主机名匹配证书的域名。 reboot/poweroff/shutdown/halt：安装完成后做什么操作reboot：重启（缺省选项） poweroff：关闭系统并断电.通常,在手工安装过程中,anaconda会显示一条信息并等待用户按任意键来重新启动系统. shutdown：关闭系统. halt：halt选项基本和shutdown -h命令相同. services：设置禁用或允许列出的服务--disabled 设置服务为禁用 --enabled 启动服务 例：services --disabled autid,cups,smartd,nfslock 服务之间用逗号隔开，不能有空格 selinux： 在系统里设置SELinux状态.在anaconda里,SELinux缺省为enforcing.selinux [--disabled|--enforcing|--permissive] --enforcing,启用SELinux,实施缺省的targeted policy. 注:如果kickstart文件里没有selinux选项,SELinux将被启用并缺省设置为--enforcing. --permissive,输出基于SELinux策略的警告,但实际上不执行这个策略. --disabled,在系统里完全地禁用 SELinux. user：在系统上创建新用户user --name= [--groups=] [--homedir=] [--password=] [--iscrypted] [--shell=] [--uid=] --name=,提供用户的名字.这个选项是必需的. --groups=,除了缺省的组以外,用户应该属于的用逗号隔开的组的列表. --homedir=,用户的主目录.如果没有指定,缺省为/home/. --password=,新用户的密码.如果没有指定,这个帐号将缺省被锁住. --iscrypted=,所提供的密码是否已经加密？ --shell=,用户的登录shell.如果不提供,缺省为系统的缺省设置. --uid=,用户的UID.如果未提供,缺省为下一个可用的非系统 UID. network：配置系统的网卡信息--device= 指定要使用network命令配置或者激活的设备。能够以和 ksdevice启动选项相同的方式指定。例如：network --bootproto=dhcp --device=eth0 对于第一个network命令，如果选项没有被指定，它默认是 1)ksdevice启动选项, 2)为了获得kickstart而激活的设备,或者 3)UI上的选择框。对于如下的network命令，需要--device选项。 --ip= 网络接口的IP地址。 --ipv6= 网络接口的IPv6地址。可以是[/]形式的静态地址，例如，3ffe:ffff:0:1::1/128(如果前缀被省略，会被假定为64)，\"auto\"地址分配基于动态的邻居发现协议，而\"dhcp\"会使用DHCPv6协议。 --gateway= 默认网关，是一个IPv4或者IPv6地址。 --nodefroute 组织设备抓取默认路由。在安装器中使用--activate选项激活其它设备时非常有用，从F16起。 --nameserver= 主域名服务器，是一个IP地址。多个域名服务器必须由逗号分隔。 --nodns 并不配置DNS服务器。 --netmask= 安装系统的网络掩码。 --hostname= 安装系统的主机名。 --ethtool= 指定将要传递给ethtool程序的设备附加的低级别设置。 --essid= 无线网网络ID。 --wepkey= 无线网WEP加密密钥。 --wpakey= 无线网WPA加密密钥(从F16起)。 --onboot= 是否在启动时使能该设备。 --dhcpclass= DHCP类别。 --mtu= 设备的MTU。 --noipv4 在该设备上禁用IPv4。 --noipv6 在该设备上禁用IPv6。 --bondslaves 使用该选项指定的网卡作为多网卡绑定的从网卡，虚拟出的网卡的名字由--device指定。例如--bondslaves=eth0,eth1。自Fedora 19开始。 --bondopts 为--bondslaves和--device选项指定的绑定接口指定一个逗号分隔的参数列表。例如：--bondopts=mode=active-backup,primary=eth1。如果一个选项本身以逗号作为分隔符，那么使用分号作为选项之间的分隔符。 --vlanid 使用--device指定的设备作为父设备来创建的vlan设备的Id(802.1q标签)。例如，network --device=eth0 --vlanid=171将会创建vlan设备eth0.171。从Fedora 19起。 logging：该命令控制anaconda安装过程中的错误日志，并不影响安装系统。--host=,发送日志信息到给定的远程主机,这个主机必须运行配置为可接受远程日志的syslogd进程. --port=,如果远程的syslogd进程没有使用缺省端口,这个选项必须被指定. --level=,debug,info,warning,error或critical中的一个.指定tty3上显示的信息的最小级别.然而,无论这个级别怎么设置,所有的信息仍将发送到日志文件. zerombr ：清除mbr信息，会同时清空系统用原有分区表clearpart：在建立新分区前清空系统上原有的分区表，默认不删除分区--all 擦除系统上原有所有分区； --drives 删除指定驱动器上的分区 --initlabel 初始化磁盘卷标为系统架构的默认卷标 --linux 擦除所有的linux分区 --none（default）不移除任何分区 ​ 例：clearpart --drives=hda,hdb --all --initlabel eula：使用这个选项以非用户互动方式接受终端用户许可证协议（End User License Agreement，EULA）。指定这个选项可防止 Initial Setup 在完成安装并第一次重启系统时提示您接受该许可证。--agreed（强制） - 接受 EULA。必须总是使用这个选项，否则 eula 命令就毫无意义。 ignoredisk:指定安装程序忽略指定的磁盘。如果您使用自动分区并希望忽略某些磁盘的话，这就很有用--only-use - 指定安装程序要使用的磁盘列表。忽略其他所有磁盘。 例如1：要在安装过程使用磁盘 sda，并忽略所有其他磁盘：ignoredisk --only-use=sda 要包括不使用 LVM 的多路经设备： ignoredisk --only-use=disk/by-id/dm-uuid-mpath-2416CD96995134CA5D787F00A5AA11017 要包括使用 LVM 的多路径设备： ignoredisk --only-use=disk/by-id/scsi-58095BEC5510947BE8C0360F604351918 --interactive - 允许手动导航高级存储页面。 repo：配置用于软件包安装来源的额外的yum库.可以指定多个repo行.repo --name=repoid [--baseurl=|--mirrorlist=url] [options] --name= - 该库的 id。这个选项是必选项。如果库名称与另一个之前添加的库冲突，则会忽略它。因为这个安装程序使用预先配置的库列表，就是说您无法添加名称与预先配置的库相同的库。 --baseurl= - 程序库的 URL。这里不支持 yum 库配置文件中使用的变量。可以使用这个选项，也可以使用 --mirrorlist，但不能同时使用这两个选项。 --mirrorlist= - URL 指向该程序库的一组镜像。这里不支持 yum 库配置文件中使用的变量。可以使用这个选项，也可以使用 --baseurl，但不能同时使用这两个选项。 --install - 将所安装系统提供的存储库配置保存在/etc/yum.repos.d/目录中。不使用这个选项，在 Kickstart 文件中配置的程序库只能在安装过程中使用，而无法在安装的系统中使用。 --cost= - 为这个库分配的 cost 整数值。如果多个库提供同样的软件包，这个数字就是用来规定那个库优先使用，cost 较低的库比 cost 较高的库优先。 --excludepkgs= - 逗号分开的软件包名称列表，同时一定不能从这个存储库中提取该软件包名称。如果多个库提供同样的软件包，且要保证其来自某个特定存储库。 可接受完整软件包名称（比如 publican）和 globs（比如 gnome-*）。 --includepkgs= - 逗号分开的软件包名称列表，同时一定要从这个存储库中提取 glob。如果多个存储库提供同样的软件包，且要保证其来自某个特定存储库，这个选项就很有用了。 --proxy=[protocol://][username[:password]@]host[:port] - 指定只有这个存储库使用的 HTTP/HTTPS/FTP 代理服务器。 这个设置不会影响其他存储库，也不会影响将 install.img 附加到 HTTP 安装的方法。 --ignoregroups=true - 组成安装树时使用这个选项，且对安装过程本身没有影响。它告诉组合工具在镜像树时不要查看软件包组信息，这样就不会镜像大量无用数据。 --noverifyssl - 连接到 HTTPS 服务器时禁止 SSL 验证。 interactive：在安装过程中使用kickstart文件里提供的信息,但允许检查和修改给定的值.将遇到安装程序的每个屏幕以及kickstart文件里给出的值.autostep：通常 Kickstart 安装会跳过不必要的页面。这个选项可让安装程序浏览所有页面，并摘要显示每个页面。部署系统时不应使用这个选项，因为它会干扰软件包安装--autoscreenshot 在安装的每一步均截屏。这些截屏将在安装过程中保存在 /tmp/anaconda-screenshots 中，并在安装完成后保存在 /root/anaconda-screenshots 中。 sshpw：安装过程中是否开启SSH与安装进程进行交互与监控sshpw --username=name password [--iscrypted|--plaintext] [--lock] --username --iscrypted --plaintext --lock autopart：自动生成分区（autopart 选项不能与 part/partition, raid、logvol 或者 volgroup 在同样的 Kickstart 文件中一同使用。）--type= - 选择您要使用的预先定义的自动分区方案之一。可接受以下值： lvm: LVM 分区方案。 btrfs: Btrfs 分区方案。 plain: 不附带 LVM 或者 Btrfs 的常规分区。 thinp: LVM 精简分区方案。 --fstype= - 选择可用文件系统类型之一。可用值为 ext2、ext3、ext4、xfs 和 vfat。默认系统为 xfs。有关使用这些文件系统的详情，请查看 第 6.14.4.1.1 节 “文件系统类型”。 --nolvm - 不使用 LVM 或者 Btrfs 进行自动分区。这个选项等同于 --type=plain。 --encrypted - 加密所有分区。这等同于在手动图形安装过程的起始分区页面中选中 加密分区 复选框。 # 注意：加密一个或多个分区时，Anaconda 尝试收集 256 字节熵，以保证对分区安全加密与安装系统互动可加速此进程（使用键盘输入或移动鼠标）。 # 如果要在虚拟机中安装系统，则可添加 virtio-rng 设备（虚拟随机数生成器）， --passphrase= - 为所有加密设备提供默认的系统范围内的密码短语。 --escrowcert=URL_of_X.509_certificate - 将所有加密卷数据加密密码保存在 /root 中，使用来自 URL_of_X.509_certificate 指定的 URL 的 X.509 证书加密。每个加密卷的密码都作为单独的文件保存。只有指定 --encrypted 时这个选项才有意义。 --backuppassphrase - 为每个加密卷添加随机生成的密码短语。将这些密码保存在 /root 目录下的独立文件中，使用 --escrowcert 指定的 X.509 证书加密。只有指定 --escrowcert 时这个选项才有意义。 --cipher= - 如果指定 Anaconda 默认 aes-xts-plain64 无法满足需要，则可以指定要使用的加密类型。这个选项必须与 --encrypted 选项一同使用，单独使用无效。 《Red Hat Enterprise Linux 7 安全指南》中有可用加密类型列表，但 Red Hat 强烈推荐您使用 aes-xts-plain64 或者 aes-cbc-essiv:sha256。 firewall：配置系统防火墙选项firewall –enable|--disable [ --trust ] [ --port= ] --enable 拒绝回应输出要求的进入连接，比如 DNS 答复或 DHCP 请求。如果需要访问在这台机器中运行的服务，可以选择通过防火墙允许具体的服务。 --disable 不配置任何iptables防御规则； --trust 在这里列出设备,比如em1,允许所有流量通过该防火墙进出那个设备.要列出一个以上的设备,请使用--trust em1 --trust em2。不要使用逗号分开的格式，比如 --trust em1, em2。 --port 可以用端口:协议（port:protocal）格式指定允许通过防火墙的端口。例如，如果想允许 IMAP 通过您的防火墙，可以指定 imap:tcp。还可以具体指定端口号码，要允许 UDP 分组 在端口 1234 通过防火墙，输入 1234:udp。要指定多个端口，用逗号将它们隔开。 --service= 这个选项提供允许服务通过防火墙的高级方法。有些服务（比如 cups、avahi 等等）需要开放多个端口，或者另外有特殊配置方可工作。 您应该使用 --port 选项指定每个具体端口，或者指定 --service= 并同时打开它们 incoming - 使用以下服务中的一个或多个来替换，从而允许指定的服务通过防火墙。 --ssh --smtp --http --ftp ​ 示例：firewall --enable --trust eth0 --trust eth1 --port=80:tcp group:在系统中生成新组。如果某个使用给定名称或者 GID 的组已存在，这个命令就会失败。另外，该 user 命令可用来为新生成的用户生成新组group --name=name [--gid=gid] --name= - 提供组名称。 --gid= - 组的 UID。如果未提供，则默认使用下一个可用的非系统 GID。 volgroup：创建逻辑卷组vgvolgroup name partition [options] ​ --noformat - 使用现有卷组，且不进行格式化。 --useexisting - 使用现有卷组并重新格式化。如果使用这个选项，请勿指定 partition。例如：volgroup rhel00 --useexisting --noformat --pesize= - 以 KiB 为单位设定卷组物理扩展大小。默认值为 4096 (4 MiB)，最小值为 1024 (1 MiB)。 --reserved-space= - 以 MB 为单位指定在卷组中预留的未使用空间量。只适用于新生成的卷组。 --reserved-percent= - 指定卷组中预留未使用空间的比例。只适用于新生成的卷组。 ​ 注意1 ：不要在逻辑卷和卷组名称中使用小横线（-）。如果使用这个字符，会完成安装，但 /dev/mapper/ 目录列出这些卷和卷组时，小横线会加倍。例如：某个卷组名为 volgrp-01，包含名 为 logvol-01 逻辑卷，该逻辑卷会以 /dev/mapper/volgrp--01-logvol--01 列出。这个限制只适用于新创建的逻辑卷和卷组名。如果您使用 --noformat 选项重复使用现有名称， 它们的名称就不会更改。 注意2: 应该先创建分区，然后创建逻辑卷组，再创建逻辑卷。例如： part pv.01 --size 10000 volgroup volgrp pv.01 logvol / --vgname=volgrp --size=2000 --name=root logvol：创建逻辑卷lvlogvol mntpoint --vgname=name --name=name [options] mntpoint — 是该分区挂载的位置，且必须是以下格式之一： 1. /path 例如：/ 或者 /home 2.swap 该分区被用作交换空间。要自动决定 swap 分区的大小，使用 --recommended 选项：swap --recommended 使用 --hibernation 选项自动决定 swap 分区的大小，同时还允许 您的系统有附加空间以便可以休眠：swap --hibernation分配的分区大小将与 --recommended 加上系统 RAM 量相等。 这些选项如下所示： --noformat- 使用现有逻辑卷且不要对其进行格式化。 --useexisting 使用现有逻辑卷并重新格式化它。 --fstype= 为逻辑卷设置文件系统类型。有效值有：xfs、ext2、ext3、ext4、swap 和 vfat。 --fsoptions= 指定在挂载文件系统时所用选项的自由格式字符串。将这个字符串复制到安装的系统的 /etc/fstab 中，并使用括号括起来。 --mkfsoptions= 指定要提供的附加参数，以便在这个分区中建立文件系统。没有对任何参数列表执行任何操作，因此必须使用可直接为 mkfs 程序提供的格式。 就是说可使用逗号分开或双引号分开的多个选项，要看具体文件系统。 --label= 为逻辑卷设置标签。 --grow 会让逻辑卷使用所有可用空间（若有），或使用设置的最大值（如果指定了最大值）。必须给出最小值，可使用 --percent= 选项或 --size= 选项。 --size= 以 MB 单位的逻辑卷大小。这个选项不能与 --percent= 选项一同使用。 --percent= 考虑任何静态大小逻辑卷时的逻辑卷大小，作为卷组中剩余空间的百分比。这个选项不能与 --size= 选项一同使用。 #重要:创建新逻辑卷时，必须使用 --size= 选项静态指定其大小，或使用 --percent= 选项指定剩余可用空间的百分比。不能再同一逻辑卷中同时使用这些选项。 --maxsize= - 当将逻辑卷被设置为可扩充时以 MB 为单位的最大值。在这里指定一个整数值，如500（不要在数字后添加单位）。 --recommended - 创建 swap 逻辑卷时可采用这个选项，以根据您的系统硬件自动决定这个卷的大小。 --resize - 重新定义逻辑卷大小。如果使用这个选项，则必须还指定 --useexisting 和 --size。 --encrypted - 指定该逻辑卷应该用 --passphrase= 选项提供的密码进行加密。如果没有指定密码短语，安装程序将使用 autopart --passphrase 命令指定默认系统级密码， 如果没有设定默认密码则会停止安装，并提示输入密码短语。 # 注意:加密一个或多个分区时，Anaconda 尝试收集 256 字节熵，以保证对分区安全加密与安装系统互动可加速此进程（使用键盘输入或移动鼠标）。如果要在虚拟机中安装系统，则可 # 添加 virtio-rng 设备（虚拟随机数生成器） --passphrase= - 指定在加密这个逻辑卷时要使用的密码短语。必须与 --encrypted 选项一同使用，单独使用这个选项无效。 --cipher= - 指定如果对 Anaconda 默认 aes-xts-plain64 不满意时要使用的加密类型。这个选项必须与 --encrypted 选项一同使用，单独使用无效。 推荐使用 aes-xts-plain64 或者 aes-cbc-essiv:sha256。 --escrowcert=URL_of_X.509_certificate 将所有加密卷数据加密密钥作为文件保存在 /root 中，使用来自 URL_of_X.509_certificate 指定的 URL 的 X.509 证书加密。每个加密卷 的密钥都作为单独的文件保存。只有指定 --encrypted 时这个选项才有意义。 --backuppassphrase - 为每个加密卷添加随机生成的密码短语。将这些密码保存在 /root 目录下的独立文件中，使用 --escrowcert 指定的 X.509 证书加密。只有指定 --escrowcert 时 这个选项才有意义。 --thinpool - 创建精简逻辑卷。（使用 none 挂载点）。 --metadatasize=size - 为新的精简池设备指定元数据大小（单位 MiB）。 --chunksize=size - 为新的精简池设备指定块大小（单位 KiB）。 --thin - 创建精简逻辑卷。（要求使用 --poolname） --poolname=name - 指定在其中创建精简逻辑卷的精简池名称。需要 --thin 选项。 --profile=name 指定与精简逻辑卷配合使用的配置文件名称。如果使用此选项，还要用于给定逻辑的卷元数据中包含该名称。默认情况下，可使用的配置文件为在 /etc/lvm/profile 目录中定 义的 default 和 thin-performance。详情请查看 lvm(8) 手册页。 --cachepvs= - 用逗号分开的物理卷列表，应作为这个卷的缓存使用。 --cachemode= - 指定应使用哪种模式缓存这个逻辑卷 - 可以是 writeback，也可以是 writethrough。 #注意:有关缓存的逻辑卷及其模式的详情，请查看 lvmcache(7) 手册页。 --cachesize= - 附加到该逻辑卷的缓存大小，单位为 MiB。这个选项需要 --cachepvs= 选项。 ​ 注意1: 应该先创建分区，然后创建逻辑卷组，再创建逻辑卷以占据逻辑组里剩余的 90% 空间。例如：： part pv.01 --size 1 --grow volgroup myvg pv.01 logvol / --vgname=myvg --name=rootvol --percent=90 timezone：设置系统的时区timezone [ --utc ] 例：timezone --utc Asia/Shanghai 软件包的选择在 Kickstart 文件中使用 %packages 命令列出要安装的软件包。 可以根据环境、组或者其软件包名称指定软件包。安装程序定义包含相关软件包的几个环境和组。有关环境和组列表请查看安装光盘中的 repodata/*-comps-variant.architecture.xml 文件。 *-comps-variant.architecture.xml 文件包含描述可用环境（使用 标签标记）和组（ 标记）的结构。每个组都有一个 ID、用户可见性数值、名称、描述和软件包列表。如果未安装选择该组，那么就会安装该软件包列表中标记为 mandatory 的软件包；如果未明确指定，也会安装标记为 default 的软件包，而标记为 optional 的软件包必须在明确指定后方可安装。 您可以使用 ID（ 标签）或者名称（ 标签）指定软件包组或者环境。 %packages 部分必须以 %end 命令结尾。 除组外，您还要指定要安装的整体环境 %packages @^Infrastructure Server %end 指定组，每个条目一行，以 @ 符号开始，接着是空格，然后是完整的组名或 *-comps-variant.architecture.xml 中指定的组 id。 %packages @X Window System @Desktop @Sound and Video %end 根据名称指定独立软件包，每行一个条目。您可以在软件包名称中使用星号（*）作为通配符。 %packages sqlite curl aspell docbook* %end 使用小横线（-）开头指定安装中不使用的软件包或组。 %packages -@Graphical Internet -autofs -ipa*fonts %end 常用软件包选择选项以下选项可用于 %packages。要使用这个选项，请将其添加到软件包选择部分的开始。例如： %packages --multilib --ignoremissing ​ --default 安装默认软件包组。这与在互动安装过程中的 软件包选择 页面中没有其他选择时要安装的软件包组对应。 --excludedocs 不要安装软件包中的任何文档。大多数情况下，这样会排除一般安装在 /usr/share/doc* 目录中的所有文件，但要排除的具体文件取决于各个软件包。 --ignoremissing 忽略所有在这个安装源中缺少的软件包、组及环境，而不是暂停安装询问是应该放弃还是继续安装。 --instLangs= 指定要安装的语言列表。注：这与软件包组等级选择不同。这个选项不会告诉您应该安装哪些软件包组，而是通过设置 RPM 宏控制应该安装独立软件包中的哪些事务文件。 --multilib 为 multilib 软件包配置已安装的系统（即允许在 64 位系统中安装 32 位软件包），并安装在这一部分指定的软件包。通常在 AMD64 和 Intel 64 系统中，只安装用于整个架构 （标记为 x86_64）的软件包以及用于所有架构（标记为 noarch）软件包。使用这个选项时，将自动安装用于 32 位 AMD 系统 Intel（标记为 i686）的软件包。这只适用于在 %packages 部分明确指定的软件包。对于那些仅作为相依性安装而没有在 Kickstart 文件中指定的软件包，将只安装其所需架构版本，即使有更多可用架构也是如此。 --nocore 禁用默认总被安装的 @Core 软件包组。禁用 @Core 软件包组应只用于创建轻量级的容器；用 --nocore 安装桌面或服务器系统将导致系统不可用。 具体软件包组参数项这个列表中的选项只用于单一软件包组。不是在 Kickstart 文件的 %packages 命令中使用，而是在组名称中添加条目。例如： %packages @Graphical Internet --optional %end ​ --nodefaults 只安装该组的强制软件包，不是默认选择。 --optional 除安装默认选择外，还要安装在 *-comps-variant.architecture.xml 文件组定义中标记为自选的软件包。 注：有些软件包组，比如 Scientific Support，没有指定任何强制或默认软件包 - 只有自选软件包。在这种情况下必须使用 --optional 选项，否则不会安装这个组中的任何软件包。 安装前脚本ks.cfg文件被解析后马上加入要运行的命令.这个部分必须处于kickstart文件的最后(在命令部分之后)而且 必须用%pre命令开头，%end结尾. 可以在%pre部分访问网络；然而,此时命名服务还未被配置,所以只能使用IP地址. 预安装脚本不会在 chroot 环境中运行 --interpreter= 定义脚本运行解释器.常用的有:/usr/bin/sh, /usr/bin/bash, and /usr/bin/python. --erroronfail 如果脚本失败则显示出错信息并暂停安装。该出错信息可让您进入记录失败原因的位置。 --log= 记录脚本运行过程中的信息到指定路径的文件中。例如：%pre --log=/mnt/sysimage/root/ks-pre.log 示例%pre #!/bin/sh hds=\"\" mymedia=\"\" for file in /proc/ide/h* do mymedia=`cat $file/media` if [ $mymedia == \"disk\" ] ; then hds=\"$hds `basename $file`\" fi done set $hds numhd=`echo $#` drive1=`echo $hds | cut -d' ' -f1` drive2=`echo $hds | cut -d' ' -f2` ​ #Write out partition scheme based on whether there are 1 or 2 hard drives if [ $numhd == \"2\" ] ; then #2 drives echo \"#partitioning scheme generated in %pre for 2 drives\" > /tmp/part-include echo \"clearpart --all\" >> /tmp/part-include echo \"part /boot --fstype xfs --size 75 --ondisk hda\" >> /tmp/part-include echo \"part / --fstype xfs --size 1 --grow --ondisk hda\" >> /tmp/part-include echo \"part swap --recommended --ondisk $drive1\" >> /tmp/part-include echo \"part /home --fstype xfs --size 1 --grow --ondisk hdb\" >> /tmp/part-include else #1 drive echo \"#partitioning scheme generated in %pre for 1 drive\" > /tmp/part-include echo \"clearpart --all\" >> /tmp/part-include echo \"part /boot --fstype xfs --size 75\" >> /tmp/part-include echo \"part swap --recommended\" >> /tmp/part-include echo \"part / --fstype xfs --size 2048\" >> /tmp/part-include echo \"part /home --fstype xfs --size 2048 --grow\" >> /tmp/part-include fi %end 安装后脚本post-install 脚本是在 chroot 环境里运行的.因此,某些任务如从安装介质复制脚本或RPM将无法执行.--nochroot 允许指定想在chroot环境之外运行的命令 --interpreter 定义脚本运行解释器.常用的有:/usr/bin/sh, /usr/bin/bash, and /usr/bin/python. --erroronfail 如果脚本失败则显示出错信息并暂停安装。该出错信息可让您进入记录失败原因的位置。 --log= 记录脚本运行过程中的信息到指定路径的文件中。例如：%pre --log=/mnt/sysimage/root/ks-pre.log 示例%post --nochroot cp /etc/resolv.conf /mnt/sysimage/etc/resolv.conf %end 四、示例**全新安装系统** install #在文本模式下安装 text #指定安装过程中语言为英语 lang en_US.UTF-8 #指定键盘类型为US布局 keyboard us ​ auth --useshadow --passalgo=sha512 # url --url=\"http://192.168.1.80/CentOS7\" #设置root用户密码 rootpw --iscrypted $1$6/87AF3n$eczKeiNRBv7H.GXnur1Ld/ #开启SELinux selinux --enforcing #关闭防火墙 firewall --disabled #设置网络信息。网卡1设置手动获取IP，不初始化IPV6,不设置为默认路由。网卡2设置DHCP获取IP，不初始化IPV6,设置为默认路由。 network --bootproto=static --ip=192.168.10.6 --device=enp0s3 --activate --nodefroute --noipv6 --nameserver=114.114.114.114 --netmask=24 --gateway=192.168.10.1 network --bootproto=dhcp --device=enp0s8 --activate --nameserver=114.114.114.114 --noipv6 #设置主机名 network --hostname=test #安装完成后重启 reboot #设置时区为上海的时区 timezone Asia/Shanghai --isUtc --nontp #将BootLoader安装在sda磁盘上 bootloader --location=mbr --boot-drive=sda #安装时清理sda磁盘上所有的分区 clearpart --all --drives=sda #将SSH和NetworkManager设置为开机自启动 services --enabled=NetworkManager,sshd firstboot --enable #只使用sda磁盘 ignoredisk --only-use=sda #对sda磁盘进行分区。单位是MB part /boot --fstype=\"xfs\" --ondisk=sda --size=200 part / --fstype=\"xfs\" --ondisk=sda --size=30720 part /opt --fstype=\"xfs\" --ondisk=sda --size=10240 part /var --fstype=\"xfs\" --grow --ondisk=sda --size=1 ​ %packages @^minimal @core %end #安装后要执行的脚本 %post --interpreter=/bin/bash --log=/root/post-install.log mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak cat >> /etc/yum.repos.d/ustc.repo /dev/null yum makecache > /dev/null yum install -y tree vim telnet nc unzip git net-tools wget bind-utils > /dev/null echo \"Set HOSTNAME test\" echo \"Disabled SELinux and Firewall\" echo \"/dev/sda /boot xfs 200MB\" echo \"/dev/sda / xfs 30G\" echo \"/dev/sda /opt xfs 10G\" echo \"/dev/sda /var xfs RemainingCapacity\" echo \"Make Yum Repository To USE USTC Yum Repository \" echo \"Installed Tools : tree vim telnet nc unzip git net-tools wget bind-utils\" echo \" #######################\" >> /etc/motd echo \" # Keep Your Curiosity #\" >> /etc/motd echo \" #######################\" >> /etc/motd %end 五、验证KS文件的语法正确性yum install -y pykickstart ksvalidator ks文件 六、比较OS不同版本间的KS语法差异ksverdiff -f RHEL6 -t RHEL7 # -f 指定要比较的第一个发行本，-t 指定要比较的最后一个发行本 参考连接★★★★★： https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/installation_guide/sect-kickstart-syntax ★★★★★：https://docs.centos.org/en-US/centos/install-guide/Kickstart2/#sect-kickstart-commands ★★★★★ https://blog.csdn.net/yanghua1012/article/details/80426659Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:11 "},"origin/pxe-引导配置文件参数详解.html":{"url":"origin/pxe-引导配置文件参数详解.html","title":"PXE引导配置文件参数详解","keywords":"","body":"default ks 　　　　#默认启动的是 'label ks' 中标记的启动内核 prompt 1 #显示 'boot: ' 提示符。为 '0' 时则不提示，将会直接启动 'default' 参数中指定的内容。 timeout 6 　　　　 #在用户输入之前的超时时间，单位为 1/10 秒。 display boot.msg #显示某个文件的内容，注意文件的路径。默认是在/var/lib/tftpboot/ 目录下。也可以指定位类似 '/install/boot.msg'这样的，路径+文件名。 F1 boot.msg 　　　 #按下 'F1' 这样的键后显示的文件。 F2 options.msg F3 general.msg F4 param.msg F5 rescue.msg label linux #'label' 指定你在 'boot:' 提示符下输入的关键字，比如boot: linux[ENTER]，这个会启动'label linux' 下标记的kernel 和initrd.img 文件。 kernel vmlinuz #kernel 参数指定要启动的内核。 append initrd=initrd.img #append 指定追加给内核的参数，能够在grub 里使用的追加给内核的参数，在这里也都可以使用。 label text kernel vmlinuz append initrd=initrd.img text label ks kernel vmlinuz append ks=http://192.168.111.130/ks.cfg initrd=initrd.img #告诉系统，从哪里获取ks.cfg文件 label local localboot 1 label memtest86 kernel memtest append - default menu.c32 prompt 1 timeout 10 menu title ########## PXE Boot Menu ########## label 1 menu label ^1) Install CentOS 7 x64 with Local Repo menudefault kernel centos7/vmlinuz append initrd=centos7/initrd.img text ks=ftp://192.168.100.1/pub/ks.cfg label 2 menu label ^2) Install CentOS 7 x64 with http://mirror.centos.org Repo kernel centos7/vmlinuz append initrd=centos7/initrd.img method=http://mirror.centos.org/centos/7/os/x86_64/ devfs=nomount ip=dhcp label 3 menu label ^3) Install CentOS 7 x64 with Local Repo using VNC kernel centos7/vmlinuz append initrd=centos7/initrd.img method=ftp://192.168.100.1/pub devfs=nomount inst.vnc inst.vncpassword=password Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:11 "},"origin/tool-SublimeText.html":{"url":"origin/tool-SublimeText.html","title":"Sublime Text 3","keywords":"","body":"Sublime Text使用总结一、简介Sublime Text是一款具有代码高亮、语法提示、自动完成且反应快速的编辑器软件Sublime Text具有漂亮的用户界面和强大的功能，例如代码缩略图，Python的插件，代码段等。还可自定义键绑定，菜单和工具栏。Sublime Text 的主要功能包括：拼写检查，书签，完整的 Python API ， Goto 功能，即时项目切换，多选择，多窗口等等。Sublime Text 是一个跨平台的编辑器，同时支持Windows、Linux、Mac OS X等操作系统。二、安装Sublime Text官网：http://www.sublimetext.com/3三、插件管理器Package ControlPackage Control：https://packagecontrol.io/installation使用Ctrl + `打开Sublime Text控制台 将下面的代码粘贴到控制台里 Sublime Text 3import urllib.request,os,hashlib; h = '6f4c264a24d933ce70df5dedcf1dcaee' + 'ebe013ee18cced0ef93d5f746d80ef60'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by) Sublime Text 2import urllib2,os,hashlib; h = '6f4c264a24d933ce70df5dedcf1dcaee' + 'ebe013ee18cced0ef93d5f746d80ef60'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); os.makedirs( ipp ) if not os.path.exists(ipp) else None; urllib2.install_opener( urllib2.build_opener( urllib2.ProxyHandler()) ); by = urllib2.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); open( os.path.join( ipp, pf), 'wb' ).write(by) if dh == h else None; print('Error validating download (got %s instead of %s), please try manual install' % (dh, h) if dh != h else 'Please restart Sublime Text to finish installation') 给Package Controller设置代理Sublime Text > Preferences > Package Settings > Package Control > Settings - User 编辑 Package Control.sublime-settings，添加两行:\"http_proxy\": \"http://代理IP地址:3128\", \"https_proxy\": \"http://代理IP地址:3128\", 解决无法安装插件问题使用第三方Channel，见附件Preference-->Package Settings-->Package Control-->Settings User{ \"bootstrapped\": true, \"channels\": [ \"D:/Sublime Text 3/data/channel_v3.json\" ] } 四、常用快捷键 快捷键 功能 **Ctrl+H** 查找替换 **Ctrl+F** 查找内容 **Ctrl+Shift+F** 在文件夹内查找内容，可进行替换 **Ctrl+L** 选择一行 **Ctrl+Shift+D** 复制当前行到下行 **Ctrl+K+U** 大写光标所在词 **Ctrl+K+L** 小写光标所在词 **Ctrl+Shift+D** 复制光标所在整行到下一行 **Ctrl+Shift+L** 鼠标选中多行（按下快捷键），即可同时编辑这些行 **Ctrl+Shift+↑/↓** 可以移动此行代码，与上/下行互换 **Ctrl+Shift+←/→** 向右/向右单位性地选中文本 **Alt+Shift+1** 窗口分屏，恢复默认1屏（非小键盘的数字） **Alt+Shift+2** 左右分屏-2列 **Alt+Shift+3** 左右分屏-3列 **Alt+Shift+4** 左右分屏-3列 **Alt+Shift+5** 等分4屏 **Alt+Shift+8** 垂直分屏-2屏 **Alt+Shift+9** 垂直分屏-3屏 **Ctrl+K+B** 开启/关闭侧边栏 **Ctrl+/** 注释单行 **Ctrl+Shift+/** 注释多行 **Ctrl+K+K** 从光标处开始删除代码至行尾 **Ctrl+Shift+K** 删除整行 **Tab** 向右缩进 **Shift+Tab** 向左缩进 **Ctrl+J** 合并选中的多行代码为一行 **Shift+↑/↓/←/→** 向上/下/左/右选中文本 **Ctrl+K+0** 展开所有折叠代码 **Ctrl+M** 光标移动至括号内结束或开始的位置 **Ctrl+Shift+M** 选择括号内的内容 **Ctrl+D** 选中光标所占的文本，继续操作则会选中下一个相同的文本 **Alt+F3** 选中文本按下快捷键，即可一次性选择全部的相同文本进行同时编辑 **Ctrl+G** 跳转到第几行 **Ctrl+Shift+W** 关闭所有打开文件 **Ctrl+Shift+V** 粘贴并格式化 **Ctrl+X** 删除当前行 **Ctrl+Z** 撤销 **Ctrl+Y** 恢复撤销 **Ctrl+U** 软撤销 **Ctrl+T** 左右字母互换 **Ctrl+Tab** 按文件浏览过的顺序，切换当前窗口的标签页 **Ctrl+PageDown** 向左切换当前窗口的标签页 **Ctrl+PageUp** 向右切换当前窗口的标签页 **Ctrl+W** 关闭当前打开文件 **Ctrl+Shift+W** 关闭所有打开文件 **Ctrl+Shift+P** 打开命令面板 **Ctrl+：** 打开搜索框，自动带#，输入关键字，查找文件中的变量名、属性名等。 **Ctrl+R** 打开搜索框，自动带@，输入关键字，查找文件中的函数名 **Ctrl+P** 打开搜索框。1、输入当前项目中的文件名，快速搜索文件2、@和关键字，查找文件中函数名3、：和数字，跳转到文件中该行代码4、#和关键字，查找变量名 五、常用插件 插件名 功能 描述 **DeleteBlankLines** 去除文本中的空白行 Windows: Ctrl+Alt+Backspace --> Delete Blank Lines Ctrl+Alt+Shift+Backspace --> Delete Surplus Blank LinesLinux: Ctrl+Alt+Backspace --> Delete Blank Lines Ctrl+Alt+Shift+Backspace --> Delete Surplus Blank Lines **ChineseLocalizations** 汉化Sublime Text 请使用主菜单的 帮助/Language 子菜单来切换语言。 目前支持 简体中文 繁体中文 日本語。 要换回英语不需要卸载本插件，请直接从菜单切换英文。 **HTML-CSS-JS Prettify** HTML/CSS/JS代码格式化 需要安装NodeJS并设置node执行文件的路径右键-->HTML-CSS-JS Prettify-->Set ‘node’ Path **GBK Encoding Support** 支持gbk编码 **Alignment** 代码格式的自动对齐 默认快捷键Ctrl+Alt+A **Clipboard History** 粘贴板历史记录，方便使用复制/剪切的内容 Ctrl+alt+v：显示历史记录Ctrl+alt+d：清空历史记录Ctrl+shift+v：粘贴上一条记录（最旧）Ctrl+shift+alt+v：粘贴下一条记录（最新） **ConvertToUTF8** 编辑并保存目前编码不被 Sublime Text 支持的文件 **IMESupport** 支持中文输入法跟随光标 **AutoFileName** 自动完成文件名的输入，如图片选取 **Trailing spaces** 检测并一键去除代码中多余的空格 一键删除多余空格：CTRL+SHITF+T（需配置），更多配置请点击标题。快捷键配置：在Preferences / Key Bindings – User加上{ \"keys\": [\"ctrl+shift+t\"], \"command\": \"delete_trailing_spaces\" } **FileDiffs** 比较当前文件与选中的代码、剪切板中代码、另一文件、未保存文件之间的差别。可配置为显示差别在外部比较工具，精确到行。 右键标签页，出现FileDiffs Menu或者Diff with Tab…选择对应文件比较即可 **DocBlockr** 生成优美注释 标准的注释，包括函数名、参数、返回值等，并以多行显示，手动写比较麻烦。输入/*、/**然后回车，还有很多用法，请参照https://sublime.wbond.net/packages/DocBlockr **SideBarEnhancements** 增强型侧边栏 **Terminal** 接使用终端打开你的项目文件夹，并支持使用快捷键。 默认调用系统自带的`PowerShell` ctrl+shift+t 打开文件所在文件夹，ctrl+shift+alt+t 打开文件所在项目的根目录文件夹 **SFTP** 快速编辑远程服务器上的文件 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-14 00:00:15 "},"origin/windows-cmd发送SMTP邮件.html":{"url":"origin/windows-cmd发送SMTP邮件.html","title":"CMD发送SMTP邮件","keywords":"","body":"Preflight邮箱开启POP3/SMTP和IMAP/SMTP服务一、操作1. windows开启telnet服务打开控制面板，找到“打开或关闭windows功能”（在“程序”里面），选中对话框中的Telnet客户端，然后确定，等待完成。这时就开启了telnet功能。 2. 在命令刚窗口输入telnet smtp.163.com 25 3. 向服务器表明身份helo 163.com # 如果成功，服务器返回 250 OK 4. 登录认证auth login # 用户名的Base64加密字符。如果成功，服务器返回一串字符，类似于：334 UGFzc3dvcmQ6（334 是不变的，后面的字母可能会变） ***** # 密码的Base64加密字符，如果登录成功，服务器返回一串字符：235 Authentication successful表示登录成功，如果不能成功登录，请检查账号密码是否正确。 ***** # 对于字符串的Base64加密可使用CMD中的“certutil -encode 包含想要加密字符串的文本文件 Base64加密后输出文本文件” 5. 填写发件人和收件人邮箱地址mail from: # 若格式不正确，服务器返回501 错误；若格式正确，服务器返回250 Ok。 rcpt to: # 若格式不正确，服务器返回501 错误；若格式正确，服务器返回250 Ok。 6. 编写邮件data # 服务器返回 354 End data with . To:******@163.com From:******@163.com Subject:test mail From:******@163.com test body 123 . # 服务器返回 250 Ok: queues as ... 表示邮件已经发送 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:13 "},"origin/windows-小技巧.html":{"url":"origin/windows-小技巧.html","title":"Windows小技巧","keywords":"","body":"1. CMD下的换行符在CMD下,可以用^作为换行符,类似于Linux下的\\ 2. CMD下查看端口使用情况netstat -ano |findstr 8080 3. CMD下杀掉进程taskkill /pid 8080 -t -f 4. CMD下校验文件的MD5、SHA1、SHA256值certutil -hashfile yourfilename.ext MD5 certutil -hashfile yourfilename.ext SHA1 certutil -hashfile yourfilename.ext SHA256 5. CMD下激活windows系统以管理员身份运行CMD卸载之前的激活密钥slmgr -upk 设置KMS服务器slmgr -skms KMS服务器 ​ 常用的KMS服务器kms.03k.org kms.chinancce.com kms.lotro.cc cy2617.jios.org kms.shuax.com kms.luody.info kms.cangshui.net zh.us.to 122.226.152.230 kms.digiboy.ir kms.library.hk kms.bluskai.com 输入新的密钥slmgr -ipk 激活密钥 密钥win10专业版密钥 W269N-WFGWX-YVC9B-4J6C9-T83GX ​ 激活slmgr -ato 6. PowerShell下载文件$client = new-object System.Net.WebClient $client.DownloadFile('#1', '#2') # #1为下载链接 #2为文件保存的路径 Note：一定要在路径中写上保存的新文件的全名（包括后缀） 建议保存的文件格式与下载的文件格式一致 7. 离线安装.NET Framework 3.5Preflightwindows 10 的系统ISO镜像 以管理员身份运行的CMD 将ISO镜像中source/sxs目录拷贝到某个路径下（以桌面为例）在以管理员身份运行的CMD执行以下命令dism.exe /online /enable-feature /featurename:netfx3 /Source:C:\\Users\\user\\Desktop\\sxs 8. 添加开机自启动bat脚本方法一：（推荐）将脚本放置“C:\\Users\\Curiouser\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup”路径下 方法二： 9. 修改远程桌面的默认端口3389Windows+R,输入regedit，打开注册表，修改一下注册表的值(十进制)，然后重启远程桌面HKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\Terminal Server\\WinStations\\RDP-Tcp\\PortNumber 防火墙放行新指定的远程桌面端口10. 防火墙放行指定端口 11. CMD下的用户管理net user：查看目前系统存在的用户 net user username：查看用户的详细信息 whoami：查看计算机当前登陆的用户 query user：查看已登陆用户的详细信息 logoff+空格+ID号：注销用户 net user 用户名 密码 /add：新增本地用户 net localgroup administrators 用户名 /add：将本地用户加入管理员用户组 net user 用户名 /del：删除用户 runas /user:用户 cmd：以某个用户运行命令 12. Windows软件授权管理工具slmgr命令 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-12 22:56:55 "},"origin/linux-小技巧.html":{"url":"origin/linux-小技巧.html","title":"Linux小技巧","keywords":"","body":"1、Linux SSH安全设置只允许某用户从指定IP地址登陆sed -i '$a AllowUsers CR@192.168.1.12 root@192.168.1.12' /etc/ssh/sshd_config ;\\ systemctl restart sshd 修改会话保持时间#ClientAliveInterval 0 #ClientAliveCountMax 3 修改成 ClientAliveInterval 30 #（每30秒往客户端发送会话请求，保持连接） ClientAliveCountMax 3 #（去掉注释即可，3表示重连3次失败后，重启SSH会话） 增加ssh登陆的验证次数MaxAuthTries 20 允许root用户登录sed -i 's/PermitRootLogin no/PermitRootLogin yes/g' /etc/ssh/sshd_config ;\\ systemctl restart sshd 设置登录方式#AuthorizedKeysFile .ssh/authorized_keys //公钥公钥认证文件 #PubkeyAuthentication yes //可以使用公钥登录 #PasswordAuthentication no //不允许使用密码登录 2、bash不显示路径命令行会变成-bash-3.2$主要原因可能是用户主目录下的配置文件丢失# 方式一 cp -a /etc/skel/. ~ # 方式二 echo \"export PS1='[\\u@\\h \\W]\\$'\" >> ~/.bash_profile ;\\ source ~/.bash_profile 3、同时监控多个文件tail -f file1 file2 4、查看网卡# 方式一 ifconfig -a # 方式二 cat /proc/net/dev 5、cp目录下的带隐藏文件的子目录cp -R /home/test/* /tmp/test /home/test下的隐藏文件都不会被拷贝，子目录下的隐藏文件倒是会的cp -R /home/test/. /tmp/test cp的时候有重复的文件需要覆盖时会让不停的输入yes来确认，可以使用yes|yes|cp -r /home/test/. /tmp/test 6、查看CPU占用最多的前10个进程ps auxw|head -1;ps auxw|sort -rn -k3|head -10 7、查看内存消耗最多的前10个进程ps auxw|head -1;ps auxw|sort -rn -k4|head -10 8、查看虚拟内存使用最多的前10个进程ps auxw|head -1;ps auxw|sort -rn -k5|head -10 9、获取出口IP地址curl http://members.3322.org/dyndns/getip curl https://ip.cn curl cip.cc curl myip.ipip.net curl ifconfig.me 10、ISO自动挂载echo \"/mnt/iso/CentOS-7-x86_64-Minimal-1804.iso /mnt/cdrom iso9660 defaults,loop 0 0\" >> /etc/fstab && \\ mount -a && \\ df -mh 11、查看系统版本号和内核信息cat /proc/version uname -a lsb_release -a cat /etc/redhat-release cat /etc/issue rpm -q redhat-release 12、查看物理CPU个数、核数、逻辑CPU个数CPU总核数 = 物理CPU个数 每颗物理CPU的核数 总逻辑CPU数 = 物理CPU个数 每颗物理CPU的核数 * 超线程数# 查看CPU信息（型号） cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c # 查看物理CPU个数 cat /proc/cpuinfo| grep \"physical id\"| sort| uniq| wc -l # 查看每个物理CPU中core的个数(即核数) cat /proc/cpuinfo| grep \"cpu cores\"| uniq # 查看逻辑CPU的个数 cat /proc/cpuinfo| grep \"processor\"| wc -l 13、Linux缓存cached是cpu与内存间的，buffer是内存与磁盘间的，都是为了解决速度不对等的问题。buffer是即将要被写入磁盘的，而cache是被从磁盘中读出来的buff：作为buffer cache的内存，是块设备的读写缓冲区 cache：作为page cache的内存，文件系统的cache。Buffer cache是针对磁盘块的缓存，也就是在没有文件系统的情况下，直接对磁盘进行操作的数据会缓存到buffer cache中。 pagecache：页面缓存（pagecache）可以包含磁盘块的任何内存映射。这可以是缓冲I/O，内存映射文件，可执行文件的分页区域——操作系统可以从文件保存在内存中的任何内容。Page cache实际上是针对文件系统的，是文件的缓存，在文件层面上的数据会缓存到page cache。 dentries：表示目录的数据结构 inodes：表示文件的数据结构 #内核配置接口 /proc/sys/vm/drop_caches 可以允许用户手动清理cache来达到释放内存的作用，这个文件有三个值：1、2、3（默认值为0） #释放pagecache $> echo 1 > /proc/sys/vm/drop_caches #释放dentries、inodes $> echo 2 > /proc/sys/vm/drop_caches #释放pagecache、dentries、inodes $> echo 3 > /proc/sys/vm/drop_caches 14、设置代理$> bash -c 'cat >> /etc/profile 15、查看网卡UUIDnmcli con | sed -n '1,2p' 16、时间戳与日期日期与时间戳的相互转换#将日期转换为Unix时间戳 date +%s #将Unix时间戳转换为指定格式化的日期时间 date -d @1361542596 +\"%Y-%m-%d %H:%M:%S\" date日期操作date +%Y%m%d #显示前天年月日 date -d \"+1 day\" +%Y%m%d #显示前一天的日期 date -d \"-1 day\" +%Y%m%d #显示后一天的日期 date -d \"-1 month\" +%Y%m%d #显示上一月的日期 date -d \"+1 month\" +%Y%m%d #显示下一月的日期 date -d \"-1 year\" +%Y%m%d #显示前一年的日期 date -d \"+1 year\" +%Y%m%d #显示下一年的日期 获得毫秒级的时间戳在linux Shell中并没有毫秒级的时间单位，只有秒和纳秒其实这样就足够了，因为纳秒的单位范围是（000000000..999999999），所以从纳秒也是可以的到毫秒的current=`date \"+%Y-%m-%d %H:%M:%S\"` #获取当前时间，例：2015-03-11 12:33:41 timeStamp=`date -d \"$current\" +%s` #将current转换为时间戳，精确到秒 currentTimeStamp=$((timeStamp*1000+`date \"+%N\"`/1000000)) #将current转换为时间戳，精确到毫秒 echo $currentTimeStamp 17、设置时区cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 18、生成文件的MD值在网络传输、设备之间转存、复制大文件等时，可能会出现传输前后数据不一致的情况。这种情况在网络这种相对更不稳定的环境中，容易出现。那么校验文件的完整性，也是势在必行的。在网络传输时，我们校验源文件获得其md5sum，传输完毕后，校验其目标文件，并对比如果源文件和目标文件md5 一致的话，则表示文件传输无异常。否则说明文件在传输过程中未正确传输。md5值是一个128位的二进制数据，转换成16进制则是32（128/4）位的进制值。 md5校验，有很小的概率不同的文件生成的md5可能相同。比md5更安全的校验算法还有SHA*系列的。Linux的md5sum命令md5sum命令用于生成和校验文件的md5值。它会逐位对文件的内容进行校验。是文件的内容，与文件名无关，也就是文件内容相同，其md5值相同。#md5sum命令的详解 $> md5sum --h Usage: md5sum [OPTION]... [FILE] With no FILE, or when FILE is -, read standard input. -b, --binary 二进制模式读取文件 -c, --check 从文件中读取、校验MD5值 --tag 创建一个BSD-style风格的校验值 -t, --text 文本模式读取文件（默认） #校验文件MD5值使用的参数 The following four options are useful only when verifying checksums: --quiet don't print OK for each successfully verified file --status don't output anything, status code shows success --strict exit non-zero for improperly formatted checksum lines -w, --warn warn about improperly formatted checksum lines --help display this help and exit --version output version information and exit #生成的MD5值重定向到文件中 $>md5sum filename > filename.md5 #生成的MD5值重定向追加到文件中 $> md5sum filename >>filename.md5 #多个文件输出到一个md5文件中，这要使用通配符* $> md5sum *.iso > iso.md5 #同时计算多个文件的MD5值 $> md5sum filetohashA.txt filetohashB.txt filetohashC.txt > hash.md5 #校验MD5:把下载的文件file和该文件的file.md5报文摘要文件放在同一个目录下 $> md5sum -c file.md5 #创建一个BSD风格的校验值 $> md5sum --tag file.md5 MD5 (file.md5) = 9192e127b087ed0ae24bb12070f3051a Python生成MD5值# 方式一：使用md5包 import md5 src = 'this is a md5 test.' m1 = md5.new() m1.update(src) print m1.hexdigest() # 方式二：使用hashlib（推荐） import hashlib m2 = hashlib.md5() m2.update(src) print m2.hexdigest() # 加密常见的问题： 1：Unicode-objects must be encoded before hashing 　　解决方案：import hashlib 　　　　　　　m2 = hashlib.md5() 　　　　　　　m2.update(src．encode('utf-8')) 　　　　　　　print m2.hexdigest() Java生成MD5值import java.security.MessageDigest; public static void main(String[] args) { String password = \"123456\"; try { MessageDigest instance = MessageDigest.getInstance(\"MD5\");// 获取MD5算法对象 byte[] digest = instance.digest(password.getBytes());// 对字符串加密,返回字节数组 StringBuffer sb = new StringBuffer(); for (byte b : digest) { int i = b & 0xff;// 获取字节的低八位有效值 String hexString = Integer.toHexString(i);// 将整数转为16进制 // System.out.println(hexString); if (hexString.length() 19、添加用户useradd (选项) （参数） #选项 －c：加上备注文字，备注文字保存在passwd的备注栏中 －d：指定用户登入时的启始目录 －D：变更预设值 －e：指定账号的有效期限，缺省表示永久有效 －f：指定在密码过期后多少天即关闭该账号 －g：指定用户所属的起始群组 －G：指定用户所属的附加群组 －m：自动建立用户的登入目录 －M：不要自动建立用户的登入目录 －n：取消建立以用户名称为名的群组 －r：建立系统账号 －s：指定用户登入后所使用的shell －u：指定用户ID号 20、su 与 sudosu : switch to another user 切换用户sudo : superuser do 允许用户使用superuser的身份执行命令su username ：切换为username，需要输入username密码 su : 切换为root用户，需要输入root密码 su - : 切换为root用户，需要输入root密码，且环境变量也改变 su - -c \"command\" ：使用root身份执行命令，完成后即退出root身份 sudo command : 与su -c相似，需要输入当前用户（superuser，/etc/sudoers中指定）密码 sudo su -：使用当前用户密码实现root身份的切换 su - hdfs -c command 切换用户并以某用户的身份去执行一条命令 su - hdfs test.sh 切换用户并以某用户的身份去执行一个shell文件 21、重新开启SELinux如果在使用setenforce命令设置selinux状态的时候出现这个提示：setenforce: SELinux is disabled。那么说明selinux已经被彻底的关闭了,如果需要重新开启selinuxvi /etc/selinux/config 更改为：SELINUX=1 必须重启linux，不重启是没办法立刻开启selinux的 重启完以后，使用getenforce,setenforce等命令就不会报“setenforce: SELinux is disabled”了。这时，我们就可以用setenforce命令来动态的调整当前是否开启selinux。22、检查软件是否已安装，没有就自动安装rpm -qa |grep \"jq\" if [ $? -eq 0 ] ;then echo \"jq hava been installed \" else yum -y install epel-release && yum -y install jq fi 23、使用privoxy代理http，https流量使用socket连接ShadowSocks服务器echo \"安装ShadowSocks\" && \\ yum -y install epel-release && yum -y install python-pip && pip install shadowsocks && \\ bash -c 'cat > /etc/shadowsocks.json /etc/systemd/system/shadowsocks.service > /etc/profile && \\ echo \"export https_proxy=http://127.0.0.1:8118\" >> /etc/profile && \\ source /etc/profile && \\ curl www.google.com 24、批量打通指定主机SSH免密钥登录脚本CentOS$> bash -c 'cat > ./HitthroughSSH.sh ./hosts.txt 25、硬盘自动分区，格式化，开机自动挂载到/data$> disk=/dev/sdc;\\ bash -c \"fdisk ${disk}>/etc/fstab ;\\ sed -i '$ s/$/ \\/data ext4 defaults 0 0/' /etc/fstab ;\\ mkdir /data ;\\ mount -a ;\\ df -h 26、在hosts文件中添加IP地址与主机名的域名映射ipaddr=$(ip addr | awk '/^[0-9]+: / {}; /inet.*global/ {print gensub(/(.*)\\/(.*)/, \"\\\\1\", \"g\", $2)}'| sed -n '1p') && \\ echo $ipaddr $HOSTNAME >> /etc/hosts 27、禁用透明大页Redhatsed -i '$a echo nerver > /sys/kernel/mm/redhat_transparent_hugepage/defrag\\necho nerver > /sys/kernel/mm/redhat_transparent_hugepage/enabled' CentOSecho never > /sys/kernel/mm/transparent_hugepage/defrag ;\\ echo never > /sys/kernel/mm/transparent_hugepage/enabled ;\\ sed -i '/GRUB_CMDLINE_LINUX/ s/\"$/ transparent_hugepage=never\"/' /etc/default/grub ;\\ grub2-mkconfig -o /boot/grub2/grub.cfg 28、安装JDK环境Prerequisite： 1. JDK安装包已下载在内网HTTP服务器中wget http://192.168.1.2/jdk/jdk-8u111-linux-x64.tar.gz;\\ tar -zxvf jdk-8u111-linux-x64.tar.gz -C /opt;\\ rm -rf jdk-8u111-linux-x64.tar.gz;\\ ln -s /opt/jdk1.8.0_111 /opt/jdk;\\ sed -i '$a export JAVA_HOME=/opt/jdk\\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\\nexport PATH=$PATH:$JAVA_HOME/bin' /etc/profile;\\ source /etc/profile;\\ ln -s /opt/jdk/bin/java /usr/bin/java;\\ java -version;\\ javac -version 29、安装Tomcat，并由systemctl托管Prerequisite： 1. 已安装JDK 2. Tomcat安装包已下载在内网HTTP服务器中wget http://192.168.1.2/tomcat/apache-tomcat-8.5.20.tar.gz;\\ tar -zxvf apache-tomcat-8.5.20.tar.gz -C /opt;\\ rm -rf apache-tomcat-8.5.20.tar.gz;\\ ln -s /opt/apache-tomcat-8.5.20 /opt/tomcat;\\ bash -c 'cat > /lib/systemd/system/tomcat.service 30、安装Nginxbash -c 'cat > /etc/yum.repos.d/nginx.repo 31、安装单机版的ZookeeperPrerequisite： 1. 已安装JDKversion=3.4.14 curl https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/zookeeper-$version.tar.gz -o /opt/zookeeper-$version.tar.gz tar -zxvf /opt/zookeeper-*.tar.gz -C /opt/ ;\\ rm -rf /opt/zookeeper-*.tar.gz ;\\ ln -s /opt/zookeeper-$version/ /opt/zookeeper ;\\ sed -i '$a export ZOOKEEPER_HOME=/opt/zookeeper\\nexport PATH=$PATH:$ZOOKEEPER_HOME/bin' /etc/profile ;\\ source /etc/profile ;\\ mv /opt/zookeeper/conf/zoo_sample.cfg /opt/zookeeper/conf/zoo.cfg ;\\ sed -i -e '/dataDir/d' -e '/dataLogDir/d' /opt/zookeeper/conf/zoo.cfg ;\\ sed -i -e '$a dataDir=/data/zookeeper/data\\ndataLogDir=/data/zookeeper/logs\\nserver.1=127.0.0.1:2888:3888\\nautopurge.purgeInterval=24\\nautopurge.purgeInterval=5' /opt/zookeeper/conf/zoo.cfg ;\\ mkdir -p /data/zookeeper/{data,logs} ;\\ echo \"1\" > /data/zookeeper/data/myid ;\\ zkServer.sh start ;\\ zkServer.sh status ;\\ jps -l 32、安装单机版的KafkaPrerequisite： 1. 已安装Zookeeperversion=2.12-2.2.0 curl https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.2.0/kafka_$version.tgz -o /opt/kafka_$version.tgz ;\\ tar -zxvf /opt/kafka_$version.tgz -C /opt;\\ rm -rf /opt/kafka_$version.tgz ;\\ ln -s /opt/kafka_$version /opt/kafka ;\\ sed -i '$a export KAFKA_HOME=/opt/kafka\\nexport PATH=$PATH:$KAFKA_HOME/bin' /etc/profile ;\\ source /etc/profile ;\\ sed -i -e 's/log.dirs=\\/tmp\\/kafka\\/logs/log.dirs=\\/data\\/kafka\\/logs/g' -e 's/log.retention.hours=168/log.retention.hours=1/g' -e '$a auto.create.topics.enable=true\\ndelete.topic.enable=true' /opt/kafka/config/server.properties ;\\ mkdir -p /data/kafka/{logs,data} ;\\ kafka-server-start.sh -daemon /opt/kafka/config/server.properties ;\\ jps -l 33、安装Hadoop客户端以hadoop 2.8.3版本为例wget https://archive.apache.org/dist/hadoop/common/hadoop-2.8.3/hadoop-2.8.3.tar.gz ;\\ tar -xvf hadoop-2.8.3.tar.gz -C /opt ;\\ rm -rf hadoop-2.8.3.tar.gz ;\\ ln -s /opt/hadoop-2.8.3 /opt/hadoop ;\\ sed -i '$a export HADOOP_HOME=/opt/hadoop\\nexport PATH=$PATH:$HADOOP_HOME/bin' /etc/profile ;\\ source /etc/profile #然后在/opt/hadoop-2.8.3/etc/hadoop/core-site.xml配置文件标签中填写HDFS NameNode节点的IP地址及端口号 fs.default.name hdfs://172.16.3.10:9000 hdfs dfs -ls / 34、安装Maven环境curl https://mirrors.tuna.tsinghua.edu.cn/apache/maven/binaries/apache-maven-3.2.2-bin.tar.gz -o /opt/apache-maven-3.2.2-bin.tar.gz && \\ tar -zxvf /opt/apache-maven-*.tar.gz -C /opt/ && \\ rm -rf /opt/apache-maven-*.tar.gz && \\ ln -s /opt/apache-maven-3.2.2 /opt/maven && \\ sed -i '$a export M2_HOME=/opt/maven\\nexport PATH=$PATH:$M2_HOME/bin' /etc/profile && \\ source /etc/profile && \\ mvn version 35、安装NodeJS环境wget https://nodejs.org/dist/v8.9.4/node-v8.9.4-linux-x64.tar.xz ;\\ tar -xvf node-v8.9.4-linux-x64.tar.xz -C /opt/ ;\\ rm -rf node-v8.9.4-linux-x64.tar.xz ;\\ ln -s /opt/node-v8.9.4-linux-x64 /opt/nodejs ;\\ sed -i '$a export NODEJS_HOME=/opt/nodejs\\nexport PATH=$PATH:$NODEJS_HOME/bin' /etc/profile;\\ source /etc/profile;\\ yum install gcc-c++ make -y;\\ npm config set registry https://registry.npm.taobao.org ;\\ npm config set sass_binary_site https://npm.taobao.org/mirrors/node-sass/ ;\\ npm version 36、安装Docker，并设置新硬盘LVM成docker的数据目录wget https://download.docker.com/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo ;\\ yum makecache ;\\ yum install docker-ce-17.12.1.ce -y ;\\ systemctl enable docker ;\\ mkdir /etc/docker ;\\ touch /etc/docker/daemon.json ;\\ bash -c ' tee /etc/docker/daemon.json > /etc/fstab ;\\ df -mh ;\\ systemctl start docker ;\\ ls /var/lib/docker/ ;\\ docker info |grep \"Insecure Registries:\" -A 4 37、字符转换命令expand/unexpand用于将文件的制表符（Tab）转换为空格符（Space），默认一个Tab对应8个空格符，并将结果输出到标准输出。若不指定任何文件名或所给文件名为”-“，则expand会从标准输入读取数据。功能与之相反的命令是unexpand，是将空格符转成Tab符。vi/vim在命令模式下通过设置\":set list\"可显示文件中的制表符“^I”expand命令参数-i, --initial do not convert tabs after non blanks -t, --tabs=NUMBER have tabs NUMBER characters apart, not 8 -t, --tabs=LIST use comma separated list of explicit tab positions --help display this help and exit --version output version information and exit unexpand命令参数-a, --all convert all blanks, instead of just initial blanks --first-only convert only leading sequences of blanks (overrides -a) -t, --tabs=N have tabs N characters apart instead of 8 (enables -a) -t, --tabs=LIST use comma separated LIST of tab positions (enables -a) --help display this help and exit --version output version information and exit 实例将文件中每行第一个Tab符替换为4个空格符，非空白符后的制表符不作转换#使用\"----\"或\"--\"代表一个制表符，使用\":\"代表一个空格 ----abcd--e $ expand -i -t 4 old-file > new-file ::::abcd--e 注意不是所有的Tab都会转换为默认或指定数量的空格符，expand会以对齐为原则将Tab符替换为适当数量的空格符，替换的原则是使后面非Tab符处在一个物理Tab边界（即Tab size的整数倍。例如：#使用\"----\"或\"--\"代表一个制表符，使用\":\"代表一个空格 abcd----efg--hi $ expand -t 4 file abcd::::efg::hi Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-26 22:06:44 "},"origin/linux-文本处理.html":{"url":"origin/linux-文本处理.html","title":"文本处理","keywords":"","body":"一、awk1、获取匹配关键字后的内容awk '{ if(match($0,\"关键字\")) {print substr($0,RSTART+RLENGTH) }}'文件 #示例 # 原始文本 2018-07-31T09:33:08.160102Z 1 [Note] A temporary password isgenerated for root@localhost: oco4Pr&a!o;v # 命令 awk '{ if(match($0,\"root@localhost: \")) {print substr($0,RSTAR+RLENGTH) }}' test.log # 结果 oco4Pr&a!o;v 2、去除文本中的空行awk NF test.txt # NF代表当前行的字段数，空行的话字段数为0,被awk解释为假，因此不进行输出。 3、获取匹配关键字后多少的位字符串# 样本 a=\"Location: https://allinone.okd311.curiouser.com:8443/oauth/token/implicit#access_token=FBHwgR1jj2coLoYYfG9SdGUke9L9HmAU2IOI9GaMKrQ&expires_in=86400&scope=user%3Afull&token_type=Bearer\" # 获取\"access_token=\"的值 # 方式一 echo $a | grep \"access_token=\" |awk -F\"access_token=\" '/access_token=/{printf substr($2,0,43)}' # 结果： FBHwgR1jj2coLoYYfG9SdGUke9L9HmAU2IOI9GaMKrQ # 方式二 echo $a | grep \"access_token=\" |awk '{ if(match($0,\"access_token=\")) {print substr($0,RSTART+RLENGTH) }}'| awk -F '&' '{print $1}' # 结果： FBHwgR1jj2coLoYYfG9SdGUke9L9HmAU2IOI9GaMKrQ 二、sedsed [-hnV][-e][-f][文本文件] 参数说明： -e 或 --expression= 以选项中指定的script来处理输入的文本文件。 -f 或 --file= 以选项中指定的script文件来处理输入的文本文件。 -h 或 --help 显示帮助。 -n 或 --quiet或--silent 仅显示script处理后的结果。 -V 或 --version 显示版本信息。 动作说明a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～ c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！ d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚； i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)； p ：打印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～ s ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！ 1、新增内容到末尾行的末尾sed '$ s/$/新增内容/' file_path 2、去除文本中空行和开头\"##\"的行sed '/^$/d;/^##/d' file_path 3、去除文本中的空行sed '/^\\s*$/d' test.txt 4、多个匹配规则sed -i -e '/hah/a lala\\nhehe' -e '/lala/d' test 5、在查找到匹配行后的操作sed -i '/hah/a lallalla' test #在查找到匹配行后添加一行 sed -i '/hah/a lala\\nhehe' test #在查找到匹配行后添加多行 sed -i '/hah/d' test #删除查找到匹配行 6、在查找匹配行的末首或末尾添加内容sed -i '/ha/ s/^/la' test #在查找包含\"ha\"的行首追加\"la\",\"laha\" sed -i '/ha/ s/$/la' test #在查找包含\"ha\"的行末追加\"la\"，\"hala\" 7、去掉文本中开头带#号注释的行sed -i -c -e '/^$/d;/^#/d' file 8、去除文本中的换行符^MWindows下保存的文本文件，上传到Linux/Unix下后总会在末尾多了一个换行符^M，导致一些xml、ini、sh等文件读取错误sed 's/^M//' 原文件>新文件 # 注意，^M = Ctrl v + Ctrl m，而不是手动输入^M 三、grepgrep [OPTION]... PATTERN [FILE]... -r 是递归查找 -n 是显示行号 -R 查找所有文件包含子目录 -i 忽略大小写 -l 只列出匹配的文件名 -L 列出不匹配的文件名 -w 只匹配整个单词，而不是字符串的一部分 -C 匹配的上下文分别显示[number]行 1、统计某文件夹下文件的个数ls -l /data|grep \"^-\"|wc -l #不包含子目录 ls -lR /data|grep \"^-\"|wc -l #不含子目录 2、统计某文件夹下目录的个数ls -l /data |grep \"^ｄ\"|wc -l #不包含子目录 ls -lR /data|grep \"^ｄ\"|wc -l #包含子目录 3、统计某目录(包含子目录)下的所有某种类型的文件ls -lR /data|grep txt|wc -l 4、去除文本中的空行grep -v '^\\s*$' test.txt 5、多个匹配规则grep pattern1 | pattern2 files ：显示匹配 pattern1 或 pattern2 的 grep pattern1 files | grep pattern2 ：显示既匹配 pattern1 又匹配 pattern2 的行。 6、xargs配合grep查找find -type f -name '*.php'|xargs grep 'GroupRecord' 7、查找路径下含有某字符串的所有文件grep -rn \"hello,world!\" * 四、egrep1、只显示文本中的非空行和非注释行egrep -v '^$|#' file_path 五、cut1、获取硬盘某个分区的UUID号追加到fstabblkid | grep /dev/sdb5 | cut -d ' ' -f 2 >>/etc/fstab;sed -i '$ s/$/ data ext4 defaults 0 0/' /etc/fstab 六、wc1、统计某个目录下某种文件内总共多少行find mapred/ -name \"*.java\" -print | xargs cat | wc -l 七、dos2unixdos2unix是将Windows格式文件转换为Unix、Linux格式的实用命令。Windows格式文件的换行符为\\r\\n ,而Unix&Linux文件的换行符为\\n. dos2unix命令其实就是将文件中的\\r\\n 转换为\\n。而unix2dos则是和dos2unix互为孪生的一个命令，它是将Linux&Unix格式文件转换为Windows格式文件的命令。安装yum install dos2unix -y #会安装dos2Unix、unix2dos、unix2mac这三条命令 用法dos2unix [options] [-c convmode] [-o file ...] [-n infile outfile ...] -h 显示命令dos2unix联机帮助信息。 -k 保持文件时间戳不变 -q 静默模式，不输出转换结果信息等 -V 显示命令版本信息 -c 转换模式 -o 在源文件转换，默认参数 -n 保留原本的旧档，将转换后的内容输出到新档案.默认都会直接在原来的文件上修改， 1、一次转换多个文件$> dos2unix filename1 filename2 filename3 2、默认情况下会在源文件上进行转换，如果需要保留源文件，那么可以使用参数-ndos2unix -n oldfilename newfilename 3、保持文件时间戳不变$ ls -lrt dosfile -rw-r--r-- 1 root root 67 Dec 26 11:46 dosfile $ dos2unix dosfile dos2unix: converting file dosfile to UNIX format ... $ ls -lrt dosfile -rw-r--r-- 1 root root 65 Dec 26 11:58 dosfile $ dos2unix -k dosfile dos2unix: converting file dosfile to UNIX format ... $ ls -lrt dosfile -rw-r--r-- 1 root root 65 Dec 26 11:58 dosfile 4、静默模式格式化文件unix2dos -q dosfile Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-26 22:06:44 "},"origin/linux-htpasswd.html":{"url":"origin/linux-htpasswd.html","title":"htpasswd","keywords":"","body":"一、Overviewshtpasswd命令是Apache的Web服务器内置工具，用于创建和更新储存用户名、域和用户基本认证的密码文件,主要用于对基于http用户的认证。二、安装yum install -y httpd-tools 三、语法htpasswd(选项)(参数) 选项 -c：创建一个加密文件 -n：不更新加密文件，只将加密后的用户名密码显示在屏幕上 -m：默认采用MD5算法对密码进行加密 -d：采用CRYPT算法对密码进行加密 -p：不对密码进行进行加密，即明文密码 -s：采用SHA算法对密码进行加密 -b：在命令行中一并输入用户名和密码而不是根据提示输入密码 -D：删除指定的用户 参数 用户：要创建或者更新密码的用户名 密码：用户的新密码 四、常见操作1、利用htpasswd命令添加用户htpasswd .passwd -bc www.linuxde.net php # 在bin目录下生成一个.passwd文件，用户名www.linuxde.net，密码：php，默认采用MD5加密方式 2、在原有密码文件中增加下一个用户htpasswd .passwd -b Jack 123456 #去掉-c选项，即可在第一个用户之后添加第二个用户，依此类推。 3、不更新密码文件，只显示加密后的用户名和密码htpasswd -nb Jack 123456 # 不更新.passwd文件，只在屏幕上输出用户名和经过加密后的密码 4、利用htpasswd命令删除用户名和密码htpasswd .passwd -D Jack 5、利用htpasswd命令修改密码htpasswd .passwd -D Jack htpasswd .passwd -b Jack 123456 # 即先使用htpasswd删除命令删除指定用户，再利用htpasswd添加用户命令创建用户即可实现修改密码的功能。 参考链接http://man.linuxde.net/htpasswd Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:03 "},"origin/linux-shyaml.html":{"url":"origin/linux-shyaml.html","title":"YAML文本处理工具shyaml","keywords":"","body":"Linux下YAML文本处理工具shyaml一、Overviews通过 shyaml，可以直接获取键、值、键值对或对应的类型二、安装pip install shyaml 三、语法cat | shyaml ACTION KEY [DEFAULT] ACTIONget-type：获取相应的类型 get-value：获取值 get-values{,-0}：对序列类型来说，获取值列表 keys{,-0}：返回键列表 values{,-0}：返回值列表 key-values,{,-0}：返回键值对 Note：结果默认是加\\n换行符，若用-0形式则以NUL字符填充 KEY为要查询的键，如不提供，则使用DEFAULT 四、示例--- idc_group: name: bx bx: news_bx: news_bx web3_bx: web3_php-fpm_bx 如果要获取idc_group.name的值则可以执行cat file.yaml | shyaml get-value idc_group.name 想获取idc_group.bx的键值对可执行cat file.yaml | shyaml key-values idc_group.bx 参考链接https://www.linuxidc.com/Linux/2016-04/130403.htm Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:04 "},"origin/linux-jq.html":{"url":"origin/linux-jq.html","title":"JSON文本处理工具jq","keywords":"","body":"Linux下JSON文本处理工具jq一、Overviewsjq 是一款命令行下处理 JSON 数据的工具。其可以接收标准输入，命令管道或者文件中的 JSON 数据，经过一系列的过滤器(filters)和表达式的转后形成我们需要的数据结构并将结果输出到标准输出中。jq 的这种特性使我们可以很容易地在 Shell 脚本中调用它。二、安装yum install -y epel-release ;\\ yum install -y jq 三、jq命令参数jq [options] [file...] options: -c 使输出紧凑，而不是把每一个JSON对象输出在一行。; -n 不读取任何输入，过滤器运行使用null作为输入。一般用作从头构建JSON数据。; -e set the exit status code based on the output; -s 读入整个输入流到一个数组(支持过滤); -r 如果过滤的结果是一个字符串，那么直接写到标准输出（去掉字符串的引号）; -R read raw strings, not JSON texts; -C 打开颜色显示; -M 关闭颜色显示; -S sort keys of objects on output; --tab use tabs for indentation; --arg a v jq 通过该选项提供了和宿主脚本语言交互的能力。该选项将值(v)绑定到一个变量(a)上。在后面的 filter 中可以直接通过变量引用这个值。例如，filter '.$a'表示查询属性名称等于变量 a 的值的属性。; --argjson a v set variable $a to JSON value ; --slurpfile a f set variable $a to an array of JSON texts read from ; 参考链接https://www.ibm.com/developerworks/cn/linux/1612_chengg_jq/index.html?ca=drs-&utm_source=tuicool&utm_medium=referral Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-12 22:47:23 "},"origin/linux-curl.html":{"url":"origin/linux-curl.html","title":"Curl命令详解","keywords":"","body":"一、Curl命令详解语法：curl [options] [URL...]参数：Options: (H) means HTTP/HTTPS only, (F) means FTP only --anyauth 可以使用“任何”身份验证方法 -a, --append FTP/SFTP上传文件时，curl将追加到目标文件，而非覆盖 --basic 使用HTTP基本验证 --cacert FILE 指定CA证书文件(SSL) --capath DIR 指定CA目录 (SSL) -E, --cert CERT[:PASSWD] Client certificate file and password (SSL) --cert-type 指定证书文件类型 (DER/PEM/ENG) (SSL) --ciphers LIST 指定SSL密码 --compressed 响应压缩格式 (deflate/gzip) -K, --config FILE 后接参数文件，参数文件中可以定义HTTP请求的相关的内容（URL、HEAD、DATA） --connect-timeout SECONDS 设置最大请求时间 -C, --continue-at OFFSET 断点续转 -b, --cookie STRING/FILE 设置cookies -c, --cookie-jar FILE 操作结束后把cookie写入到文件中 --create-dirs 建立本地目录层次结构 --crlf 上传时把LF转变成CRLF --crlfile FILE Get a CRL list in PEM format from the given file -d, --data DATA HTTP POST data (H) --data-ascii DATA 以ascii的方式post数据 --data-binary DATA 以二进制的方式post数据 --data-urlencode DATA HTTP POST data url encoded (H) --delegation STRING GSS-API delegation permission --digest 使用HTTP数字身份验证 --disable-eprt 禁止使用EPRT或LPRT --disable-epsv 禁止使用EPSV -D, --dump-header FILE 把header信息写入到文件中 --egd-file FILE 为随机数据(SSL)设置EGD socket路径 --engine ENGINGE 指定加密引擎(SSL). \"--engine list\" for list -f, --fail 连接失败时不显示http错误 -F, --form CONTENT form表单提交 --form-string STRING 模拟http表单提交数据 --ftp-account DATA Account data string (F) --ftp-alternative-to-user COMMAND String to replace \"USER [name]\" (F) --ftp-create-dirs 如果远程目录不存在，创建远程目录 --ftp-method [MULTICWD/NOCWD/SINGLECWD] 控制CWD的使用 --ftp-pasv 使用 PASV/EPSV 代替端口 -P, --ftp-port ADR Use PORT with given address instead of PASV (F) --ftp-skip-pasv-ip Skip the IP address for PASV (F) --ftp-pret Send PRET before PASV (for drftpd) (F) --ftp-ssl-ccc Send CCC after authenticating (F) --ftp-ssl-ccc-mode ACTIVE/PASSIVE Set CCC mode (F) --ftp-ssl-control Require SSL/TLS for ftp login, clear for transfer (F) -G, --get 使用get请求发送 -d参数指定的数据 -g, --globoff 禁用网址序列和范围使用{}和[] -H, --header LINE 增加Head头 -I, --head 只显示文档信息 -h, --help 显示帮助信息 --hostpubmd5 MD5 Hex encoded MD5 string of the host public key. (SSH) -0, --http1.0 强制使用HTTP 1.0协议 --ignore-content-length 忽略的HTTP头信息的长度 -i, --include 输出响应Head头 -k, --insecure 允许curl使用非安全的ssl连接并且传输数据（证书不受信） --interface INTERFACE 使用指定网络接口/地址 -4, --ipv4 解析域名为ipv4地址(域名有多个ip时) -6, --ipv6 解析域名为ipv6地址(域名有多个ip时) -j, --junk-session-cookies 读取文件时忽略session cookie (H) --keepalive-time SECONDS 设置连接的保活时间 --key KEY 私钥文件名(SSL/SSH) --key-type TYPE 私钥文件类型 (DER/PEM/ENG) (SSL) --krb LEVEL 使用指定安全级别的krb (F) --libcurl FILE Dump libcurl equivalent code of this command line --limit-rate RATE 指定最大的传输速率 -l, --list-only 列出ftp目录下的文件名称(F) --local-port RANGE 强制使用本地端口号 -L, --location curl自动重定向（3xx） --location-trusted like --location and send auth to other hosts (H) -M, --manual 显示全手动 --mail-from FROM 指定发信人邮箱(SMTP) --mail-rcpt TO 指定收信人邮箱(SMTP) --mail-auth AUTH Originator address of the original email --max-filesize BYTES 允许下载文件的最大大小 --max-redirs NUM Maximum number of redirects allowed (H) -m, --max-time SECONDS 设置整个操作的允许消耗的最大时间，对于在延时网络下的批量操作有利 --metalink Process given URLs as metalink XML file --negotiate 使用HTTP Negotiate身份验证(H) -n, --netrc 从netrc文件中读取用户名和密码 --netrc-optional 使用 .netrc 或者 URL来覆盖-n --netrc-file FILE 指定.netrc文件 -N, --no-buffer 禁用输出流缓冲区 --no-keepalive 连接不保活 --no-sessionid Disable SSL session-ID reusing (SSL) --noproxy List of hosts which do not use proxy --ntlm 使用 HTTP NTLM 身份验证 -o, --output FILE 将响应数据输出到指定文件，后接文件参数 --pass PASS 私钥密码 (SSL/SSH) --post301 301重定向后不切换至GET请求 (H) --post302 302重定向后不切换至GET请求 (H) --post303 303重定向后不切换至GET请求 (H) -#, --progress-bar 对发送和接收进行简单的进度条展示 --proto PROTOCOLS Enable/disable specified protocols --proto-redir PROTOCOLS Enable/disable specified protocols on redirect -x, --proxy [PROTOCOL://]HOST[:PORT] 设置代理 --proxy-anyauth 选择任一代理身份验证方法 (H) --proxy-basic 在代理上使用基本身份验证 (H) --proxy-digest 在代理上使用数字身份验证 (H) --proxy-negotiate 在代理上使用Negotiate身份验证 (H) --proxy-ntlm 在代理上使用ntlm身份验证 (H) -U, --proxy-user USER[:PASSWORD] 设置代理用户名和密码 --proxy1.0 HOST[:PORT] 使用HTTP/1.0的代理 -p, --proxytunnel Operate through a HTTP proxy tunnel (using CONNECT) --pubkey KEY 公钥文件 (SSH) -Q, --quote CMD 文件传输前，发送命令到服务器 (F/SFTP) --random-file FILE File for reading random data from (SSL) -r, --range RANGE 检索来自HTTP/1.1或FTP服务器字节范围 --raw Do HTTP \"raw\", without any transfer decoding (H) -e, --referer 发送\"Referer Page\"到服务器 -J, --remote-header-name Use the header-provided filename (H) -O, --remote-name 把输出写到文件中，保留远程文件的文件名 --remote-name-all Use the remote file name for all URLs -R, --remote-time 在本地生成文件时，保留远程文件时间 -X, --request COMMAND 指定HTTP请求方法 --resolve HOST:PORT:ADDRESS 强制解析HOST:PORT到某个ADDRESS --retry NUM 传输出现问题时，重试的次数 --retry-delay SECONDS 传输出现问题时，设置重试间隔时间 --retry-max-time SECONDS 传输出现问题时，设置最大重试时间 -S, --show-error 显示错误信息 -s, --silent 静默模式。不输出任何东西 --socks4 HOST[:PORT] 用socks4代理给定主机和端口 --socks4a HOST[:PORT] 用socks4a代理给定主机和端口 --socks5 HOST[:PORT] 用socks5代理给定主机和端口 --socks5-basic socks5代理开启username/password认证 --socks5-gssapi socks5代理开启GSS-API认证 --socks5-hostname HOST[:PORT] SOCKS5 proxy, pass host name to proxy --socks5-gssapi-service NAME SOCKS5 proxy service name for gssapi --socks5-gssapi-nec Compatibility with NEC SOCKS5 server -Y, --speed-limit RATE 如果在speed-time期间，下载比speed-limit这个更慢，则下载废止 -y, --speed-time SECONDS 如果在speed-time期间，下载比speed-limit这个更慢，则下载废止。默认30s --ssl Try SSL/TLS (FTP, IMAP, POP3, SMTP) --ssl-reqd Require SSL/TLS (FTP, IMAP, POP3, SMTP) -2, --sslv2 使用SSLv2的（SSL） -3, --sslv3 使用SSLv3的（SSL） --ssl-allow-beast Allow security flaw to improve interop (SSL) --stderr FILE 指定错误信息输出文件 --tcp-nodelay 使用TCP_NODELAY选项 -t, --telnet-option OPT=VAL Telnet选项设置 --tftp-blksize VALUE 设置TFTP BLKSIZE(必须大于512) -z, --time-cond TIME 传送时间设置 -1, --tlsv1 强制使用TLS version 1.x --tlsv1.0 使用TLSv1.0 (SSL) --tlsv1.1 使用TLSv1.1 (SSL) --tlsv1.2 使用TLSv1.2 (SSL) --trace FILE dump出输入输出数据至文件 --trace-ascii FILE 跟'--trace'一样，但是没有hex输出 --trace-time 跟踪/详细输出时，添加时间戳 --tr-encoding Request compressed transfer encoding (H) -T, --upload-file FILE 上传文件 --url URL URL to work with -B, --use-ascii 使用ASCII文本传输 -u, --user USER[:PASSWORD] 设置服务端用户和密码 --tlsuser USER TLS用户名 --tlspassword STRING TLS密码 --tlsauthtype STRING TLS认证类型(default SRP) --unix-socket FILE Connect through this UNIX domain socket -A, --user-agent STRING 发送用户代理给服务器 (H) -v, --verbose 获取更多输入输出相关的内容，对于debug非常有用 -V, --version 显示当前的curl版本 -w, --write-out FORMAT 指定完成请求以后输出什么信息 --xattr Store metadata in extended file attributes -q If used as the first parameter disables .curlrc 二、实例详解1、通过-o/-O选项保存下载的文件到指定的文件中-o：将文件保存为命令行中指定的文件名的文件中-O：使用URL中默认的文件名保存文件到本地# 将文件下载到本地并命名为mygettext.html curl -o mygettext.html http://www.gnu.org/software/gettext/manual/gettext.html # 将文件保存到本地并命名为gettext.html curl -O http://www.gnu.org/software/gettext/manual/gettext.html 2、显示response中的Headers或Body-i：显示response header 和 body-I：只显示response headercurl -i https://www.baidu.com curl -I https://www.baidu.com 3、同时获取多个文件curl -O URL1 -O URL2 4、设置代理-x：为CURL设置代理curl -x 192.168.1.2:3128 http://google.com/ 5、允许重定向-L：允许重定向curl -L -x 192.168.1.2:3128 http://google.com/ 6、限速--limit-rate： 对CURL的最大网络使用进行限制curl --limit-rate 1000B -O http://www.gnu.org/software/gettext/manual/gettext.html 7、添加认证信息-u: 在访问需要认证的页面时，可通过-u选项提供用户名和密码进行授权curl -u username:password URL # 通常的做法是在命令行只输入用户名，之后会提示输入密码，这样可以保证在查看历史记录时不会将密码泄露 curl -u username URL 8、获取更多信息-v 和 -trace：获取更多信息curl -v -L -x 192.168.1.2:3128 http://google.com/ 9、自定义HTTP请求-X: 可以指定curl发送HTTP请求的方法，例如GET(默认),PUT,POST,DELETE等-H：添加请求的Header信息-d/--data: 添加请求的Bodycurl -XPUT \"http://127.0.0.1:9200/test/test/1\" \\ -H 'Content-Type: application/json' \\ -d ' { \"id\": \"191\", \"prd_id\": \"4\", \"mer_id\": \"1000005\", \"data_status\": \"0\", \"datachange_createtime\": \"1543915326\", \"datachange_lasttime\": \"1543915368\" }' 10、断点续传-C: 可对大文件使用断点续传功能 curl -C -O http://www.gnu.org/software/gettext/manual/gettext.html 11、模仿浏览器-A：指定浏览器去访问网站(有些网站需要使用特定的浏览器去访问他们，有些还需要使用某些特定的版本)curl -A \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.0)\" http://google.com/ 12、显示下载进度条-# ：显示下载进度条curl -# -O http://www.linux.com/dodo1.JPG 13、伪造referer（盗链）很多服务器会检查http访问的referer从而来控制访问。比如：你是先访问首页，然后再访问首页中的邮箱页面，这里访问邮箱的referer地址就是访问首页成功后的页面地址，如果服务器发现对邮箱页面访问的referer地址不是首页的地址，就断定那是个盗连了-e: 设定referercurl -e \"www.linux.com\" http://mail.linux.com # 这样就会让服务器其以为你是从www.linux.com点击某个链接过来的 14、保存与使用Cookie-D: 保存Cookie-b: 使用Cookie# 将网站的cookies信息保存到sugarcookies文件中 curl -D sugarcookies http://localhost/sugarcrm/index.php # 使用上次保存的cookie信息 curl -b sugarcookies http://localhost/sugarcrm/index.php 15、忽略证书不受信问题-k: 忽略HTTPS证书不受信问题curl -k https://allinone.okd311.curiouser.com:8443 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:02 "},"origin/linux-lvm.html":{"url":"origin/linux-lvm.html","title":"LVM原理及使用","keywords":"","body":"LVM原理及使用一、原理简介LVM是 Logical Volume Manager(逻辑卷管理)的简写，它由Heinz Mauelshagen在Linux 2.4内核上实现。 LVM将一个或多个硬盘的分区在逻辑上集合，相当于一个大硬盘来使用，当硬盘的空间不够使用的时候，可以继续将其它的硬盘的分区加入其中，这样可以实现磁盘空间的动态管理，相对于普通的磁盘分区有很大的灵活性。 与传统的磁盘与分区相比，LVM为计算机提供了更高层次的磁盘存储。它使系统管理员可以更方便的为应用与用户分配存储空间。在LVM管理下的存储卷可以按需要随时改变大小与移除(可能需对文件系统工具进行升级)。LVM也允许按用户组对存储卷进行管理，允许管理员用更直观的名称(如\"sales'、'development')代替物理磁盘名(如'sda'、'sdb')来标识存储卷 LVM功能实际是通过内核中的dm模块（device mapper）实现，它将一个或多个底层块设备组织成一个逻辑设备的模块，在/dev/目录下以dm-#形式展现 只要是块设备都可以用于创建LVM2。注意分区时ID号要是8e 物理存储介质（The physical media）：指系统的存储设备--硬盘，如：/dev/hda1、/dev/sda等等，是存储系统最低层的存储单元 物理卷PV（physical volume）：物理卷就是指硬盘分区或从逻辑上与磁盘分区具有同样功能的设备(如RAID)，是LVM的基本存储逻辑块，但和基本的物理存储介质（如分区、磁盘等）比较，却包含有与LVM相关的管理参数 卷组VG（volume group）：在较低的逻辑层从多个PV中抽象出来的卷组，由一个或多个物理卷组成 PE（physical extend）：每一个物理卷被划分为称为PE(Physical Extents)的基本单元，具有唯一编号的PE是可以被LVM寻址的最小单元。PE的大小是可配置的，默认为4MB 逻辑卷LV（logical volume）：由多个LV“块”组成可供挂载使用的设备文件 二、使用步骤1、安装相关软件包yum install -y lvm2 2、创建PVpvcreate /dev/sdc pvdisplay \"/dev/sdc\" is a new physical volume of \"100.00 GiB\" --- NEW Physical volume --- PV Name /dev/sdc VG Name PV Size 100.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID KiXHSv-PbKj-kOiM-yXhN-ntiw-ULpt-JhvgnB 3、创建VG# vgcreate命令用法 vgcreate -s [N[mgt]] VG名称 PV名称 # -s 指定VG中的PE大小，单位：MB,GB,TB vgcreate -s 16M docker /dev/sdc 4、查看VGvgdisplay --- Volume group --- VG Name docker System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 4 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 1 Max PV 0 Cur PV 2 Act PV 2 VG Size 199.99 GiB PE Size 4.00 MiB Total PE 51198 Alloc PE / Size 38399 / 5、创建LV#lvcreate命令参数 lvcreate -l PE个数 -n LV名称 VG名称 ​ lvcreate -l 6399 -n docker-lib docker 6、查看LV容量lvdisplay --- Logical volume --- LV Path /dev/docker/docker LV Name docker VG Name docker LV UUID hlbSQl-RfGK-PpUZ-u7Vx-5t3X-WOX7-dxLomX LV Write Access read/write LV Creation host, time node7.test.openshift.com, 2018-09-07 16:11:37 +0800 LV Status available # open 1 LV Size 7、格式化LVmkfs.ext3 LV_Name mkfs.ext4 LV_Name mkfs.xfs LV_Name 8、挂载LVecho \"LV_Name 挂载目录点 文件系统格式 defaults 0 0\" >> /etc/fstab mount -a 三、扩容VG和LVVG已无PE可用 新增硬盘 在线扩容（不卸载umount） 1、创建PVpvcreate /dev/sdd 2、将PV添加到VG中。之后可看PE数量增加vgextend VG_Name /dev/sdd 3、扩容LV(之后可看LV容量增加)lvresize -l +6399 LV_Name 或者 lvresize -L +50G LV_Name 4、检查并修复文件系统e2fsck -f LV_Name 5、将扩容后的LV完整地扩充到文件系统中LV文件系统是ext4时 resize2fs LV_Name LV文件系统是xfs时 xfs_growfs LV_Name Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:04 "},"origin/linux-交换分区.html":{"url":"origin/linux-交换分区.html","title":"Linux交换分区","keywords":"","body":"交换分区概念及管理一、什么是交换分区呢？Linux divides its physical RAM (random access memory) into chucks of memory called pages. Swapping is the process whereby a page of memory is copied to the preconfigured space on the hard disk, called swap space, to free up that page of memory. The combined sizes of the physical memory and the swap space is the amount of virtual memory available.Swap space in Linux is used when the amount of physical memory (RAM) is full. If the system needs more memory resources and the RAM is full, inactive pages in memory are moved to the swap space. While swap space can help machines with a small amount of RAM, it should not be considered a replacement for more RAM. Swap space is located on hard drives, which have a slower access time than physical memory.Swap space can be a dedicated swap partition (recommended), a swap file, or a combination of swap partitions and swap files.Linux内核为了提高读写效率与速度，会将文件在内存中进行缓存，这部分内存就是Cache Memory(缓存内存)。即使你的程序运行结束后，Cache Memory也不会自动释放。这就会导致你在Linux系统中程序频繁读写文件后，你会发现可用物理内存变少。当系统的物理内存不够用的时候，就需要将物理内存中的一部分空间释放出来，以供当前运行的程序使用。那些被释放的空间可能来自一些很长时间没有什么操作的程序，这些被释放的空间被临时保存到Swap空间中，等到那些程序要运行时，再从Swap分区中恢复保存的数据到内存中。这样，系统总是在物理内存不够时，才进行Swap交换。二、Swap交换分区对性能的影响我们知道Linux可以使用文件系统中的一个常规文件或独立分区作为Swap交换空间，相对而言，交换分区要快一些。但是和RAM比较而言，Swap交换分区的性能依然比不上物理内存，目前的服务器上RAM基本上都相当充足，那么是否可以考虑抛弃Swap交换分区，是否不需要保留Swap交换分区呢？这个其实是我的疑问之一。在这篇What Is a Linux SWAP Partition, And What Does It Do?博客中，作者给出了swap交换空间的优劣Advantages:Provides overflow space when your memory fills up completely Can move rarely-needed items away from your high-speed memory Allows you to hibernate Disadvantages:Takes up space on your hard drive as SWAP partitions do not resize dynamically Can increase wear and tear to your hard drive Does not necessarily improve performance (see below) 其实保留swap分区概括起来可以从下面来看：首先，当物理内存不足以支撑系统和应用程序（进程）的运作时，这个Swap交换分区可以用作临时存放使用率不高的内存分页，把腾出的内存交给急需的应用程序（进程）使用。有点类似机房的UPS系统，虽然正常情况下不需要使用，但是异常情况下， Swap交换分区还是会发挥其关键作用。其次，即使你的服务器拥有足够多的物理内存，也有一些程序会在它们初始化时残留的极少再用到的内存分页内容转移到 swap 空间，以此让出物理内存空间。对于有发生内存泄漏几率的应用程序（进程），Swap交换分区更是重要，因为谁也不想看到由于物理内存不足导致系统崩溃。最后，现在很多个人用户在使用Linux，有些甚至是PC的虚拟机上跑Linux系统，此时可能常用到休眠（Hibernate），这种情况下也是推荐划分Swap交换分区的。其实少量使用Swap交换空间是不会影响性能，只有当RAM资源出现瓶颈或者内存泄露，进程异常时导致频繁、大量使用交换分区才会导致严重性能问题。另外使用Swap交换分区频繁，还会引起kswapd0进程（虚拟内存管理中, 负责换页的）耗用大量CPU资源，导致CPU飙升。关于Swap分区的优劣以及是否应该舍弃，我有点恶趣味的想到了这个事情：人身上的两个器官，阑尾和扁桃体。切除阑尾或扁桃体是否也是争论不休。另外，其实不要Swap交换分区，Linux也是可以正常运行的（有人提及过这个问题）三、Swap分区大小设置建议系统的Swap分区大小设置多大才是最优呢？ 关于这个问题，应该说只能有一个统一的参考标准，具体还应该根据系统实际情况和内存的负荷综合考虑，像ORACLE的官方文档就推荐如下设置，这个是根据物理内存来做参考的。 **RAM** **Swap Space** **Up to 512 MB** 2 times the size of RAM **Between 1024 MB and 2048 MB** 1.5 times the size of RAM **Between 2049 MB and 8192 MB** Equal to the size of RAM **More than 8192 MB** 0.75 times the size of RAM 另外在其它博客中看到下面一个推荐设置，当然我不清楚其怎么得到这个标准的。是否合理也无从考证。可以作为一个参考。4G以内的物理内存，SWAP 设置为内存的2倍。 4-8G的物理内存，SWAP 等于内存大小。 8-64G 的物理内存，SWAP 设置为8G。 64-256G物理内存，SWAP 设置为16G。 四、什么时候使用Swap分区空间?系统在什么情况或条件下才会使用Swap分区的空间呢？ 其实是Linux通过一个参数swappiness来控制的。当然还涉及到复杂的算法。这个参数值可为 0-100，控制系统 swap 的使用程度。高数值可优先系统性能，在进程不活跃时主动将其转换出物理内存。低数值可优先互动性并尽量避免将进程转换处物理内存，并降低反应延迟。默认值为 60。注意：这个只是一个权值，不是一个百分比值，涉及到系统内核复杂的算法。关于该参数请参考这篇文章[转载]调整虚拟内存，在此不做过多赘述。下面是关于swappiness的相关资料The Linux 2.6 kernel added a new kernel parameter called swappiness to let administrators tweak the way Linux swaps. It is a number from 0 to 100. In essence, higher values lead to more pages being swapped, and lower values lead to more applications being kept in memory, even if they are idle. Kernel maintainer Andrew Morton has said that he runs his desktop machines with a swappiness of 100, stating that \"My point is that decreasing the tendency of the kernel to swap stuff out is wrong. You really don't want hundreds of megabytes of BloatyApp's untouched memory floating about in the machine. Get it out on the disk, use the memory for something useful.\" Swappiness is a property of the Linux kernel that changes the balance between swapping out runtime memory, as opposed to dropping pages from the system page cache.有两种临时修改swappiness参数的方法，系统重启后失效方法1：echo 10 > /proc/sys/vm/swappiness 方法2:sysctl vm.swappiness=10 永久修改swappiness参数的方法就是在配置文件/etc/sysctl.conf里面修改vm.swappiness的值，然后重启系统echo 'vm.swappiness=10' >>/etc/sysctl.conf 如果有人会问是否物理内存使用到某个百分比后才会使用Swap交换空间，可以明确的告诉你不是这样一个算法，如下截图所示，及时物理内存只剩下8M了，但是依然没有使用Swap交换空间，而另外一个例子，物理内存还剩下19G，居然用了一点点Swap交换空间。 五、交换分区管理1、查看Swap分区大小$ free -mh total used free shared buff/cache available Mem: 31G 21G 256M 8.4M 9.6G 9.4G Swap: 4.0G 0B 4.0G $ swapon -s 或者 cat /proc/swaps Filename Type Size Used Priority /dev/vdb partition 4194300 0 -1 2、释放Swap分区空间swapon -s 3、使用swapoff关闭交换分区swapoff /dev/vdb 4、使用swapon启用交换分区swapon /dev/vdb Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:06 "},"origin/linux-硬盘读写性能测试.html":{"url":"origin/linux-硬盘读写性能测试.html","title":"Linux硬盘读写性能测试","keywords":"","body":"Linux硬盘读写性能测试Linux下可使用dd命令来测试硬盘读写速度一、语法简介dd if=path/to/input_file of=/path/to/output_file bs=block_size count=number_of_blocks ​参数if=file 　　　　　　　　　　　　　　　　输入文件名，缺省为标准输入 of=file 　　　　　　　　　　　　　　　　输出文件名，缺省为标准输出 ibs=bytes 　　　　　　　　　　　　　　　一次读入 bytes 个字节(即一个块大小为 bytes 个字节) obs=bytes 　　　　　　　　　　　　　　　一次写 bytes 个字节(即一个块大小为 bytes 个字节) bs=bytes 　　　　　　　　　　　　　　　 同时设置读写块的大小为 bytes ，可代替 ibs 和 obs cbs=bytes 　　　　　　　　　　　　　　　一次转换 bytes 个字节，即转换缓冲区大小 skip=blocks 　　　　　　　　　　　　　 从输入文件开头跳过 blocks 个块后再开始复制 seek=blocks 　　　　　　　　　　 从输出文件开头跳过 blocks 个块后再开始复制(通常只有当输出文件是磁盘或磁带时才有效) count=blocks 　　　　　　　　　　　　　仅拷贝 blocks 个块，块大小等于 ibs 指定的字节数 conv=conversion[,conversion...] 用指定的参数转换文件。 iflag=FLAGS　　　　　　　　　　　　　　指定读的方式FLAGS，参见“FLAGS参数说明” oflag=FLAGS　　　　　　　　　　　　　　指定写的方式FLAGS，参见“FLAGS参数说明” ​ #conv 转换参数： ascii 　　　　　　　　　　　　　　　　　转换 EBCDIC 为 ASCII ebcdic 　　　　　　　　　　　　 　 转换 ASCII 为 EBCDIC ibm 　　　　　　　　　　　　　　　　　　转换 ASCII 为 alternate EBCDIC block 　　　　　　　　　　　　　　　　 把每一行转换为长度为 cbs 的记录，不足部分用空格填充 unblock 　　　　　　　　　　　　　　　 使每一行的长度都为 cbs ，不足部分用空格填充 lcase 　　　　　　　　　　　　　　　　 把大写字符转换为小写字符 ucase 　　　　　　　　　　　　　　　　 把小写字符转换为大写字符 swab 　　　　　　　　　　　　　　　　 交换输入的每对字节 noerror 　　　　　　　　　　　　　　　 出错时不停止 notrunc 　　　　　　　　　　　　　　　 不截短输出文件。 sync 　　　　　　　　　　　　　　　　　 把每个输入块填充到ibs个字节，不足部分用空(NUL)字符补齐 FLAGS 参数说明：​append -append mode (makes sense only for output; conv=notrunc sug-gested) direct　　　　　　　　　　　　　　　 读写数据采用直接IO方式 directory　　　　　　　　　　　　　　读写失败除非是directory dsync　　　　　　　　　　　　　　　　 读写数据采用同步IO sync　　　　　　　　　　　　　　　　　同上，但是针对是元数据 fullblock　　　　　　　　　　　　　　堆积满block（accumulate full blocks of input ）(iflag only) nonblock　　　　　　　　　　　　　　 读写数据采用非阻塞IO方式 noatime　　　　　　　　　　　　　　　 读写数据不更新访问时间 二、time+dd 测磁盘读写速度1、相关参数time有计时作用，dd用于复制，从if读出，写到of if=/dev/zero（产生字符）不产生IO，因此可以用来测试纯写速度 同理of=/dev/null（回收站、无底洞）不产生IO，可以用来测试纯读速度 将/tmp/test拷贝到/var则同时测试了读写速度 bs是每次读或写的大小，即一个块的大小，count是读写块的数量 当写入到驱动盘的时候，我们简单的从无穷无用字节的源 /dev/zero 读取，当从驱动盘读取的时候，我们读取的是刚才的文件，并把输出结果发送到无用的 /dev/null。在整个操作过程中， DD 命令会跟踪数据传输的速度并且报告出结果。2、测试磁盘写能力time dd if=/dev/zero of=/testw.dbf bs=4k count=100000 因为/dev//zero是一个伪设备，它只产生空字符流，对它不会产生IO，所以，IO都会集中在of文件中，of文件只用于写，所以这个命令相当于测试磁盘的写能力。命令结尾添加oflag=direct将跳过内存缓存，添加oflag=sync将跳过hdd缓存。3、测试磁盘读能力time dd if=/dev/sdb of=/dev/null bs=4k 因为/dev/sdb是一个物理分区，对它的读取会产生IO，/dev/null是伪设备，相当于黑洞，of到该设备不会产生IO，所以，这个命令的IO只发生在/dev/sdb上，也相当于测试磁盘的读能力。（Ctrl+c终止测试）4、测试同时读写能力time dd if=/dev/sdb of=/testrw.dbf bs=4k 在这个命令下，一个是物理分区，一个是实际的文件，对它们的读写都会产生IO（对/dev/sdb是读，对/testrw.dbf是写），假设它们都在一个磁盘中，这个命令就相当于测试磁盘的同时读写能力。5、测试纯写入性能dd if=/dev/zero of=test bs=8k count=10000 oflag=direct 6、测试纯读取性能dd if=test of=/dev/null bs=8k count=10000 iflag=direct 注意：dd 只能提供一个大概的测试结果，而且是连续 I/O 而不是随机 I/O，理论上文件规模越大，测试结果越准确。 同时，iflag/oflag 提供 direct 模式，direct 模式是把写入请求直接封装成 I/O 指令发到磁盘，非 direct 模式只是把数据写入到系统缓存就认为 I/O 成功，并由操作系统决定缓存中的数据什么时候被写入磁盘。参考链接http://www.360doc.com/content/15/0906/17/8737500_497292503.shtml Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:06 "},"origin/vim-小技巧.html":{"url":"origin/vim-小技巧.html","title":"Vim小技巧","keywords":"","body":"vim常用配置bash -c 'cat > ~/.vimrc 命令行模式 **功能** **命令** **描述** 水平分屏 ：sp 水平分屏打开另一个文件 垂直分屏 ：vsp 垂直分屏打开另一个文件 多屏间切换 Ctrl+W+W 多屏退出 ：qall 排序 ：sort 去除重复行 ：sort u 移动到文本开头 gg 移动到文本结尾 G 移动到行首 0 即行首有空格的情况，会移动到空格之前 移动到行末 $ 即行末有空格的情况，会移动到空格之后 向下翻页 Ctrl+f 向上翻页 Ctrl+b 以word为单位移动 单词数+W/b,B/b,E/e 2w表示向后移动2个word； 2b表示向前移动2个word； 2e表示向后移动2个word(但是会移动到word字符之后) 如果想忽略标点符号的word，就用大写 W B E 行内查找字符 f+字符 向后移动到某字符 F+字符 向前移动到字符a处 全文查找当前光标处的单词 * 向后查找 #+字符 从文件开头到文件尾开始查找匹配字符 ?+字符 从文件尾倒着到文件开头开始查找匹配字符 光标右边最近数字进行自加 Ctrl+A 光标右边最近数字进行自减 Ctrl+X 删除文本中的空行 ：g/^$/d 注释文本行 v进入视图模式，选择要注释的行，然后Ctrl+v进入块选择模式，然后大写I插入#或者/，再ESC退出 ：起始行号,结束行号s/^注释符//g 在10 - 20行添加 // 注释 :10,20s#^#//#g 在10 - 20行添加 # 注释 :10,20s/^/#/g 快速搜索光标所在单词 Shift+* 显示匹配个数 :%s/xxx//gn 插入模式 **功能** **命令** **描述** 删除光标前面的单词 Ctrl+W 删除光标前面的一行 Ctrl+U 在光标前面插入一个tab Ctrl+I 将光标以下所有内容向上提 Ctrl+H 将光标以下所有内容向下提 Ctrl+J/M 向下联想 Ctrl+N 向上联想 Ctrl+P 示例1、行首或行尾加字符#每行行首加“#” :%s/^/#/g #每行行尾加\" ;\\\" :%s/$/ ;\\\\/g #第二行到第十五行的行首添加“==” :2,15 s/^/==/g #第二行到文本末行的行首添加“==” :2,$ s/^/==/g #第二行到文本首行的行首添加“==” :2,1 s/^/==/g 2、将文本中相同数字进行自增原始文本docker save -o 1.tar docker.io/openshiftistio/origin-ansible:0.7.1 ;\\ docker save -o 1.tar docker.io/openshiftistio/origin-ansible:0.8.0 ;\\ docker save -o 1.tar docker.io/skydive/skydive:latest \\ :g/1.tar/ s//\\=line('.').'.tar'/ 效果文本docker save -o 1.tar docker.io/openshiftistio/origin-ansible:0.7.1 ;\\ docker save -o 2.tar docker.io/openshiftistio/origin-ansible:0.8.0 ;\\ docker save -o 3.tar docker.io/skydive/skydive:latest ;\\ 3、去除文本中的换行符^MWindows下保存的文本文件，上传到Linux/Unix下后总会在末尾多了一个换行符^M，导致一些xml、ini、sh等文件读取错误进入命令模式 %s/^M//g (注意，^M = Ctrl v + Ctrl m，而不是手动输入^M) # ^M 表示清除成功 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:13 "},"origin/linux-yum.html":{"url":"origin/linux-yum.html","title":"Yum-RPM包管理","keywords":"","body":"YUM详解一、OverviewsYum（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。Yum的关键之处是要有可靠的repository，顾名思义这就是软件的仓库，它可以是http或者ftp站点，也可以是本地的软件池，但是必须包含rpm的header，rmp的header包括了rmp的各种信息，包括描述、功能、提供的文件、依赖性等，正是收集了这些信息，才能自动化的完成余下的任务。repo文件是yum源（软件仓库）的配置文件，通常一个repo文件定义了一个或者多个软件仓库的细节内容RPM包的名称规则示例：ttpd-manual- 2.0.40-21.i386.rpm，ttp-manual是软件包的名称，2是主版本号；0是次版本号；40是修正号；21是编译的次数；i386是适合的平台二、YUM命令详解yum的命令形式一般是如下：yum [选项] [参数] [package ...]选项 -h, --help 显示此帮助消息并退出 -t, --tolerant 忽略错误 -C, --cacheonly 完全从系统缓存运行，不升级缓存 -c [config file], --config=[config file] 配置文件路径 -R [minutes], --randomwait=[minutes] 命令最长等待时间 -d [debug level], --debuglevel=[debug level] 调试输出级别 --showduplicates 在 list/search 命令下，显示源里重复的条目 -e [error level], --errorlevel=[error level] 错误输出级别 --rpmverbosity=[debug level name] RPM 调试输出级别 -q, --quiet 静默执行 -v, --verbose 详尽的操作过程 -y, --assumeyes 回答全部问题为是 --assumeno 回答全部问题为否 --version 显示 Yum 版本然后退出 --installroot=[path] 设置安装根目录 --enablerepo=[repo] 启用一个或多个软件源(支持通配符) --disablerepo=[repo] 禁用一个或多个软件源(支持通配符) -x [package], --exclude=[package] 采用全名或通配符排除软件包 --disableexcludes=[repo] 禁止从主配置，从源或者从任何位置排除 --disableincludes=[repo] disable includepkgs for a repo or for everything --obsoletes 更新时处理软件包取代关系 --noplugins 禁用 Yum 插件 --nogpgcheck 禁用 GPG 签名检查 --disableplugin=[plugin] 禁用指定名称的插件 --enableplugin=[plugin] 启用指定名称的插件 --skip-broken 忽略存在依赖关系问题的软件包 --color=COLOR 配置是否使用颜色 --releasever=RELEASEVER 在 yum 配置和 repo 文件里设置 $releasever 的值 --downloadonly 仅下载而不更新 --downloaddir=DLDIR 指定一个其他文件夹用于保存软件包 --setopt=SETOPTS 设置任意配置和源选项 --bugfix Include bugfix relevant packages, in updates --security Include security relevant packages, in updates --advisory=ADVS, --advisories=ADVS Include packages needed to fix the given advisory, in updates --bzs=BZS Include packages needed to fix the given BZ, in updates --cves=CVES Include packages needed to fix the given CVE, in updates --sec-severity=SEVS, --secseverity=SEVS Include security relevant packages matching the severity, in updates 参数check 检查 RPM 数据库问题 check-update 检查是否有可用的软件包更新 clean 删除缓存数据 deplist 列出软件包的依赖关系 distribution-synchronization 已同步软件包到最新可用版本 downgrade 降级软件包 erase 从系统中移除一个或多个软件包 fs Acts on the filesystem data of the host, mainly for removing docs/lanuages for minimal hosts. fssnapshot Creates filesystem snapshots, or lists/deletes current snapshots. groups 显示或使用、组信息 help 显示用法提示 history 显示或使用事务历史 info 显示关于软件包或组的详细信息 install 向系统中安装一个或多个软件包 langavailable Check available languages langinfo List languages information langinstall Install appropriate language packs for a language langlist List installed languages langremove Remove installed language packs for a language list 列出一个或一组软件包 load-transaction 从文件名中加载一个已存事务 makecache 创建元数据缓存 provides 查找提供指定内容的软件包 reinstall 覆盖安装软件包 repo-pkgs 将一个源当作一个软件包组，这样我们就可以一次性安装/移除全部软件包。 repolist 显示已配置的源 search 在软件包详细信息中搜索指定字符串 shell 运行交互式的 yum shell swap Simple way to swap packages, instead of using shell update 更新系统中的一个或多个软件包 update-minimal Works like upgrade, but goes to the 'newest' package match which fixes a problem that affects your system updateinfo Acts on repository update information upgrade 更新软件包同时考虑软件包取代关系 version 显示机器和/或可用的源版本。 常用命令清除缓存yum clean [headers, packages, metadata, dbcache, plugins, expire-cache, rpmdb, all] headers--清除缓存目录(/var/cache/yum)下的 headers packages--清除缓存目录(/var/cache/yum)下的软件包 all--清除所有缓存 yum update与yum upgrade的区别yum update 只更新软件，不更新内核 yum upgrade 升级所有包，不改变软件设置和系统设置，系统版本升级，内核不改变 三、repo文件[serverid] #serverid是用于区别各个不同的repository，必须有一个独一无二的名称。若重复了，是前面覆盖后面--还是反过来呢？？？用enabled 测试是后面覆盖前面 name=Some name for this server #name，是对repository的描述，支持像$releasever $basearch这样的变量; name=Fedora Core $releasever - $basearch - Released Updates baseurl=url://path/to/repository/ # 1. 格式: baseurl=url://server1/path/to/repository/, url支持的协议有 http:// ftp:// file://三种。 # 2. baseurl后可以跟多个url，你可以自己改为速度比较快的镜像站，但#baseurl只能有一个 # 3. 其中url指向的目录必须是这个repository header目录的上一级，它也支持$releasever $basearch这样的变量。 gpgcheck=1 exclude=gaim failovermethod=priority #failovermethode有两个选项roundrobin和priority，意思分别是有多个url可供选择时，yum选择的次序，roundrobin是随机选择，如果连接失 败则使用下一个，依次循环，priority则根据url的次序从第一个开始。如果不指明，默认是roundrobin。 enabled=[1 or 0] # 1. 当某个软件仓库被配置成 enabled=0 时，yum 在安装或升级软件包时不会将该仓库做为软件包提供源。使用这个选项，可以启用或禁用软件仓库。 # 2. 通过 yum 的 --enablerepo=[repo_name] 和 --disablerepo=[repo_name] 选项，或者通过 PackageKit 的\"添加/删除软件\"工具，也能够方便地启用和禁用指定的软件仓库 变量解释： # $releasever 发行版的版本，从[main]部分的distroverpkg获取，如果没有，则根据redhat-release包进行判断。 # $arch cpu体系，如i686,athlon等 # $basearch cpu的基本体系组，如i686和athlon同属i386，alpha和alphaev6同属alpha。 四、yum-fastestmirror插件yum-fastestmirror插件，它会自动选择最快的mirror。它的配置文件/etc/yum/pluginconf.d/fastestmirror.conf，yum镜像的速度测试记录文件/var/cache/yum/x86_64/7/timedhosts.txt**禁用插件配置修改插件的配置文件 sed -i 's/enabled=1/enabled=0/' /etc/yum/pluginconf.d/fastestmirror.conf enabled = 1//由1改为0，禁用该插件 修改yum的配置文件 # sed -i 's/plugins=1/plugins=0/' /etc/yum.conf plugins=1 //改为0，不使用插件 五、YUM源的创建1、使用Nexus的YUN格式仓库作为YUM镜像源详见：Nexus中yum仓库的配置与使用2、Createrepo创建本地YUM镜像源将CentOS版本系统镜像中的Packages并上传到主机上的某一目录下 安装createrepo用来创建软件包的索引。或者将系统镜像中repodata目录放到rpm包路径下 yum install createrepo -y createrepo /data/localrepo/Office 会在创建repodata索引文件夹 在/etc/yum.repos.d/目录下创建repo文件local.repo [local] name=Local Yum Office Repository 仓库名 baseurl=file:///data/localrepo/Office 仓库中rpm包存放路径 gpgcheck=1 是否检查GPG-KEY，0为不检查，1为检查 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 GPG-KEY的存放路径 enabled=1 设置为1表示启用本Repo Createrepo命令参数详解 -u --baseurl 指定Base URL的地址 -o --outputdir 指定元数据的输出位置 -x --excludes 指定在形成元数据时需要排除的包 -i --pkglist 指定一个文件，该文件内的包信息将被包含在即将生成的元数据中，格式为每个包信息独占一行，不含通配符、正则，以及范围表达式。 -n --includepkg 通过命令行指定要纳入本地库中的包信息，需要提供URL或本地路径。 -q --quiet 安静模式执行操作，不输出任何信息。 -g --groupfile 指定本地软件仓库的组划分，范例如下：createrepo -g comps.xml /path/to/rpms注意：组文件需要和rpm包放置于同一路径下。 -v --verbose 输出详细信息。 -c --cachedir 指定一个目录，用作存放软件仓库中软件包的校验和信息。 当createrepo在未发生明显改变的相同仓库文件上持续多次运行时，指定cachedir会明显提高 其性能。 --update 如果元数据已经存在，且软件仓库中只有部分软件发生了改变或增减， 则可用update参数直接对原有元数据进行升级，效率比重新分析rpm包依赖并生成新的元数据要 高很多。 -p --pretty 以整洁的格式输出xml文件。 -d --database 该选项指定使用SQLite来存储生成的元数据，默认项。 3、HTTPD+Createrepo创建YUM镜像源第二种方法创建的镜像源只能在本地使用，要是能在局域网中提供公共的服务，需要一个能提供HTTP服务的容器，可使用HTTPD（又称Apache），步骤省略。六、Reposync同步YUM远程仓库的安装包1、安装yum install yum-utils -y 2、命令参数Usage: Reposync is used to synchronize a remote yum repository to a local directory using yum to retrieve the packages. /usr/bin/reposync [options] Options: -h, --help show this help message and exit -c CONFIG, --config=CONFIG config file to use (defaults to /etc/yum.conf) -a ARCH, --arch=ARCH act as if running the specified arch (default: current arch, note: does not override $releasever. x86_64 is a superset for i*86.) --source operate on source packages -r REPOID, --repoid=REPOID secify repo ids to query, can be specified multiple times (default is all enabled) -e CACHEDIR, --cachedir=CACHEDIR directory in which to store metadata -t, --tempcache Use a temp dir for storing/accessing yum-cache -d, --delete delete local packages no longer present in repository -p DESTDIR, --download_path=DESTDIR Path to download packages to: defaults to current dir --norepopath Don't add the reponame to the download path. Can only be used when syncing a single repository (default is to add the reponame) -g, --gpgcheck Remove packages that fail GPG signature checking after downloading -u, --urls Just list urls of what would be downloaded, don't download -n, --newest-only Download only newest packages per-repo -q, --quiet Output as little as possible -l, --plugins enable yum plugin support -m, --downloadcomps also download comps.xml --download-metadata download all the non-default metadata 3、示例$> bash -c 'cat > ceph.repo 七、Yumdownloadonly下载RPM包及依赖包1、下载yumdownloadonly插件yum install yum-plugin-downloadonly 2、下载到指定目录（依赖包会一起下载）yum install --downloadonly --downloaddir=/root/httpd httpd 附录：清华镜像站中常见的软件源1、VirtualBox[virtualbox] name=Oracle Linux / RHEL / CentOS-$releasever / $basearch - VirtualBox baseurl=http://download.virtualbox.org/virtualbox/rpm/el/$releasever/$basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://www.virtualbox.org/download/oracle_vbox.asc 2、Nginx[nginx] name=nginx repo baseurl=http://nginx.org/packages/OS/OSRELEASE/$basearch/ gpgcheck=0 enabled=1 3、EPEL[epel] name=Extra Packages for Enterprise Linux 7 - $basearch #baseurl=http://download.fedoraproject.org/pub/epel/7/$basearch metalink=https://mirrors.fedoraproject.org/metalink?repo=epel-7&arch=$basearch failovermethod=priority enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 4、Ceph[Ceph] name=Ceph packages for $basearch baseurl=http://download.ceph.com/rpm-jewel/el7/$basearch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=http://download.ceph.com/rpm-jewel/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 5、ELK Stack[ELK-Stack-5.x] name=ELK Stack repository for 5.x packages baseurl=https://mirrors.tuna.tsinghua.edu.cn/elasticstack/5.x/yum/ gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md [ELK-Stack-6.x] name=ELK Stack repository for 6.x packages baseurl=https://mirrors.tuna.tsinghua.edu.cn/elasticstack/6.x/yum/ gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md 6、MySQL[MySQL-Community-5.6] name=MySQL Community 5.6 baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql56-community-el7/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 [MySQL-Community-5.7] name=MySQL Community 5.7 baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql57-community-el7/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 [MySQL-Community-Connectors] name=MySQL Community Connectors baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-connectors-community-el7/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 [MySQL-Community-Tools] name=MySQL Community Tools baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-tools-community-el7/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 7、MongoDBbash -c 'cat > /etc/yum.repos.d/mongoDb.repo Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:05 "},"origin/linux-zsh.html":{"url":"origin/linux-zsh.html","title":"ZSH","keywords":"","body":"Linux zsh && oh-my-zsh一、简介Zsh 也许是目前最好用的 shell，是 bash 替代品中较为优秀的一个。Zsh 官网：http://www.zsh.org/Zsh具有以下主要优势：完全兼容bash，之前bash下的使用习惯，shell脚本都可以完全兼容 更强大的tab补全 更智能的切换目录 命令选项、参数补齐 大小写字母自动更正 有着丰富多彩的主题 更强大的alias命令 智能命令错误纠正 集成各种类型的插件 oh-my-zsh 是最为流行的 zsh 配置文件，提供了大量的主题和插件，极大的拓展了 zsh 的功能，推动了 zsh 的流行，有点类似于 rails 之于 ruby。二、安装zshCentOSyum install -y zsh Ubuntuapt-get install -y zsh 检查下系统的 shell：$ cat /etc/shells，你会发现多了一个：/bin/zsh设置用户的默认shell# 给root用户设置 chsh -s /bin/zsh root # 给普通账户设置 chsh -s /bin/zsh 用户名 #　恢复bash chsh -s /bin/bash [user] 三、安装oh-my-zshoh-my-zsh 帮我们整理了一些常用的 Zsh 扩展功能和主题，我们无需自己去捣搞 Zsh，直接用 oh-my-zsh 就足够了。oh-my-zsh 官网：https://ohmyz.sh/oh-my-zsh Github：https://github.com/robbyrussell/oh-my-zshcurlsh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" wgetsh -c \"$(wget -O- https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" 手动curl -Lo install.sh https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh sh install.sh 四、oh-my-zsh配置oh-my-zsh的配置文件路径为~/.zshrc# If you come from bash you might have to change your $PATH. # export PATH=$HOME/bin:/usr/local/bin:$PATH # 指定oh-my-zsh的安装路径 export ZSH=\"/root/.oh-my-zsh\" # 设置主题。如果设置为\"random\", 每次启动oh-my-zsh会随机加载主题,可使用`echo $RANDOM_THEME`查看每次加载的主题 ZSH_THEME=\"alanpeabody\" # 设置随机的主题 # ZSH_THEME_RANDOM_CANDIDATES=( \"robbyrussell\" \"agnoster\" ) # 大小写是否敏感 # CASE_SENSITIVE=\"true\" # Uncomment the following line to use hyphen-insensitive completion. # Case-sensitive completion must be off. _ and - will be interchangeable. # HYPHEN_INSENSITIVE=\"true\" # 设置是否自动更新 # DISABLE_AUTO_UPDATE=\"true\" # 设置自动更新的天数。默认13天 # export UPDATE_ZSH_DAYS=13 # 设置是否开启`ls`进行颜色显示 # DISABLE_LS_COLORS=\"true\" # 设置是否显示终端标题 # DISABLE_AUTO_TITLE=\"true\" # 设置是否开启语法修正 # ENABLE_CORRECTION=\"true\" # Uncomment the following line to display red dots whilst waiting for completion. # COMPLETION_WAITING_DOTS=\"true\" # Uncomment the following line if you want to disable marking untracked files # under VCS as dirty. This makes repository status check for large repositories # much, much faster. # DISABLE_UNTRACKED_FILES_DIRTY=\"true\" # 历史输入命令的时间展示格式 # HIST_STAMPS=\"mm/dd/yyyy\" # 设置自定义配置文件的路径 # ZSH_CUSTOM=/path/to/new-custom-folder # 配置要加载的插件（配置的插件要能在 ~/.oh-my-zsh/plugins/* 下找到，自定义的插件目录为 ~/.oh-my-zsh/custom/plugins/ ）.注意：插件安装的越多，zsh的启动速度越慢，选择使用率最高的插件才是最好的选择 plugins=( git zsh-autosuggestions zsh-syntax-highlighting kubectl docker sudo extract ) source /etc/profile source $ZSH/oh-my-zsh.sh # 用户配置 # 设置man文档的环境变量 # export MANPATH=\"/usr/local/man:$MANPATH\" # 设置语言环境变量 # export LANG=en_US.UTF-8 # 设置本地和远程sessions的首选编辑器 # if [[ -n $SSH_CONNECTION ]]; then # export EDITOR='vim' # else # export EDITOR='mvim' # fi # 设置编译标志 # export ARCHFLAGS=\"-arch x86_64\" # 设置别名 alias ll=\"ls -alh\" alias k='kubectl' . ~/.oh-my-zsh/custom/oc_zsh_completion 五、oh-my-zsh常用插件git：可以使用git缩写，默认自带git add --all => gaa查看所有缩写：alias | grep git autojump：快速跳转文件夹 last-working-dir：可以记录上一次退出命令行时候的所在路径，并且在下一次启动命令行的时候自动恢复到上一次所在的路径。 wd：快速地切换到常用的目录 wd add web相当于给当前目录做了一个标识，标识名叫做 web ，我们下次如果再想进入这个目录，只需输入：wd web catimg：将图片的内容输出到命令行 catimg demo.jpgzsh-syntax-highlighting：命令高亮 正确路径自带下划线安装：git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting zsh-autosuggestions：自动补全可能的路径安装：git clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions sudo：连按两次Esc添加或去掉sudo extract：功能强大的解压插件 执行x demo.tar.gz git-open：在终端里打开当前项目的远程仓库地址安装：git clone https://github.com/paulirish/git-open.git $ZSH_CUSTOM/plugins/git-open 六、oh-my-zsh常用主题官方主题：https://github.com/robbyrussell/oh-my-zsh/wiki/ThemesCopyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:05 "},"origin/linux-进程管理工具SystemD.html":{"url":"origin/linux-进程管理工具SystemD.html","title":"Systemd-进程管理","keywords":"","body":"Linux 进程管理工具SystemD一、简介SystemD即为system daemon，是linux下的一种init软件，由Lennart Poettering带头开发，并在LGPL 2.1及其后续版本许可证下开源发布，开发目标是提供更优秀的框架以表示系统服务间的依赖关系，并依此实现系统初始化时服务的并行启动，同时达到降低Shell的系统开销的效果，最终代替现在常用的System V与BSD风格init程序。SystemD是一个专用于 Linux 操作系统的系统与服务管理器。当作为启动进程(PID=1)运行时，它将作为初始化系统运行，也就是启动并维护各种用户空间的服务。Linux内核加载启动后，用户空间的第一个进程就是初始化进程，这个程序的物理文件约定位于/sbin/init，当然也可以通过传递内核参数来让内核启动指定的程序。这个进程的特点是进程号为1，代表第一个运行的用户空间进程。不同发行版采用了不同的启动程序，主要有以下几种主流选择：以Ubuntu为代表的Linux发行版采用upstart。 以7.0版本之前的CentOS为代表的System V init。 CentOS 7.0版本开始的Systemd。 为了与传统的 SysV 兼容，如果将 systemd 以 init 名称启动，并且\"PID≠1\"，那么它将执行 telinit 命令并将所有命令行参数原封不动的传递过去。 这样对于普通的登录会话来说，无论是调用 init 还是调用 telinit 都是等价的。 当作为系统实例运行时，systemd将会按照system.conf配置文件以及system.conf.d配置目录中的指令工作；当作为用户实例运行时，systemd 将会按照user.conf配置文件 以及 user.conf.d配置目录中的指令工作。 systemd将各种系统启动和运行相关的对象，表示为各种不同类型的单元(unit)，并提供了处理不同单元之间依赖关系的能力。大部分单元都静态的定义在单元文件中，但是有少部分单元则是动态自动生成的：其中一部分来自于其他传统的配置文件(为了兼容性)，而另一部分则动态的来自于系统状态或可编程的运行时状态。单元既可以处于活动(active)状态，也可以处于停止(inactive)状态，当然也可以处于启动中(activating)或停止中(deactivating)的状态。还有一个特殊的失败(failed)状态，意思是单元以某种方式失败了(进程崩溃了、或者触碰启动频率限制、或者退出时返回了错误代码、或者遇到了操作超时之类的故障)。当进入失败(failed)状态时，导致故障的原因将被记录到日志中以方便日后排查。需要注意的是，不同的单元可能还会有各自不同的\"子状态\"，但它们都被映射到上述五种状态之一。历史上，Linux 的启动一直采用init进程。这种方法有两个缺点启动时间长。init进程是串行启动，只有前一个进程启动完，才会启动下一个进程。 启动脚本复杂。init进程只是执行启动脚本，不管其他事情。脚本需要自己处理各种情况，这往往使得脚本变得很长。 当前系统/etc/inittab这个文件的内容，这个文件是systme V init的标准配置文件，如今变成了# inittab is no longer used when using systemd. # # ADDING CONFIGURATION HERE WILL HAVE NO EFFECT ON YOUR SYSTEM. # # Ctrl-Alt-Delete is handled by /etc/systemd/system/ctrl-alt-del.target # # systemd uses 'targets' instead of runlevels. By default, there are two main targets: # # multi-user.target: analogous to runlevel 3 # graphical.target: analogous to runlevel 5 # # To set a default target, run: # systemctl set-default TARGET.target 在systemd掌权后，inittab不再起作用，也没有了“运行级”的概念。现在起作用的配置文件是/etc/systemd/system/default.target这个文件了。此文件的内容如下：# This file is part of systemd. # # systemd is free software; you can redistribute it and/or modify it # under the terms of the GNU Lesser General Public License as published by # the Free Software Foundation; either version 2.1 of the License, or # (at your option) any later version. [Unit] Description=Multi-User System Documentation=man:systemd.special(7) Requires=basic.target Conflicts=rescue.service rescue.target After=basic.target rescue.service rescue.target AllowIsolate=yes 作为系统初始化系统，systemd 的最大特点有两个：令人惊奇的激进的并发启动能力，极大地提高了系统启动速度； 用 CGroup 统计跟踪子进程，干净可靠。 此外，和其前任不同的地方在于:systemd 已经不仅仅是一个初始化系统了 Systemd 出色地替代了 sysvinit 的所有功能。因为 init 进程是系统所有进程的父进程这样的特殊性，systemd 非常适合提供曾经由其他服务提供的功能，比如定时任务 (以前由 crond 完成) ；会话管理 (以前由 ConsoleKit/PolKit 等管理) 有助于标准化 Linux 的管理！如果所有的 Linux 发行版都采纳了 systemd，那么系统管理任务便可以很大程度上实现标准化 此外 systemd 有个很棒的承诺：接口保持稳定，不会再轻易改动 根据 Linux 惯例，字母d是守护进程（daemon）的缩写。 Systemd 这个名字的含义，就是它要守护整个系统 systmed是一个用户空间的程序，属于应用程序，不属于Linux内核范畴，Linux内核的主要特征在所有发行版中是统一的，厂商可以自由改变的是用户空间的应用程序 二、基本概念1. Unit单元系统初始化要做很多工作，如挂在文件系统，启动sshd服务，配置交换分区，这都可以看做是一个配置单元，Systemd 可以管理所有系统资源。不同的资源统称为 Unit（单位）。systemd把配置单元分成分成12种Service unit：系统服务 Target unit：多个 Unit 构成的一个逻辑分组，可以当成是SystemV中的运行级。 Device Unit：硬件设备 Mount Unit：文件系统的挂载点，systemd据此进行自动挂载，为了与SystemV兼容，目前systemd自动处理/etc/fstab并转化为mount Automount Unit：自动挂载点 Path Unit：文件或路径 Scope Unit：不是由 Systemd 启动的外部进程 Slice Unit：进程组 Snapshot Unit：Systemd 快照，可以切回某个快照 Socket Unit：进程间通信的 socket Swap Unit：配置swap交换分区文件 Timer Unit：定时器。用来定时触发用户定义的操作，它可以用来取代传统的atd，crond等。 每一个配置单元都有一个对应的配置文件，系统管理员的任务就是编写和维护这写不同的配置文件，比如一个MySql服务对应一个mysql.service文件。2. 依赖关系systemd并不能完全解除各个单元之间的依赖关系，如物理设备单元准备就绪之前，不可能执行挂载单元。为此需要定义各个单元之间的依赖关系。有依赖的地方就会有出现死循环的可能，比如A依赖于B，B依赖于C，C依赖于A，那么导致死锁。systemd为此提供了两种不同程度的依赖关系，一个是require，一个是want，出现死循环时，systemd会尝试忽略want类型的依赖，如仍不能解锁，那么systemd报错。如前所述，在 Systemd 中，所有的服务都并发启动，比如 Avahi、D-Bus、livirtd、X11、HAL 可以同时启动。乍一看，这似乎有点儿问题，比如 Avahi 需要 syslog 的服务，Avahi 和 syslog 同时启动，假设 Avahi 的启动比较快，所以 syslog 还没有准备好，可是 Avahi 又需要记录日志，这岂不是会出现问题？Systemd 的开发人员仔细研究了服务之间相互依赖的本质问题，发现所谓依赖可以分为三个具体的类型，而每一个类型实际上都可以通过相应的技术解除依赖关系。并发启动原理之一：解决 socket 依赖绝大多数的服务依赖是套接字依赖。比如服务 A 通过一个套接字端口 S1 提供自己的服务，其他的服务如果需要服务 A，则需要连接 S1。因此如果服务 A 尚未启动，S1 就不存在，其他的服务就会得到启动错误。所以传统地，人们需要先启动服务 A，等待它进入就绪状态，再启动其他需要它的服务。Systemd 认为，只要我们预先把 S1 建立好，那么其他所有的服务就可以同时启动而无需等待服务 A 来创建 S1 了。如果服务 A 尚未启动，那么其他进程向 S1 发送的服务请求实际上会被 Linux 操作系统缓存，其他进程会在这个请求的地方等待。一旦服务 A 启动就绪，就可以立即处理缓存的请求，一切都开始正常运行。那么服务如何使用由 init 进程创建的套接字呢？Linux 操作系统有一个特性，当进程调用 fork 或者 exec 创建子进程之后，所有在父进程中被打开的文件句柄 (file descriptor) 都被子进程所继承。套接字也是一种文件句柄，进程 A 可以创建一个套接字，此后当进程 A 调用 exec 启动一个新的子进程时，只要确保该套接字的 close_on_exec 标志位被清空，那么新的子进程就可以继承这个套接字。子进程看到的套接字和父进程创建的套接字是同一个系统套接字，就仿佛这个套接字是子进程自己创建的一样，没有任何区别。这个特性以前被一个叫做 inetd 的系统服务所利用。Inetd 进程会负责监控一些常用套接字端口，比如 Telnet，当该端口有连接请求时，inetd 才启动 telnetd 进程，并把有连接的套接字传递给新的 telnetd 进程进行处理。这样，当系统没有 telnet 客户端连接时，就不需要启动 telnetd 进程。Inetd 可以代理很多的网络服务，这样就可以节约很多的系统负载和内存资源，只有当有真正的连接请求时才启动相应服务，并把套接字传递给相应的服务进程。和 inetd 类似，systemd 是所有其他进程的父进程，它可以先建立所有需要的套接字，然后在调用 exec 的时候将该套接字传递给新的服务进程，而新进程直接使用该套接字进行服务即可。并发启动原理之二：解决 D-Bus 依赖D-Bus 是 desktop-bus 的简称，是一个低延迟、低开销、高可用性的进程间通信机制。它越来越多地用于应用程序之间通信，也用于应用程序和操作系统内核之间的通信。很多现代的服务进程都使用D-Bus 取代套接字作为进程间通信机制，对外提供服务。比如简化 Linux 网络配置的 NetworkManager 服务就使用 D-Bus 和其他的应用程序或者服务进行交互：邮件客户端软件 evolution 可以通过 D-Bus 从 NetworkManager 服务获取网络状态的改变，以便做出相应的处理。D-Bus 支持所谓\"bus activation\"功能。如果服务 A 需要使用服务 B 的 D-Bus 服务，而服务 B 并没有运行，则 D-Bus 可以在服务 A 请求服务 B 的 D-Bus 时自动启动服务 B。而服务 A 发出的请求会被 D-Bus 缓存，服务 A 会等待服务 B 启动就绪。利用这个特性，依赖 D-Bus 的服务就可以实现并行启动。并发启动原理之三：解决文件系统依赖系统启动过程中，文件系统相关的活动是最耗时的，比如挂载文件系统，对文件系统进行磁盘检查（fsck），磁盘配额检查等都是非常耗时的操作。在等待这些工作完成的同时，系统处于空闲状态。那些想使用文件系统的服务似乎必须等待文件系统初始化完成才可以启动。但是 systemd 发现这种依赖也是可以避免的。Systemd 参考了 autofs 的设计思路，使得依赖文件系统的服务和文件系统本身初始化两者可以并发工作。autofs 可以监测到某个文件系统挂载点真正被访问到的时候才触发挂载操作，这是通过内核 automounter 模块的支持而实现的。比如一个 open()系统调用作用在\"/misc/cd/file1\"的时候，/misc/cd 尚未执行挂载操作，此时 open()调用被挂起等待，Linux 内核通知 autofs，autofs 执行挂载。这时候，控制权返回给 open()系统调用，并正常打开文件。Systemd 集成了 autofs 的实现，对于系统中的挂载点，比如/home，当系统启动的时候，systemd 为其创建一个临时的自动挂载点。在这个时刻/home 真正的挂载设备尚未启动好，真正的挂载操作还没有执行，文件系统检测也还没有完成。可是那些依赖该目录的进程已经可以并发启动，他们的 open()操作被内建在 systemd 中的 autofs 捕获，将该 open()调用挂起（可中断睡眠状态）。然后等待真正的挂载操作完成，文件系统检测也完成后，systemd 将该自动挂载点替换为真正的挂载点，并让 open()调用返回。由此，实现了那些依赖于文件系统的服务和文件系统本身同时并发启动。当然对于\"/\"根目录的依赖实际上一定还是要串行执行，因为 systemd 自己也存放在/之下，必须等待系统根目录挂载检查好。不过对于类似/home 等挂载点，这种并发可以提高系统的启动速度，尤其是当/home 是远程的 NFS 节点，或者是加密盘等，需要耗费较长的时间才可以准备就绪的情况下，因为并发启动，这段时间内，系统并不是完全无事可做，而是可以利用这段空余时间做更多的启动进程的事情，总的来说就缩短了系统启动时间。3. Target和runlevelsystemd使用target取代了systemV的运行级的概念，Sysvinit 运行级别和 systemd 目标的对应表 Sysvinit 运行级别 Systemd 目标 备注 0 runlevel0.target, poweroff.target 关闭系统。 1, s, single runlevel1.target, rescue.target 单用户模式。 2, 4 runlevel2.target, runlevel4.target, multi-user.target 用户定义/域特定运行级别。默认等同于 3。 3 runlevel3.target, multi-user.target 多用户，非图形化。用户可以通过多个控制台或网络登录。 5 runlevel5.target, graphical.target 多用户，图形化。通常为所有运行级别 3 的服务外加图形化登录。 6 runlevel6.target, reboot.target 重启 emergency emergency.target 紧急 Shell 三、Systemd包含的命令Systemd 是一个完整的软件包，Systemd 并不是一个命令，而是一组命令，涉及到系统管理的方方面面。安装完成后有很多物理文件组成，大致分布为，配置文件位于/etc/systemd这个目录下，配置工具命令位于/bin，和/sbin这两个目录下，预先准备的备用配置文件位于/lib/systemd目录下，还有库文件和帮助手册等等。这是一个庞大的软件包。详情使用rpm -ql systemd即可查看。1. 电源管理工具Systemd 可用于管理系统。关机不是每个登录用户在任何情况下都可以执行的，一般只有管理员才可以关机。正常情况下系统不应该允许 SSH 远程登录的用户执行关机命令。否则其他用户正在工作，一个用户把系统关了就不好了。为了解决这个问题，传统的 Linux 系统使用 ConsoleKit 跟踪用户登录情况，并决定是否赋予其关机的权限。现在 ConsoleKit 已经被 systemd 的 logind 所替代。logind 不是 pid-1 的 init 进程。它的作用和 UpStart 的 session init 类似，但功能要丰富很多，它能够管理几乎所有用户会话(session)相关的事情。logind 不仅是 ConsoleKit 的替代，它可以：维护，跟踪会话和用户登录情况。如上所述，为了决定关机命令是否可行，系统需要了解当前用户登录情况，如果用户从 SSH 登录，不允许其执行关机命令；如果普通用户从本地登录，且该用户是系统中的唯一会话，则允许其执行关机命令；这些判断都需要 logind 维护所有的用户会话和登录情况。 Logind 也负责统计用户会话是否长时间没有操作，可以执行休眠/关机等相应操作。 为用户会话的所有进程创建 CGroup。这不仅方便统计所有用户会话的相关进程，也可以实现会话级别的系统资源控制。 负责电源管理的组合键处理，比如用户按下电源键，将系统切换至睡眠状态。 多席位(multi-seat) 管理。如今的电脑，即便一台笔记本电脑，也完全可以提供多人同时使用的计算能力。多席位就是一台电脑主机管理多个外设，比如两个屏幕和两个鼠标/键盘。席位一使用屏幕 1 和键盘 1；席位二使用屏幕 2 和键盘 2，但他们都共享一台主机。用户会话可以自由在多个席位之间切换。或者当插入新的键盘，屏幕等物理外设时，自动启动 gdm 用户登录界面等。所有这些都是多席位管理的内容。ConsoleKit 始终没有实现这个功能，systemd 的 logind 能够支持多席位。 # 重启系统 $ systemctl reboot # 关闭系统，切断电源 $ systemctl poweroff # CPU停止工作 $ systemctl halt # 暂停系统 $ systemctl suspend # 让系统进入冬眠状态 $ systemctl hibernate # 让系统进入交互式休眠状态 $ systemctl hybrid-sleep # 启动进入救援状态（单用户状态） $ systemctl rescue 2. 服务消耗分析工具systemd-analyze命令用于查看启动耗时# 查看启动耗时 $ systemd-analyze # 查看每个服务的启动耗时 $ systemd-analyze blame # 显示瀑布状的启动过程流 $ systemd-analyze critical-chain # 显示指定服务的启动流 $ systemd-analyze critical-chain atd.service 3. 主机信息信息管理工具hostnamectl命令用于查看当前主机的信息。# 显示当前主机的信息 $ hostnamectl # 设置主机名。 $ hostnamectl set-hostname rhel7 4. 本地化设置管理工具localectl命令用于查看本地化设置。# 查看本地化设置 $ localectl # 设置本地化参数。 $ localectl set-locale LANG=en_GB.utf8 $ localectl set-keymap en_GB 5. 时区管理工具timedatectl命令用于查看当前时区设置。# 查看当前时区设置 $ timedatectl # 显示所有可用的时区 $ timedatectl list-timezones # 设置当前时区 $ timedatectl set-timezone America/New_York $ timedatectl set-time YYYY-MM-DD $ timedatectl set-time HH:MM:SS 6. 登陆会话管理工具loginctl命令用于查看当前登录的用户。# 列出当前session $ loginctl list-sessions # 列出当前登录用户 $ loginctl list-users # 列出显示指定用户的信息 $ loginctl show-user test 三、Unit每一个 Unit 都有一个配置文件，告诉 Systemd 怎么启动这个 Unit 。 Systemd 默认从目录/etc/systemd/system/读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录/usr/lib/systemd/system/，真正的配置文件存放在那个目录。 配置文件的后缀名，就是该 Unit 的种类，比如sshd.socket。如果省略，Systemd 默认后缀名为.service，所以sshd会被理解成sshd.service。 1. Unit配置文件结构配置文件分成几个区块。每个区块的第一行，是用方括号表示的区别名，比如[Unit]。注意：配置文件的区块名和字段名，都是大小写敏感的。 每个区块内部是一些等号连接的键值对，键值对的等号两侧不能有空格。 [Unit] Description=ATD daemon [Service] Type=forking ExecStart=/usr/bin/atd [Install] WantedBy=multi-user.target Unit 配置文件的完整字段清单，请参考官方文档。[Unit]区块[Unit]区块通常是配置文件的第一个区块。用来定义 Unit 的元数据，以及配置与其他 Unit 的关系。它的主要字段如下。Description：简短描述 Documentation：文档地址 Requires：当前 Unit 依赖的其他 Unit，如果它们没有运行，当前 Unit 会启动失败 Wants：与当前 Unit 配合的其他 Unit，如果它们没有运行，当前 Unit 不会启动失败 BindsTo：与Requires类似，它指定的 Unit 如果退出，会导致当前 Unit 停止运行 Before：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之后启动 After：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之前启动 Conflicts：这里指定的 Unit 不能与当前 Unit 同时运行 Condition...：当前 Unit 运行必须满足的条件，否则不会运行 Assert...：当前 Unit 运行必须满足的条件，否则会报启动失败 [Service]区块[Service]区块用来定义如何启动当前服务，只有 Service 类型的 Unit 才有这个区块。它的主要字段如下。Type：定义启动时的进程行为。它有以下几种值。 Type=simple：默认值，执行ExecStart指定的命令，启动主进程 Type=forking：以 fork 方式从父进程创建子进程，创建后父进程会立即退出 Type=oneshot：一次性进程，Systemd 会等当前服务退出，再继续往下执行 Type=dbus：当前服务通过D-Bus启动 Type=notify：当前服务启动完毕，会通知Systemd，再继续往下执行 Type=idle：若有其他任务执行完毕，当前服务才会运行 ExecStart：启动当前服务的命令 ExecStartPre：启动当前服务之前执行的命令 ExecStartPost：启动当前服务之后执行的命令 ExecReload：重启当前服务时执行的命令 ExecStop：停止当前服务时执行的命令 ExecStopPost：停止当其服务之后执行的命令 RestartSec：自动重启当前服务间隔的秒数 Restart：定义何种情况 Systemd 会自动重启当前服务，可能的值包括always（总是重启）、on-success、on-failure、on-abnormal、on-abort、on-watchdog TimeoutSec：定义 Systemd 停止当前服务之前等待的秒数 Environment：指定环境变量 [Install]区块[Install]通常是配置文件的最后一个区块，用来定义如何启动，以及是否开机启动。它的主要字段如下。WantedBy：它的值是一个或多个 Target，当前 Unit 激活时（enable）符号链接会放入/etc/systemd/system目录下面以 Target 名 +.wants后缀构成的子目录中 RequiredBy：它的值是一个或多个 Target，当前 Unit 激活时，符号链接会放入/etc/systemd/system目录下面以 Target 名 + .required后缀构成的子目录中 Alias：当前 Unit 可用于启动的别名 Also：当前 Unit 激活（enable）时，会被同时激活的其他 Unit 2. Unit管理3.2.1 systemctl enable命令用于在上面两个目录之间，建立符号链接关系。如果配置文件里面设置了开机启动，systemctl enable命令相当于激活开机启动。$ systemctl enable clamd@scan.service # 等同于 $ ln -s '/usr/lib/systemd/system/clamd@scan.service' '/etc/systemd/system/multi-user.target.wants/clamd@scan.service' 3.2.2 systemctl disable用于在两个目录之间，撤销符号链接关系，相当于撤销开机启动。systemctl disable clamd@scan.service 3.2.3 systemctl list-unit-filessystemctl list-unit-files会显示每个配置文件的状态。而每个配置文件的状态，一共有四种enabled：已建立启动链接 disabled：没建立启动链接 static：该配置文件没有[Install]部分（无法执行），只能作为其他配置文件的依赖 masked：该配置文件被禁止建立启动链接 # 列出所有配置文件 $ systemctl list-unit-files # 列出指定类型的配置文件 $ systemctl list-unit-files --type=service 3.2.4 systemctl list-units查看当前系统的所有 Unit# 列出正在运行的 Unit $ systemctl list-units # 列出所有Unit，包括没有找到配置文件的或者启动失败的 $ systemctl list-units --all # 列出所有没有运行的 Unit $ systemctl list-units --all --state=inactive # 列出所有加载失败的 Unit $ systemctl list-units --failed # 列出所有正在运行的、类型为 service 的 Unit $ systemctl list-units --type=service 3.2.5 systemctl statussystemctl status命令用于查看系统状态和单个 Unit 的状态# 显示系统状态 $ systemctl status # 显示单个 Unit 的状态 $ sysystemctl status httpd httpd.service - The Apache HTTP Server Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled) Active: active (running) since 金 2014-12-05 12:18:22 JST; 7min ago Main PID: 4349 (httpd) Status: \"Total requests: 1; Current requests/sec: 0; Current traffic: 0 B/sec\" CGroup: /system.slice/httpd.service ├─4349 /usr/sbin/httpd -DFOREGROUND ├─4350 /usr/sbin/httpd -DFOREGROUND ├─4351 /usr/sbin/httpd -DFOREGROUND ├─4352 /usr/sbin/httpd -DFOREGROUND ├─4353 /usr/sbin/httpd -DFOREGROUND └─4354 /usr/sbin/httpd -DFOREGROUND 12月 05 12:18:22 localhost.localdomain systemd[1]: Starting The Apache HTTP Server... 12月 05 12:18:22 localhost.localdomain systemd[1]: Started The Apache HTTP Server. 12月 05 12:22:40 localhost.localdomain systemd[1]: Started The Apache HTTP Server. # 显示远程主机的某个 Unit 的状态 $ systemctl -H root@rhel7.example.com status httpd.service 3.2.6 查询Unit状态# 显示某个 Unit 是否正在运行 $ systemctl is-active application.service # 显示某个 Unit 是否处于启动失败状态 $ systemctl is-failed application.service # 显示某个 Unit 服务是否建立了启动链接 $ systemctl is-enabled application.service 3.2.7 Unit状态管理# 立即启动一个服务 $ systemctl start apache.service # 立即停止一个服务 $ systemctl stop apache.service # 重启一个服务 $ systemctl restart apache.service # 杀死一个服务的所有子进程 $ systemctl kill apache.service # 重新加载一个服务的配置文件 $ systemctl reload apache.service # 重载所有修改过的配置文件 $ systemctl daemon-reload # 显示某个 Unit 的所有底层参数 $ systemctl show httpd.service # 显示某个 Unit 的指定属性的值 $ systemctl show -p CPUShares httpd.service # 设置某个 Unit 的指定属性 $ systemctl set-property httpd.service CPUShares=500 3.2.8 查询Unit间的依赖关系Unit 之间存在依赖关系：A 依赖于 B，就意味着 Systemd 在启动 A 的时候，同时会去启动 B。# 列出一个 Unit 的所有依赖。 $ systemctl list-dependencies nginx.service # 上面命令的输出结果之中，有些依赖是 Target 类型（详见下文），默认不会展开显示。如果要展开 Target，就需要使用--all参数。 $ systemctl list-dependencies --all nginx.service 四、Unit的日志管理Systemd 统一管理所有 Unit 的启动日志。带来的好处就是，可以只用journalctl一个命令，查看所有日志（内核日志和应用日志）。日志的配置文件是/etc/systemd/journald.conf。journalctl功能强大，用法非常多。# 查看所有日志（默认情况下 ，只保存本次启动的日志） $ journalctl # 查看内核日志（不显示应用日志） $ journalctl -k # 查看系统本次启动的日志 $ journalctl -b $ journalctl -b -0 # 查看上一次启动的日志（需更改设置） $ journalctl -b -1 # 查看指定时间的日志 $ journalctl --since=\"2012-10-30 18:17:16\" $ journalctl --since \"20 min ago\" $ journalctl --since yesterday $ journalctl --since \"2015-01-10\" --until \"2015-01-11 03:00\" $ journalctl --since 09:00 --until \"1 hour ago\" # 显示尾部的最新10行日志 $ journalctl -n # 显示尾部指定行数的日志 $ journalctl -n 20 # 实时滚动显示最新日志 $ journalctl -f # 查看指定服务的日志 $ journalctl /usr/lib/systemd/systemd # 查看指定进程的日志 $ journalctl _PID=1 # 查看某个路径的脚本的日志 $ journalctl /usr/bin/bash # 查看指定用户的日志 $ journalctl _UID=33 --since today # 查看某个 Unit 的日志 $ journalctl -u nginx.service $ journalctl -u nginx.service --since today # 实时滚动显示某个 Unit 的最新日志 $ journalctl -u nginx.service -f # 合并显示多个 Unit 的日志 $ journalctl -u nginx.service -u php-fpm.service --since today # 查看指定优先级（及其以上级别）的日志，共有8级 # 0: emerg # 1: alert # 2: crit # 3: err # 4: warning # 5: notice # 6: info # 7: debug $ journalctl -p err -b # 日志默认分页输出，--no-pager 改为正常的标准输出 $ journalctl --no-pager # 以 JSON 格式（单行）输出 $ journalctl -b -u nginx.service -o json # 以 JSON 格式（多行）输出，可读性更好 $ journalctl -b -u nginx.serviceqq -o json-pretty # 显示日志占据的硬盘空间 $ journalctl --disk-usage # 指定日志文件占据的最大空间 $ journalctl --vacuum-size=1G # 指定日志文件保存多久 $ journalctl --vacuum-time=1years 五、Target启动计算机的时候，需要启动大量的 Unit。如果每一次启动，都要一一写明本次启动需要哪些 Unit，显然非常不方便。Systemd 的解决方案就是 Target。简单说，Target 就是一个 Unit 组，包含许多相关的 Unit 。启动某个 Target 的时候，Systemd 就会启动里面所有的 Unit。从这个意义上说，Target 这个概念类似于\"状态点\"，启动某个 Target 就好比启动到某种状态。传统的init启动模式里面，有 RunLevel 的概念，跟 Target 的作用很类似。不同的是，RunLevel 是互斥的，不可能多个 RunLevel 同时启动，但是多个 Target 可以同时启动。1. 与传统 RunLevel 的对应关系如下: Traditional runlevel New target name --> Symbolically linked to Runlevel 0 runlevel0.target -> poweroff.target Runlevel 1 runlevel1.target -> rescue.target Runlevel 2 runlevel2.target -> multi-user.target Runlevel 3 runlevel3.target -> multi-user.target Runlevel 4 runlevel4.target -> multi-user.target Runlevel 5 runlevel5.target -> graphical.target Runlevel 6 runlevel6.target -> reboot.target 2. 与init进程的主要差别如下:默认的 RunLevel（在/etc/inittab文件设置）现在被默认的 Target 取代，位置是/etc/systemd/system/default.target，通常符号链接到graphical.target（图形界面）或者multi-user.target（多用户命令行）。 启动脚本的位置，以前是/etc/init.d目录，符号链接到不同的 RunLevel 目录 （比如/etc/rc3.d、/etc/rc5.d等），现在则存放在/lib/systemd/system和/etc/systemd/system目录。 3. Target管理# 查看当前系统的所有 Target $ systemctl list-unit-files --type=target # 查看一个 Target 包含的所有 Unit $ systemctl list-dependencies multi-user.target # 查看启动时的默认 Target $ systemctl get-default # 设置启动时的默认 Target $ sudo systemctl set-default multi-user.target # 切换 Target 时，默认不关闭前一个 Target 启动的进程， # systemctl isolate 命令改变这种行为， # 关闭前一个 Target 里面所有不属于后一个 Target 的进程 $ systemctl isolate multi-user.target 六、常见服务的Unit配置1. Zookeeperbash -c ' cat > /usr/lib/systemd/system/zookeeper.service 2. Kafkabash -c ' cat > /usr/lib/systemd/system/kafka.service 3. Tomcatbash -c ' cat > /usr/lib/systemd/system/tomcat.service 参考http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-part-two.html https://blog.51cto.com/andyxu/2122109?source=dra Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-16 17:20:54 "},"origin/正反向代理服务的区别.html":{"url":"origin/正反向代理服务的区别.html","title":"代理服务器","keywords":"","body":"正反向代理服务的区别一、代理的作用首先要明确代理服务器的作用:代理：可以直白地理解为：代理服务器是一种代替谁去访问什么的服务器。代理客户端浏览器去访问客户端浏览器访问不了的服务，代理应用服务器负载均衡地对外提供服务。可以根据代替谁来划分为\"正向代理\"和\"反向代理\" 缓存加速：缓存那些不经常变动的资源，加速访问。 鉴权过滤记录：允许那些认证过的客户端去访问指定的资源或服务，还可以记录下访问记录。 二、正向代理正向代理（forward proxy）是指代替内部网络的客户端，去访问Internet或其他网络上的服务，并将访问的结果返还给客户端，同时将结果缓存下来，加速访问。正向代理还可以按客户端是否感知分为透明代理与传统代理。常见的正向代理服务软件有:Squid Varnish Nginx 三、反向代理反向代理（Reverse Proxy）方式是指以代理服务器来接受Internet上的连接请求，然后将请求转发给内部网络上的服务器；并将从服务器上得到的结果返回给Internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。常见的反向代理服务软件有:Nginx Apache HAProxy 未完代待整理更新Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:13 "},"origin/常见正向代理服务软件之间的区别.html":{"url":"origin/常见正向代理服务软件之间的区别.html","title":"正向代理","keywords":"","body":"常见正向代理服务软件的对比区别(挖坑)https://www.zhihu.com/search?type=content&q=%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E6%A6%82%E5%BF%B5Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:13 "},"origin/squid-简介安装.html":{"url":"origin/squid-简介安装.html","title":"简介安装日志","keywords":"","body":"正向代理服务Squid一、简介Squid是一个高性能的正向代理缓存服务器，支持FTP、gopher、HTTPS、HTTP等协议，主要提供了缓存加速、应用层过滤的功能。和一般的代理缓存软件不同，Squid用一个单独的、非模块化的、I/O驱动的进程来处理所有的客户端请求。squid代理服务器的工作机制：代理服务器（Proxy Server）是个人网络和Internet服务商之间的中间代理机构，负责转发合法的网络信息，对转发进行控制和登记。其最基本的功能就是连接，此外还包括安全性、缓存，内容过滤，访问控制管理等功能。当客户机通过代理请求Web页面时，执行的代理服务器会先检查自己的缓存，当缓存中有客户机需要访问的页面，则直接将缓存服务器中的页面内容反馈给客户机；如果缓存中没有客户机需要访问的页面，则由代理服务器想Internet发送访问请求，当获得返回的Web页面以后，将页面数据保存到缓存中并发送给客户机。 由于客户机的web访问请求实际上代理服务器来代替完成的，所以隐藏了用户的真实IP地址，从而起到一定的保护作用。Squid可以基于访问控制列表（ACL）和访问权限列表（ARL）执行内容过滤与权限管理功能，还可以基于多种条件禁止用户访问存在威胁或不适宜的网站资源。根据实现的方式不同，正向代理模式可以分为：传统代理：也就是普通的代理服务，需要我们客户端在浏览器、聊天工具等一些程序中设置代理服务器的地址和端口，然后才能使用代理来访问网络，这种方式相比较而言比较麻烦，因为客户机还需手动指定代理服务器，所以一般用于Internet环境。 透明代理：与传统代理实现的功能是一样的，区别在于客户机不需要手动指定代理服务器的地址和端口，而是通过默认路由、防火墙策略将web访问重定向，实际上仍然交给代理服务器来处理，重定向的过程完全是由squid服务器进行的，所以对于客户机来说，甚至不知道自己使用了squid代理服务，因此呢，我们称之为透明模式。透明代理多用于局域网环境，如在Linux网关中启用透明代理后，局域网主机无须进行额外设置就能享受更好的上网速度。 二、安装YUMyum install squid -y; \\ systemctl enable squid; \\ systemctl start squid 三、传统代理服务配置配置文件/etc/squid/squid.confacl localnet src 10.0.0.0/8 # RFC1918 possible internal network acl localnet src 172.16.0.0/12 # RFC1918 possible internal network acl localnet src 192.168.0.0/16 # RFC1918 possible internal network acl localnet src fc00::/7 # RFC 4193 local private network range acl localnet src fe80::/10 # RFC 4291 link-local (directly plugged) machines acl SSL_ports port 443 acl Safe_ports port 80 # http acl Safe_ports port 21 # ftp acl Safe_ports port 443 # https acl Safe_ports port 70 # gopher acl Safe_ports port 210 # wais acl Safe_ports port 1025-65535 # unregistered ports acl Safe_ports port 280 # http-mgmt acl Safe_ports port 488 # gss-http acl Safe_ports port 591 # filemaker acl Safe_ports port 777 # multiling http acl CONNECT method CONNECT http_access allow Safe_ports http_access deny CONNECT !SSL_ports http_access allow localhost manager http_access deny manager http_access allow localnet http_access allow localhost http_access deny all http_port 3128 # Squid代理服务监听端口 cache_dir ufs /data/squid 100 16 256 coredump_dir /data/squid refresh_pattern ^ftp: 1440 20% 10080 refresh_pattern ^gopher: 1440 0% 1440 refresh_pattern -i (/cgi-bin/|\\?) 0 0% 0 refresh_pattern . 0 20% 4320 四、常用命令1、启动等命令squid reload #不重启服务，生效配置 squid –z #初始化缓存空间 初始化你在 squid.conf 里配置的 cache 目录,只需要第一次的时候执行就可以了 squid -k parse #验证squid.conf的语法和配置 squid -N -d1 #在前台启动squid，并输出启动过程 squid -s #后台运行squid。 squid -k shutdown #停止 squid squid -k reconfigure #载入新的配置文件 squid -k rotate #轮循日志 2、squid命中率分析# 获取squid运行状态信息 squidclient -p 3128 mgr:info squidclient -p 3128 mgr:5min # 可以看到详细的性能情况,其中PORT是你的proxy的端口，5min可以是60min #获取squid内存使用情况 squidclient -p 3128 mgr:mem #获取squid已经缓存的列表 squidclient -p 3128 mgr:objects use it carefully,it may crash #获取squid的磁盘使用情况 squidclient -p 3128 mgr:diskd #强制更新某个url squidclient -p 3128 -m PURGE http://www.xxx.com/xxx.php #更多的请查看 squidclient -h 或者 squidclient -p 3128 mgr: #查命中率： squidclient -h(具体侦听IP) -p80(具体侦听端口) mgr:info 3、定期清除swap.state内无效数据当squid应用运行了一段时间以后，cache_dir对应的swap.state文件就会变得越来越大，里面的无效接口数据越来越多，这可能影响squid的响应时间，因此需要使用rotate命令来使squid清理swap.state里面的无效数据，减少swap.state的大小squid -k rotate -f /path/to/squid/conf_file #添加定时清理任务 vi /etc/crontab 0 0 * * * root /usr/local/sbin/squid -k rotate -f /usr/local/etc/squid/squid1.conf 4、统计客户端个数netstat -lanp|grep 3128|grep \"ESTABLISHED\"|awk '{print $5}'|awk -F':' '{print $1}'|sort -u|wc -l 5、统计客户端的连接总数netstat -lanp|grep 3128|grep \"ESTABLISHED\"|wc -l 6、显示传输数据大于指定大小的访问 tailf /var/log/squid/access.log | awk '{if($5>1000)print}'|awk '{print $3 \" \" $5 \" \" $7}' 五、日志默认输出格式squid日志配置项是在/etc/squid/squid.conf中配置的，默认日志输出文件路径/var/log/squid/access.log默认的日志输出格式#1:时间戳 2:响应时间 3:客户端IP 4:结果/状态码 5:传输大小 6:请求方式 7:客户端请求的URL 8:客户端身份 9:对端编码/对端主机 10:内容类型 1531077064.951 81 10.248.2.67 TCP_MISS/200 6277 GET http://bbs.talkop.com/forum.php? - HIER_DIRECT/180.76.184.69 text/xml 时间戳（%tl %ts）: 请求完成时间，以 Unix 时间来记录的（UTC 1970-01-01 00:00:00 开始的时间）它是毫秒级的。squid使用这种格式而不是人工可读的时间格式，是为了简化某些日志处理程序的工作 响应时间（%6tr）: 对HTTP响应来说，该域表明squid花了多少时间来处理请求。在squid接收到HTTP请求时开始计时，在响应完全送出后计时终止。响应时间是毫秒级的。尽管时间值是毫秒级的，但是精度可能是10毫秒。在squid负载繁重时，计时变得没那么精确 客户端地址（%>a）: 该域包含客户端的IP地址，或者是主机名 结果/状态码（%Ss/%03Hs）: 该域包含2个 token，以斜杠分隔。第一个token叫结果码，它把协议和响应结果（例如TCPHIT或UDP_DENIED）进行归类。这些是squid专有的编码，以TCP开头的编码指HTTP请求，以UDP_开头的编码指ICP查询。第2个token是HTTP响应状态码（例如200,304,404等）。状态码通常来自原始服务器。在某些情形下，squid可能有义务自己选择状态码 传输size（%: 该域指明传给客户端的字节数。严格的讲，它是squid告诉TCP/IP协议栈去发送给客户端的字节数。这就是说，它不包括TCP/IP头部的overhead。也请注意，传输size正常来说大于响应的Content-Length。传输size包括了HTTP响应头部，然而Content- Length不包括 请求方式（%rm）: 该域包含请求方式 URI（%ru）: 该域包含来自客户端请求的URI。大多数记录下来的URI实际是URL（例如，它们有主机名）。在记日志时，squid删掉了在第一个问号(?)之后的所有URI字符，除非禁用了strip_query_terms指令 客户端身份: 无 对端编码/对端主机: 对端信息包含了2个token，以斜杠分隔。它仅仅与cache 不命中的请求有关。第一个token指示如何选择下一跳，第二个token是下一跳的地址。当squid发送一个请求到邻居cache时，对端主机地址是邻居的主机名。假如请求是直接送到原始服务器的，则squid会写成原始服务器的IP地址或主机名–假如禁用了log_ip_on_direct。NONE/-这个值指明squid不转发该请求到任何其他服务器 内容类型（%mt）: 原始access.log的默认的最后一个域，是HTTP响应的内容类型。 squid从响应的Content-Type头部获取内容类型值。假如该头部丢失了，squid使用一个横杠(-)代替 假如激活了 log_mime_hdrs 指令，squid在每行追加2个附加的域：HTTP请求头部: Squid 编码HTTP请求头部，并且在一对方括号之间打印它们。方括号是必须的，因为squid不编码空格字符。编码方案稍许奇怪。回车（ASCII 13）和换行（ASCII 10）分别打印成\\r和\\n。其他不可打印的字符以RFC 1738风格来编码，例如Tab（ASCII 9）变成了%09。 HTTP响应头部: Squid编码HTTP响应头部，并且在一对方括号之间打印它们。注意这些是发往客户端的头部，可能不同于从原始服务器接收到的头部。 参考链接http://www.squid-cache.org/ https://blog.51cto.com/10693404/2149207 https://blog.51cto.com/14154700/2406060 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-12 22:47:23 "},"origin/squid-acl访问权限控制.html":{"url":"origin/squid-acl访问权限控制.html","title":"ACL访问权限","keywords":"","body":"一、ACL概念Squid提供了强大的代理控制机制，通过合理设置ACL（Access Control List，访问控制列表）并进行限制，可以针对源地址、目标地址、访问的URL路径、访问的时间等各种条件进行过滤。ACL访问控制的步骤：使用acl配置项定义需要控制的条件 通过http_access配置项对已定义的列表做“允许”或“拒绝”访问的控制 二、ACL用法概述1、定义ACL访问列表定义格式：acl aclname acltype string1… #acl 列表名称 列表类型 列表内容 ... acl aclname acltype \"File_Path\"… #acl 列表名称 列表类型 \"文件路径\" ... #当使用文件时，该文件的格式为每行包含一个条目。 常用的ACL列表类型:src：指明源地址acl aclname src ip-address/netmask ... 客户ip地址 acl aclname src addr1-addr2/netmask ... 地址范围 dst：指明目标地址，即客户请求的服务器的IP地址。语法为：acl aclname dst ip-address/netmask ... srcdomain：指明客户所属的域，Squid将根据客户IP反向查询DNS。语法为：acl aclname srcdomain foo.com ... dstdomain：指明请求服务器所属的域，由客户请求的URL决定。语法为：acl aclname dstdomain foo.com ... 此处需要注意的是：如果用户使用服务器IP而非完整的域名时，Squid将进行反向的DNS解析来确定其完整域名，如果失败，就记录为“none”。 time：指明访问时间。语法如下：acl aclname time [day-abbrevs] [h1:m1-h2:m2][hh:mm-hh:mm] 日期的缩写指代关系如下： S：指代Sunday M：指代Monday T：指代Tuesday W：指代Wednesday H：指代Thursday F：指代Friday A：指代Saturday 另外，h1：m1必须小于h2：m2，表达式为[hh：mm-hh：mm]。 port：指定访问端acl aclname port 80 70 21 ... acl aclname port 0-1024 ... 指定一个端口范围 method：指定请求方法。比如：acl aclname method GET POST ... url_regex：URL规则表达式匹配，语法为：acl aclname url_regex[-i] pattern urlpath_regex：URL-path规则表达式匹配，略去协议和主机名。其语法为：acl aclname urlpath_regex[-i] pattern Notes：acltype可以是任一个在ACL中定义的名称。 任何两个ACL元素不能用相同的名字。 每个ACL由列表值组成。当进行匹配检测的时候，多个值由逻辑或运算连接；换句话说，任一ACL元素的值被匹配，则这个ACL元素即被匹配。 并不是所有的ACL元素都能使用访问列表中的全部类型。 不同的ACL元素写在不同行中，Squid将这些元素组合在一个列表中。 2、http_access访问控制列表使用访问控制根据访问控制列表允许或禁止某一类用户访问。如果某个访问没有相符合的项目，则默认为应用最后一条项目的“非”。比如最后一条为允许，则默认就是禁止。通常应该把最后的条目设为“deny all”或“allow all”来避免安全性隐患。使用该访问控制列表要注意如下问题：这些规则按照它们的排列顺序进行匹配检测，一旦检测到匹配的规则，匹配检测就立即结束。 访问列表可以由多条规则组成。 如果没有任何规则与访问请求匹配，默认动作将与列表中最后一条规则对应。 一个访问条目中的所有元素将用逻辑与运算连接（如下所示）： http_access Action声明1 AND 声明2 AND 多个http_access声明间用或运算连接，但每个访问条目的元素间用与运算连接。 列表中的规则总是遵循由上而下的顺序。 三、ACL示例允许网段10.0.0.124/24以及192.168.10.15/24内的所有客户机访问代理服务器，并且允许在文件/etc/squid/guest列出的客户机访问代理服务器，除此之外的客户机将拒绝访问本地代理服务器： acl clients src 10.0.0.124/24 192.168.10.15/24 acl guests src “/etc/squid/guest” acl all src 0.0.0.0/0.0.0.0 http_access allow clients http_access allow guests http_access deny all 其中，文件“/etc/squid/guest”中的内容为： 172.168.10.3/24 210.113.24.8/16 10.0.1.24/25 允许域名为job.net、gdfq.edu.cn的两个域访问本地代理服务器，其他的域都将拒绝访问本地代理服务器： acl permitted_domain src job.net gdfq.edu.cn acl all src 0.0.0.0/0.0.0.0 http_access allow permitted_domain http_access deny all 使用正则表达式，拒绝客户机通过代理服务器访问包含有诸如“sexy”等关键字的网站： acl deny_url url_regex -i sexy http_access deny deny_url 拒绝客户机通过代理服务器访问文件中指定IP或者域名的网站，其中文件/etc/squid/ deny_ip中存放有拒绝访问的IP地址，文件/etc/squid/deny_dns中存放有拒绝访问的域名： acl deny_ip dst “etc/squid/deny_ip” acl deny_dns dst “etc/squid/deny_dns” http_access deny deny_ip http_access deny deny_dns 允许和拒绝指定的用户访问指定的网站，其中，允许客户1访问网站http://www.sina.com.cn，而拒绝客户2访问网站http://www.163.com： acl client1 src 192.168.0.118 acl client1_url url_regex ^http://www.sina.com.cn acl client2 src 192.168.0.119 acl client2_url url_regex ^http://www.163.com http_access allow client1 client1_url http_access deny client2 client2_url 允许所有的用户在规定的时间内（周一至周四的8：30到20：30）访问代理服务器，只允许特定的用户（系统管理员，其网段为：192.168.10.0/24）在周五下午访问代理服务器，其他的在周五下午一点至六点一律拒绝访问代理服务器： acl allclient src 0.0.0.0/0.0.0.0 acl administrator 192.168.10.0/24 acl common_time time MTWH 8:30-20:30 acl manage_time time F 13:00-18:00 http_access allow allclient common_time http_access allow administrator manage_time http_access deny manage_time Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:12 "},"origin/常见反向代理服务软件之间的区别.html":{"url":"origin/常见反向代理服务软件之间的区别.html","title":"反向代理","keywords":"","body":"常见反向代理服务软件的对比区别(挖坑)Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:13 "},"origin/nginx.html":{"url":"origin/nginx.html","title":"Nginx","keywords":"","body":"Nginx一、简介官网：http://nginx.org/nginx是一款自由的、开源的、高性能的HTTP服务器和反向代理服务器；同时也是一个IMAP、POP3、SMTP代理服务器；nginx可以作为一个HTTP服务器进行网站的发布处理，另外nginx可以作为反向代理进行负载均衡的实现。反向代理 负载均衡 二、安装1. 二进制RPM安装以RPM方式安装的配置文件在/etc/nginx/目录下 二进制安装自带的模块 二进制安装(例如YUM)的nginx不支持动态的安装和新加载模块的，新增模块需要重新编译安装了nginx #To set up the yum repository for RHEL/CentOS, create the file named /etc/yum.repos.d/nginx.repo with the following contents: [nginx] name=nginx repo baseurl=http://nginx.org/packages/OS/OSRELEASE/$basearch/ gpgcheck=0 enabled=1 #Replace “OS” with “rhel” or “centos”, depending on the distribution used, and “OSRELEASE” with “6” or “7”, for 6.x or 7.x versions, respectively. $ bash -c 'cat > /etc/yum.repos.d/nginx.repo 2. 源码编译安装指定模块①安装编译工具yum install -y gcc gc++ perl gcc-c++ ②安装编译必备库the PCRE library – required by NGINX Core and Rewrite modules and provides support for regular expressionspcre是一个正则库，nginx使用正则进行重写要用到，必须安装 pcre库有两个版本：pcre、pcre2(新版的库)。推荐下载pcre，pcre2是编译是通不过的。 编译pcre就必须用到c++编译器，使用pcre2就使用gcc编译器。 # yum安装 $ rpm -qa pcre pcre-devel $ yum install pcre pcre-devel # 源码编译安装 $ version=8.43 && \\ wget ftp://ftp.pcre.org/pub/pcre/pcre-$version.tar.gz && \\ tar -zxf pcre-$version.tar.gz&& \\ cd pcre-$version && \\ ./configure && \\ make && \\ make install the zlib library – required by NGINX Gzip module for headers compression:# yum安装 $ rpm -qa zlib zlib-devel $ yum install zlib zlib-devel # 源码编译安装 $ version=1.2.11 && \\ wget http://zlib.net/zlib-$version.tar.gz && \\ tar -zxf zlib-$version.tar.gz && \\ cd zlib-$version && \\ ./configure && \\ make && \\ make install the OpenSSL library – required by NGINX SSL modules to support the HTTPS protocolOpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及 SSL 协议，并提供丰富的应用程序供测试或其它目的使用。nginx 不仅支持 http 协议，还支持 https（即在ssl协议上传输http），所以需要在 Centos 安装 OpenSSL 库# yum安装 $ rpm -qa openssl openssl-devel $ yum install openssl openssl-devel # 源码编译安装 $ version=1.0.2t && \\ wget http://www.openssl.org/source/openssl-$version.tar.gz && \\ tar -zxf openssl-$version.tar.gz && \\ cd openssl-$version && \\ ./config && \\ make && \\ make install ③下载解压Nginx源码包$ version=1.17.6 && \\ wget http://nginx.org/download/nginx-$version.tar.gz && \\ tar -zxf nginx-*.tar.gz && \\ cd nginx-$version ④配置编译参数创建nginx用户----->创建相关目录------>配置编译参数$ ./configure --help #查看编译配置参数 --with开头的，默认是禁用的(没启动的，想使用的话需要在编译的时候加上) --without开头的，默认是启用的(不想启用此模块时，可以在编译的时候加上这个参数) # --help print this message # --prefix=PATH 指定安装目录 # --sbin-path=PATH 指定二进制执行程序文件存放位置。 # --modules-path=PATH 指定第三方模块的存放路径 # --conf-path=PATH 指定配置文件nginx.conf存放位置 # --error-log-path=PATH 指定错误日志存放位置 # --pid-path=PATH 指定nginx.pid文件存放位置 # --lock-path=PATH 指定nginx.lock文件存放位置 # --user=USER 指定程序运行时的非特权用户 # --group=GROUP 指定程序运行时的非特权用户组 # --build=NAME set build name # --builddir=DIR 指定编译目录 # --with-select_module enable select module # --without-select_module disable select module # --with-poll_module enable poll module # --without-poll_module disable poll module # --with-threads enable thread pool support # --with-file-aio enable file AIO support # --with-http_ssl_module enable ngx_http_ssl_module # --with-http_v2_module enable ngx_http_v2_module # --with-http_realip_module enable ngx_http_realip_module # --with-http_addition_module enable ngx_http_addition_module # --with-http_xslt_module enable ngx_http_xslt_module # --with-http_xslt_module=dynamic enable dynamic ngx_http_xslt_module # --with-http_image_filter_module enable ngx_http_image_filter_module # --with-http_image_filter_module=dynamic enable dynamic ngx_http_image_filter_module # --with-http_geoip_module enable ngx_http_geoip_module # --with-http_geoip_module=dynamic enable dynamic ngx_http_geoip_module # --with-http_sub_module enable ngx_http_sub_module # --with-http_dav_module enable ngx_http_dav_module # --with-http_flv_module enable ngx_http_flv_module # --with-http_mp4_module enable ngx_http_mp4_module # --with-http_gunzip_module enable ngx_http_gunzip_module # --with-http_gzip_static_module enable ngx_http_gzip_static_module # --with-http_auth_request_module enable ngx_http_auth_request_module # --with-http_random_index_module enable ngx_http_random_index_module # --with-http_secure_link_module enable ngx_http_secure_link_module # --with-http_degradation_module enable ngx_http_degradation_module # --with-http_slice_module enable ngx_http_slice_module # --with-http_stub_status_module enable ngx_http_stub_status_module # --without-http_charset_module disable ngx_http_charset_module # --without-http_gzip_module disable ngx_http_gzip_module # --without-http_ssi_module disable ngx_http_ssi_module # --without-http_userid_module disable ngx_http_userid_module # --without-http_access_module disable ngx_http_access_module # --without-http_auth_basic_module disable ngx_http_auth_basic_module # --without-http_autoindex_module disable ngx_http_autoindex_module # --without-http_geo_module disable ngx_http_geo_module # --without-http_map_module disable ngx_http_map_module # --without-http_split_clients_module disable ngx_http_split_clients_module # --without-http_referer_module disable ngx_http_referer_module # --without-http_rewrite_module disable ngx_http_rewrite_module # --without-http_proxy_module disable ngx_http_proxy_module # --without-http_fastcgi_module disable ngx_http_fastcgi_module # --without-http_uwsgi_module disable ngx_http_uwsgi_module # --without-http_scgi_module disable ngx_http_scgi_module # --without-http_memcached_module disable ngx_http_memcached_module # --without-http_limit_conn_module disable ngx_http_limit_conn_module # --without-http_limit_req_module disable ngx_http_limit_req_module # --without-http_empty_gif_module disable ngx_http_empty_gif_module # --without-http_browser_module disable ngx_http_browser_module # --without-http_upstream_hash_module disable ngx_http_upstream_hash_module # --without-http_upstream_ip_hash_module disable ngx_http_upstream_ip_hash_module # --without-http_upstream_least_conn_module disable ngx_http_upstream_least_conn_module # --without-http_upstream_keepalive_module disable ngx_http_upstream_keepalive_module # --without-http_upstream_zone_module disable ngx_http_upstream_zone_module # --with-http_perl_module enable ngx_http_perl_module # --with-http_perl_module=dynamic enable dynamic ngx_http_perl_module # --with-perl_modules_path=PATH set Perl modules path # --with-perl=PATH set perl binary pathname # --http-log-path=PATH set http access log pathname # --http-client-body-temp-path=PATH set path to store http client request body temporary files # --http-proxy-temp-path=PATH set path to store http proxy temporary files # --http-fastcgi-temp-path=PATH set path to store http fastcgi temporary files # --http-uwsgi-temp-path=PATH set path to store http uwsgi temporary files # --http-scgi-temp-path=PATH set path to store http scgi temporary files # --without-http disable HTTP server # --without-http-cache disable HTTP cache # --with-mail enable POP3/IMAP4/SMTP proxy module # --with-mail=dynamic enable dynamic POP3/IMAP4/SMTP proxy module # --with-mail_ssl_module enable ngx_mail_ssl_module # --without-mail_pop3_module disable ngx_mail_pop3_module # --without-mail_imap_module disable ngx_mail_imap_module # --without-mail_smtp_module disable ngx_mail_smtp_module # --with-stream enable TCP/UDP proxy module # --with-stream=dynamic enable dynamic TCP/UDP proxy module # --with-stream_ssl_module enable ngx_stream_ssl_module # --with-stream_realip_module enable ngx_stream_realip_module # --with-stream_geoip_module enable ngx_stream_geoip_module # --with-stream_geoip_module=dynamic enable dynamic ngx_stream_geoip_module # --with-stream_ssl_preread_module enable ngx_stream_ssl_preread_module # --without-stream_limit_conn_module disable ngx_stream_limit_conn_module # --without-stream_access_module disable ngx_stream_access_module # --without-stream_geo_module disable ngx_stream_geo_module # --without-stream_map_module disable ngx_stream_map_module # --without-stream_split_clients_module disable ngx_stream_split_clients_module # --without-stream_return_module disable ngx_stream_return_module # --without-stream_upstream_hash_module disable ngx_stream_upstream_hash_module # --without-stream_upstream_least_conn_module disable ngx_stream_upstream_least_conn_module # --without-stream_upstream_zone_module disable ngx_stream_upstream_zone_module # --with-google_perftools_module enable ngx_google_perftools_module # --with-cpp_test_module enable ngx_cpp_test_module # --add-module=PATH enable external module # --add-dynamic-module=PATH enable dynamic external module # --with-compat dynamic modules compatibility # --with-cc=PATH set C compiler pathname # --with-cpp=PATH set C preprocessor pathname # --with-cc-opt=OPTIONS set additional C compiler options # --with-ld-opt=OPTIONS set additional linker options # --with-cpu-opt=CPU build for the specified CPU, valid values:pentium, pentiumpro, pentium3, pentium4,athlon, opteron, sparc32, sparc64, ppc64 # --without-pcre disable PCRE library usage # --with-pcre force PCRE library usage # --with-pcre=DIR 设置pcre源码目录路径 # --with-pcre-opt=OPTIONS set additional build options for PCRE # --with-pcre-jit build PCRE with JIT compilation support # --with-zlib=DIR set path to zlib library sources # --with-zlib-opt=OPTIONS set additional build options for zlib # --with-zlib-asm=CPU use zlib assembler sources optimized for the specified CPU, valid values:pentium, pentiumpro # --with-libatomic force libatomic_ops library usage # --with-libatomic=DIR set path to libatomic_ops library sources # --with-openssl=DIR set path to OpenSSL library sources # --with-openssl-opt=OPTIONS set additional build options for OpenSSL # --with-debug enable debug logging $ groupadd nginx && \\ useradd nginx -s /sbin/nologin -M -g nginx && \\ mkdir -p /opt/nginx-1.17.6/logs $ ./configure \\ --prefix=/opt/nginx-1.17.6 \\ --user=nginx \\ --group=nginx \\ --sbin-path=/opt/nginx-1.17.6/nginx \\ --error-log-path=/opt/nginx-1.17.6/logs/error.log \\ --conf-path=/opt/nginx-1.17.6/nginx.conf \\ --pid-path=/opt/nginx-1.17.6/nginx.pid \\ --with-pcre \\ --with-openssl=/usr/lib64/openssl \\ --with-http_stub_status_module ⑤编译安装# make命令将源代码编译为二进制文件 $ make # 根据配置阶段指定的路径和功能将软件以特定的方式安装到指定位置 $ make install ⑥设置环境变量ln -s /opt/nginx-1.17.6/nginx /usr/bin/nginx 3. 启动手动控制Nginx的生命周期$ nginx -t #启动测试 $ nginx #启动 托管给Systemd$ bash -c 'cat > /usr/lib/systemd/system/nginx.service 4. 验证# 查看监听的端口 $ lsof -i :80 $ netstat -lanp |grep 80 # 使用命令行工具访问页面 $ curl 127.0.0.1 $ wget 127.0.0.1 # 查看进程 $ ps -ef | grep nginx # root 2564 1 0 23:21 ? 00:00:00 nginx: master process /opt/nginx-1.17.6/nginx # nginx 2565 2564 0 23:21 ? 00:00:00 nginx: worker process 三、Nginx目录结构编译安装的目录结构#由于编译时指定了相关路径 $ tree /opt/nginx-1.17.6 /opt/nginx-1.17.6 ├── 3party_module ├── client_body_temp ├── fastcgi.conf ├── fastcgi.conf.default ├── fastcgi_params ├── fastcgi_params.default ├── fastcgi_temp ├── html #站点目录 │ ├── 50x.html #错误页 │ └── index.html #首页 ├── koi-utf ├── koi-win ├── logs #日志目录 │ ├── access.log #nginx访问日志 │ └── error.log #Nginx的错误日志 ├── mime.types #媒体类型 ├── mime.types.default ├── nginx #Nginx的二进制启动命令脚本 ├── nginx.conf #Nginx的主要配置文件 ├── nginx.conf.default ├── nginx.pid #Nginx所有的进程号文件 ├── nginx-rtmp-module ├── proxy_temp #临时目录 ├── scgi_params ├── scgi_params.default ├── scgi_temp ├── uwsgi_params ├── uwsgi_params.default ├── uwsgi_temp └── win-utf 四、命令行参数$ nginx -s signal #Where signal may be one of the following: # stop — fast shutdown # quit — graceful shutdown # reload — 重新加载配置文件 # reopen — reopening the log files Nginx重新加载配置文件的过程：主进程接受到加载信号后： 1、首先会校验配置的语法，然后生效新的配置， 2、如果成功，则主进程会启动新的工作进程，同时发送终止信号给旧的工作进程。 3、否则主进程回退配置，继续工作。 在第二步，旧的工作进程收到终止信号后，会停止接收新的连接请求，知道所有现有的请求处理完，然后退出。 $ nginx -t #检查配置文件语法是否错误，并尝试启动 $ nginx -q # suppress non-error messages during configuration testing. $ nginx -T # same as -t, but additionally dump configuration files to standard output (1.9.2). $ nginx #启动Nginx $ nginx -v #查看nginx的版本 $ nginx -V #查看nginx的版本，编译器版本，编译时的参数等 $ nginx -p prefix # set nginx path prefix, i.e. a directory that will keep server files (default value is /usr/local/nginx). $ nginx -c file # 指定配置文件（不使用默认路径下的配置文件） $ nginx -? | -h # print help for command-line parameters. $ nginx -g directives # set global configuration directives, for example, #nginx -g \"pid /var/run/nginx.pid; worker_processes `sysctl -n hw.ncpu`;\" 五、Nginx模块六、示例配置文件七、问题0. Nginx添加模块并不停服升级不管Nginx是用YUM二进制还是源码编译方式安装的，后续如果有新需求是现有Nginx模块无法满足，需要添加新模块才能完成的情况时，都是要对Nginx进行重新编译安装，然后不停服，不能影响现有的业务地平滑升级 （该操作有风险，需在开发环境测试通过再在生产环境进行操作）① 查看现有的nginx编译参数nginx -V # 或者 /opt/nginx1.17.6/nginx -V ② 备份旧版本的nginx可执行文件期间nginx不会停止服务mv /opt/nginx1.17.6/nginx /opt/nginx1.17.6/nginx.bak ③ 安装编译必备组件④ 下载相同版本的nginx源码包⑤ 下载第三方模块⑥ 配置编译参数要加上原有的编译参数⑦ 编译新的Nginx只make, 不要make install，不然会覆盖原来已安装的nginx⑧ 替换Nginx文件⑨ 修改新配置文件， 并检查配置文件语法**⑩ 新配置的平滑升级$ kill -USR2 旧Nginx主进程号或进程文件路径 # 此时旧的Nginx主进程将会把自己的进程文件改名为.oldbin，然后执行新版Nginx。新旧Nginx会同时运行，共同处理请求。 这时要逐步停止旧版 Nginx $ kill -WINCH 旧Nginx主进程号 # 慢慢旧Nginx进程就都会随着任务执行完毕而退出，新的Nginx进程会逐渐取代旧进程。 1. 启动Nginx时报“nginx: [emerg] getpwnam(\"nginx\") failed”原因：nginx用户没有创建成功2. 浏览器，curl、wget等访问不了nginx页面原因：可能是没有关闭SELinux和防火墙 ，检查一下3. 访问资源403的问题排查通过yum安装的nginx一切正常，但是访问时报403，于是查看nginx日志，路径为/var/log/nginx/error.log。打开日志发现报错Permission denied，详细报错如下：open() \"/data/www/1.txt\" failed (13: Permission denied), client: 192.168.1.194, server: www.web1.com, request: \"GET /1.txt HTTP/1.1\", host: \"www.web1.com\" 原因：1、由于启动用户和nginx工作用户不一致所致查看nginx的启动用户，发现是nobody，而为是用root启动的ps aux | grep \"nginx: worker process\" | awk'{print $1}' 解决方案：将nginx.config中的user改为和启动用户一致 2、配置文件中指定的文件例如配置文件中index index.html index.htm这行中的指定的文件。 server { listen 80; server_name localhost; index index.php index.html; root /data/www/; } 如果在/data/www/下面没有index.php,index.html的时候，直接访问文件会报403 forbidden。解决方案：创建一下相应的文件 3、权限问题，如果nginx没有web目录的操作权限，也会出现403错误。 解决办法：修改web目录的读写权限，或者是把nginx的启动用户改成目录的所属用户，重启Nginx即可解决 chmod -R 777 /data/www/ 4、SELinux设置为开启状态（enabled）的原因。 查看SELinux的状态 $ getenforce Enforcing 为开启状态 解决方案：①临时或永久关闭Selinux #临时关别Selinux setenforce 0 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-27 23:32:39 "},"origin/iSCSI-简介配置使用.html":{"url":"origin/iSCSI-简介配置使用.html","title":"群晖Synology的iSCSI","keywords":"","body":"iSCSI的简介配置使用一、iSCSI简介iSCSI（Internet Small Computer System Interface），Internet小型计算机系统接口，又称为IP-SAN，是由IBM 下属的两大研发机构一一加利福尼亚AImaden和以色列Haifa研究中心共同开发的，是一个供硬件设备使用的、可在IP协议上层运行的SCSI指令集，是一种开放的基于IP协议的工业技术标准。一种基于因特网及SCSI-3协议下的存储技术，于2003年2月11日成为正式的标准iSCSI使用 TCP/IP 协议（一般使用TCP端口860和3260）。 本质上，iSCSI 让两个主机通过 IP 网络相互协商然后交换SCSI命令。这样一来，iSCSI 就是用广域网仿真了一个常用的高性能本地存储总线，从而创建了一个存储局域网（SAN）。不像某些 SAN 协议，iSCSI 不需要专用的电缆；它可以在已有的交换和 IP 基础架构上运行。然而，如果不使用专用的网络或者子网（ LAN 或者 VLAN ），iSCSI SAN 的部署性能可能会严重下降两部计算机之间利用iSCSI的协议来交换SCSI命令，让计算机可以透过高速的局域网集线来把SAN模拟成为本地的储存装置iSCSI target：就是iSCSI的server，可以是一个物理磁阵；也可以是软件实现的iSCSI server。有硬件方式实现的iSCSI target，例如iSCSI的hba卡和带isoe（iSCSI offload engine）网卡（硬件上将iSCSI 包接包和封包） iSCSI initiator：就是iSCSI的客户端，它可以是一个软件，也可以是一个硬件。如果是软件在linux上，用户态实现的是tgt框架（linux scsi target frame）；还有一种内核太实现的架构是iet（iSCSI enterprise target）。在centos上目前已经默认安装了tgt了。 iqn（iSCSI qualified name）：initiator和target通过iqn号来逻辑寻址。一个iqn号由四部分组成：iqn.日期.域名:域名组织分配的名字例如：iqn.2000-01.com.synology:Synology.Target-1.826d6a066b LUN：全称是Logical Unit Number，中文名是逻辑单元号。LUN是在存储设备上可以被应用服务器识别的独立存储单元。一个LUN的空间来源于存储池Pool，Pool的空间来源于组成磁盘阵列的若干块硬盘。从应用服务器的角度来看，一个LUN可以被视为一块可以使用的硬盘。例如，在Linux系统中，它在/dev/rdsk、/dev/dsk目录下有相应的设备名称；在Windows系统中，格式化后的新LUN会对应一个类似于D E F的盘符。 Thick LUN：中文名是传统非精简LUN，是LUN类型的一种，支持虚拟资源分配，能以较为简便的方式进行创建、扩容和压缩操作。Thick LUN在创建完成后就会从存储池Pool中分配满额的存储空间，即LUN的大小完全等于分配的空间。因此，它拥有较高的可预测性。 Thin LUN：中文名是精简LUN，也是LUN类型的一种，支持虚拟资源分配，能够以较简便的方式进行创建、扩容和压缩操作。Thin LUN在创建的时候，可以设置一个初始分配容量。创建完成后，存储池Pool只会分配这个初始容量大小的空间剩余的空间仍然放在存储池中。当Thin LUN已分配的存储空间的使用率达到阈值时，存储系统才会再从Pool中划分一定的配额给Thin LUN。如此反复，直到达到Thin LUN最初设定的全部容量。因此，它拥有较高的存储空间利用率。 二、群晖Synology的iSCSI存储创建LUN 创建TargetTarget关联LUN 三、Windows挂载 参考链接https://jingyan.baidu.com/article/e4511cf37feade2b845eaff8.html https://blog.csdn.net/M_joy666/article/details/80566705 附录：Thick LUN与Thin LUN的区别1、空间分配上的区别Thick LUN在创建时会分配所有需要的空间 Thin LUN是一种按需分配的空间组织方法，它在创建时存储池不会分配所有需要的空间，而是根据使用情况动态分配。二者的空间分配区别如下图所示：2、空间回收的区别注：这里的空间回收指的是释放存储池Pool中的资源，并且这些资源可以被其他LUN使用。Thick LUN没有空间回收的概念，因为它在创建时就占用存储池中所有分配给它的空间，即使Thick LUN中的数据被删除，存储池中分配给它的空间还是被占用，不能被其他的LUN使用。但是如果手动删除不再使用的Thick LUN，则对应的空间会被回收。 Thin LUN不仅能够做到空间占用率增大时自动分配新的存储空间，而且当Thin LUN中的文件删除时也可以实现空间的释放，从而实现存储空间的反复利用，大大提高存储空间的利用率。Thin LUN的空间回收如下图所示：3、性能的区别Thick LUN由于在一开始就会拥有所分配的空间，所以Thick LUN在顺序读写的时候拥有较高的性能，但是会造成空间资源的浪费。 Thin LUN由于是实时分配空间，每次扩容时，需要重新增加容量，后台重新格式化，这个时候性能会受到一定影响，而且每次分配空间可能会导致硬盘中存储空间不连续，这样硬盘读写数据时在寻找存放位置上花费的时间会较多，会在顺序读写时对性能有一定影响。4、使用场景的区别Thick LUN：①对性能要求较高的场景 ②对存储空间利用率不太敏感的场景 ③对成本要求不太高的场景 Thin LUN：①对性能要求一般的场景； ②对存储空间利用率比较敏感的场景； ③对成本比较敏感的场景； ④应用环境很难预 估存储空间的场景 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:52:00 "},"origin/gitbook-简介安装配置.html":{"url":"origin/gitbook-简介安装配置.html","title":"GitBook","keywords":"","body":"GitBook简介安装配置一、GitBook简介gitbook 是一个基于node.js的命令行工具 gitbook 支持markdown/asciiDoc语法格式构建书籍 gitbook 支持输出静态网页（可定制和可扩展）和电子书（PDF，ePub或Mobi）等多种格式，其中默认输出静态网页格式 gitbook 不仅支持本地构建书籍,还可以托管在gitbook 官网上，或者Github上 二、GitBook安装1、安装NodeJs环境NodeJs官网下载链接:https://nodejs.org/en/download/Linux以安装NodeJs 10.16.3为例wget https://nodejs.org/dist/v10.16.3/node-v10.16.3-linux-x64.tar.xz && \\ tar -xvf node-v10.16.3-linux-x64.tar.xz -C /opt/ && \\ rm -rf node-v10.16.3-linux-x64.tar.xz && \\ ln -s /opt/node-v10.16.3-linux-x64 /opt/nodejs && \\ sed -i '$a export NODEJS_HOME=/opt/nodejs\\nexport PATH=$PATH:$NODEJS_HOME/bin' /etc/profile && \\ source /etc/profile && \\ yum install gcc-c++ make -y && \\ npm config set registry https://registry.npm.taobao.org && \\ npm config set sass_binary_site https://npm.taobao.org/mirrors/node-sass/ && \\ node -v && \\ npm version Windows直接在官网下载MSI格式的安装包进行安装2、安装Gitbook CLI命令行工具gitbook-cli 是 gitbook 的一个命令行工具, 通过它可以在电脑上安装和管理多个版本的gitbook.npm install gitbook-cli -g 三、GitBook版本的管理gitbook-cli 和 gitbook 是两个软件，gitbook-cli 会将下载的 gitbook 的不同版本放到 ~/.gitbook中, 可以通过设置GITBOOK_DIR环境变量来指定另外的文件夹GitBook可以在本地安装多个版本并在执行命令的时候指定某个版本，如果指定的版本还没安装就会自动下载安装，下载后的GitBook会被放到~/.gitbook目录下。$ gitbook --help Usage: gitbook [options] [command] Options: -v, --gitbook [version] specify GitBook version to use -d, --debug enable verbose error -V, --version Display running versions of gitbook and gitbook-cli -h, --help output usage information Commands: ls List versions installed locally current Display currently activated version ls-remote List remote versions available for install fetch [version] Download and install a alias [folder] [version] Set an alias named pointing to uninstall [version] Uninstall a version update [tag] Update to the latest version of GitBook help List commands for GitBook * run a command with a specific gitbook version # 查看当前GitBook CLI版本 gitbook -V # 列出本地安装版本 gitbook ls # 列出当前使用版本 gitbook current # 列出远程可用版本 gitbook ls-remote # 安装指定版本(如果安装比较慢的话，将npm镜像源切到国内的CNPM镜像源。可使用NRM管理NPM的镜像源) gitbook fetch [version] # 卸载指定版本 gitbook uninstall [version] # 更新指定版本 gitbook update [tag] 四、GitBook CLI命令1、gitbook 可用命令$ gitbook help build [book] [output] 构建书籍 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) --format Format to build to (Default is website; Values are website, json, ebook) --[no-]timing Print timing debug information (Default is false) serve [book] [output] serve the book as a website for testing --port 指定监听端口(默认端口4000) --lrport Port for livereload server to listen on (Default is 35729) --[no-]watch Enable file watcher and live reloading (Default is true) --[no-]live Enable live reloading (Default is true) --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) --format Format to build to (Default is website; Values are website, json, ebook) install [book] 安装所有插件资源 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) parse [book] parse and print debug information about a book --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) init [book] 初始化创建书籍文件结构 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) pdf [book] [output] 构建书籍为ebook文件 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) epub [book] [output] 构建书籍为ebook文件 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) mobi [book] [output] 构建书籍为ebook文件 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) 2、gitbook init初始化创建书籍文件结构gitbook init # 在当前路径下自动生成README.md 和 SUMMARY.md。也可以先手动创建SUMMARY.md，再执行gitbook init，如果SUMMARY.md中配置的文件夹和文件不存在，就会自动创建文件夹和文件，已经存在的文件夹和文件不会被覆盖。 gitbook init ./directory # 可将书籍初始化到指定目录 3、gitbook build构建gitbook书籍静态HTML资源gitbook build [book] [output] # 会在书籍的文件夹中生成一个 _book 的文件夹, 里面有生成的静态HTML资源。可将 _book 文件夹下的文件拷贝到nginx、httpd等web服务器内 gitbook build --gitbook=2.0.1 # 指定Gitbook版本 4、gitbook serve启动本地预览书籍服务gitbook serve [book] [output] 浏览器中打开： http://localhost:4000 预览GitBook书籍5、输出书籍文件Prerequisite：ebook-convert：GitBook在生成PDF的过程中使用到calibre的转换功能，没有安装Calibre或安装了Calibre没有配置环境变量都会导致转换PDF失败。Calibre下载地址：https://calibre-ebook.com/download 在 Typora 中安装 Pandoc 进行导出 # 输出书籍为PDF格式文件 gitbook pdf [book] [output] # 输出书籍为epub格式文件 gitbook epub [book] [output] # 输出书籍为mobi格式文件 gitbook mobi [book] [output] 6、gitbook install安装插件样式资源gitbook install [book] #会在当前路径下生成node_modules文件夹，里面为插件的样式资源 7、gitbook parse 解析电子书gitbook parse [book] 五、GitBook的文件结构 文件/文件夹 描述 是否必须 README.md 书籍的简介 必须 SUMMARY.md 书籍的目录结构 可选 book.json GitBook的插件样式配置文件 可选 GLOSSARY.md 词汇、术语列表 可选 _book文件夹 GitBook输出的静态HTML文件 node_modules文件夹 插件的样式资源 六、SUMMARY.md编写规则SUMMARY.md 的格式是一个链接列表。链接的标题将作为章节的标题，链接的目标是该章节文件的路径 向父章节添加嵌套列表将创建子章节 每章都有一个专用页面（part#/README.md），并分为子章节。 目录中的章节可以使用锚点指向文件的特定部分。 目录可以分为以标题或水平线 ---- 分隔的部分 Parts 只是章节组，没有专用页面，但根据主题，它将在导航中显示。 七、book.json编写规则常规设置 变量 描述 **root** 包含所有图书文件的根文件夹的路径，除了 `book.json` **structure** 指定 Readme，Summary，Glossary 和 Languages 的名称（而不是使用默认名称，如README.md）。这些文件必须在项目的根目录下（或 `root` 属性指定的根目录）structure.readme：Readme 文件名（默认值是 `README.md` ）structure.summary：Summary 文件名（默认值是 `SUMMARY.md` ）structure.glossary：Glossary 文件名（默认值是 `GLOSSARY.md` ）structure.languages：Languages 文件名（默认值是 `LANGS.md` ） **title** 您的书名，默认值是从 README 中提取出来的。在 GitBook.com 上，这个字段是预填的。 **description** 您的书籍的描述，默认值是从 README 中提取出来的。在 GitBook.com 上，这个字段是预填的。 **author** 作者名。在GitBook.com上，这个字段是预填的。 **isbn** 国际标准书号 ISBN **language** 本书的语言类型 —— [ISO code](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) 。默认值是 `en` **direction** 文本阅读顺序。可以是 `rtl` （从右向左）或 `ltr` （从左向右），默认值依赖于 `language` 的值。 **gitbook** 应该使用的GitBook版本。使用 [SemVer](http://semver.org/) 规范，并接受类似于 `“> = 3.0.0”` 的条件。 **links** 在左侧导航栏添加链接信息 **plugins** 要加载的插件列表 **pluginsConfig** 插件的配置 Gitbook 默认带有 5 个插件：highlight：语法高亮插件 search：搜索插件 sharing：分享插件 font-settings：字体设置插件 livereload：热加载插件 Note：去除插件\"plugins\": [ \"-search\" ]插件配置示例{ \"author\": \"Curiouser \", \"title\": \"Devops Roadmap\", \"plugins\": [ \"-search\", \"-lunr\", \"-sharing\", \"-highlight\", \"search-pro\", \"splitter\", \"github\", \"popup\", \"sectionx\", \"expandable-chapters\", \"sharing-plus\", \"code\", \"auto-scroll-table\", \"theme-fexa\", \"tbfed-pagefooter\", \"back-to-top-button\", \"emphasize\", \"edit-link\", \"prism\", \"donate\", \"theme-comscore\", \"github-buttons\", \"github-issue-feedback\" ], \"pluginsConfig\": { \"theme-default\": { \"showLevel\": true }, \"github-issue-feedback\": { \"repo\": \"RationalMonster/rationalmonster.github.io\" }, \"github\": { \"url\": \"https://github.com/RationalMonster\" }, \"github-buttons\": { \"buttons\": [{ \"user\": \"RationalMonster\", \"repo\": \"rationalmonster.github.io\", \"type\": \"star\", \"size\": \"small\", \"count\": \"true\" }] }, \"sharing\": { \"weibo\": true, \"qq\": \"true\", \"google\": true, \"all\": [ \"facebook\", \"twitter\" ] }, \"code\": { \"copybuttons\": \"true\" }, \"theme-fexa\":{ \"search-placeholder\":\"搜索文章\", \"logo\": \"assets/logo.png\" }, \"tbfed-pagefooter\": { \"copyright\":\"Copyright Curiouser\", \"modify_label\": \"该文件最后修改时间：\", \"modify_format\": \"YYYY-MM-DD HH:mm:ss\" }, \"edit-link\": { \"base\": \"https://github.com/RationalMonster/rationalmonster.github.io/blob/master\", \"label\": \"ORIGIN In Github\" }, \"prism\": { \"css\": [ \"prismjs/themes/prism-tomorrow.css\" ], \"lang\": { \"flow\": \"typescript\" }, \"ignore\": [ \"mermaid\", \"eval-js\" ] }, \"donate\": { \"wechat\": \"../assets/wechat-donate.jpg\", \"title\": \"\", \"button\": \"赏\", \"wechatText\": \"微信打赏\" } } } 八、GLOSSARY.md 编写规则GLOSSARY.md 的格式是 h2 标题的列表，以及描述段落 九、忽略文件和文件夹GitBook将读取 .gitignore，.bookignore 和 .ignore 文件，来过滤不需要进行git版本控制的文件和文件夹。这些文件中的格式遵循 .gitignore 的规则：# This is a comment # Ignore the file test.md test.md # Ignore everything in the directory \"bin\" bin/* ### gitbook ### _node !docs _book node_modules ### IDEA ### .idea/ ### VS Code ### .vscode/ ### OS ### .DS_Store 十、封面封面用于所有电子书格式。您可以自己提供一个，也可以使用 autocover plugin 生成一个。要提供封面，请将 cover.jpg 文件放在书本的根目录下。添加一个 cover_small.jpg 将指定一个较小版本的封面。封面应为 JPEG 文件。好的封面应该遵守以下准则：cover.jpg 的尺寸为 1800x2360 像素，cover_small.jpg 为 200x262 没有边界 清晰可见的书名 任何重要的文字应该在小版本中可见 十一、多语言支持gitbook 支持构建用多种语言书写的书籍。每种语言应该是一个子目录，遵循正常的gitbook格式，然后需要在根目录下放置一个名为 LANGS.md 的文件，存放下列内容：# Languages * [English](en/) * [French](fr/) * [Español](es/) 注意：当一个语言的书(如：en)有 book.json 时，它的配置将扩展主要配置。 唯一的一个例外是插件，插件是全局设置的，并且不能指定语言特定的插件 插件的配置必须写在根目录下的 book.json 文件中。然后其他语言的配置可以分别写在各自语言目录下的 book.json 文件中。 LANGS.md 文件中各个语言出现的顺序，就是书籍首页出现的顺利。因此，写在第一位的语言，就自然成为书籍首页打开时的默认语言。 当一个语言的书(如：en)有 book.json 时，它的配置将扩展主要配置。唯一的一个例外是插件，插件是全局设置的，并且不能指定语言特定的插件。十二、托管到 Github PagesGithub 有个功能： GitHub Pages 。它允许用户在 GitHub 仓库托管你的个人、组织或项目的静态页面（自动识别 html、css、javascript）。1、建立 xxx.github.io 仓库要使用这个功能，首先，你必须建立一个严格遵循以下命名要求的仓库：Github账号名.github.io 举例，我的 Github 账号为 dunwu，则这个仓库应该叫 dunwu.github.io。通常，这个仓库被用来作为个人或组织的博客。2、建立 gh-pages 分支xxx.github.io 仓库中建立一个名为 gh-pages 的分支。只要 gh-pages 中的内容符合一个静态站点要求，就可以在如下地址中进行访问：https://Github用户名.gitbooks.io/Github 仓库 。3、自动化发布方式一：使用 gh-pages 插件在本地安装插件npm i -D gh-pages 在 package.json 文件中添加脚本命令：如下：-d 命令参数后面是要发布的静态站点内容的目录\"scripts\": { \"deploy\": \"gh-pages -d build\" }, 方式二：使用脚本cd build \\ git init \\ git checkout -b gh-pages \\ git add . \\ git commit -am \"Update\" \\ git push git@github.com:****4/gitbook-notes gh-pages --force\" 问题gitbook serve时，偶尔不规律性地出现编译错误,而且每次出现的错误文件还可能不一样，实在是头疼得很，每次修改要编译多次才能成功修改 C:\\Users\\当前用户名\\.gitbook\\versions\\当前使用的gitbook版本\\lib\\output\\website\\copyPluginAssets.js文件中的112行，将confirm: true改为confirm: false 参考链接https://www.jianshu.com/p/f38d8ff999cb?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 20:51:59 "},"origin/telegram-Bot机器.html":{"url":"origin/telegram-Bot机器.html","title":"Telegram机器人","keywords":"","body":"Telegram Bot机器人一、简介Telegram Bot是运行在Telegram内部的第三方应用程序，相当于Telegram的一个特殊账户。用户可以向Telegram Bot发送消息，命令和内联请求等方式与Telegram Bot人进行交互，而Telegram Bot开发者可以通过Telegram Bot API，用https请求方式来控制机器人二、创建客户端搜索\"Botfather\" 查看帮助 发送\"/newboot\"来创建Bot机器人,根据提示一步一步进行.(当设置用户名时)TOKEN 一定要保护好！以后接口访问都要用到！ 三、APITelegram有两种api，一种是bot api，一种是telegram api。bot api是基于http访问，telegram api是基于mtproto访问，访问需要加密，相对要复杂一些。后者也可以实现发送消息等功能 可使用PostMan或者Curl等工具发送HTTPS请求调用Bot的API。 当时用Curl命令时可使用\"-x\"参数设置代理。例如“curl -x 127.0.0.1:3128 -sk https://www.google.com” 四、Bot APIBot API文档链接1. Bot API相关信息Bot API支持GET和POST方法的HTTPS请求，URL格式为:\"https://api.telegram.org/bot[你的bot机器人Token]/方法名\"例如：https://api.telegram.org/bot123456:ABC-DEF1234ghIkl-zyx57W2v1u123ew11/getMe支持一下几种传参方式：URL query string application/x-www-form-urlencoded application/json (except for uploading files) multipart/form-data (use to upload files) Bot机器人将返回JSON格式的对象，里面会包含返回状态信息注意: API方法大小写敏感 请求格式必须是UTF-8编码 2. 示例使用PostMan给Bot机器人发送消息 使用Curl命令给Bot机器人发送消息curl -x 梯子IP地址 -sk \\ -X POST \\ https://api.telegram.org/bot90****93:AAF***RfFma8/sendMessage \\ -d 'chat_id=623***17' \\ -d 'parse_mode=Markdown' \\ -d 'text=*Jenkins '$BUILD_NUMBER' *' 3. 支持的消息格式MarkDown风格*bold text* _italic text_ [inline URL](http://www.example.com/) [inline mention of a user](tg://user?id=123456789) `inline fixed-width code` ​```block_language pre-formatted fixed-width code block ​``` HTML风格 *bold text* _italic text_ [inline URL](http://www.example.com/) [inline mention of a user](tg://user?id=123456789) `inline fixed-width code` ​```block_language pre-formatted fixed-width code block ​``` 标签不能嵌套 所有不属于标签或HTML实体的' '和' & '符号必须替换为相应的HTML实体 (\"\"对应\"\\>\"、\"\\&\"对应\"\\&\") 支持所有数字类型的HTML实体 该API目前仅支持以下命名的HTML实体:' '、' & '和' \" ' 4. 支持的方法 Bot API方法 描述 getMe sendMessage 发送文本信息,支持Markdown、HTML格式化的文本信息 forwardMessage sendPhoto 发送图片 sendAudio 发送音频，最大50 MB sendDocument 发送文档，最大50 MB sendVideo 发送视频，最大50 MB sendAnimation 发送动图，最大50 MB(支持无声音的GIF或H.264/MPEG-4 AVC格式动图) sendVoice 发送录音，最大50 MB sendVideoNote sendMediaGroup sendLocation 发送定位 editMessageLiveLocation stopMessageLiveLocation sendVenue sendContact 发送名片 sendPoll 发送投票 sendChatAction getUserProfilePhotos getFile kickChatMember unbanChatMember restrictChatMember promoteChatMember setChatPermissions exportChatInviteLink setChatPhoto deleteChatPhoto setChatTitle 设置聊天室标题 setChatDescription 设置聊天室描述 pinChatMessage unpinChatMessage leaveChat 离开聊天室 getChat 查找聊天室 getChatAdministrators 获取聊天室管理员 getChatMembersCount 获取聊天室成员个数 getChatMember 获取聊天室成员 setChatStickerSet deleteChatStickerSet answerCallbackQuery Inline mode methods Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-11-11 22:42:55 "}}